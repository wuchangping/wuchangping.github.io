<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>从马斯克们的Hyperloop到用Flink加Pravega打造的“流原生”式大数据处理平台</title>
      <link href="/2018/09/22/streaming-native-platform/"/>
      <url>/2018/09/22/streaming-native-platform/</url>
      
        <content type="html"><![CDATA[<h1 id="开篇-马斯克们的Hyperloop"><a href="#开篇-马斯克们的Hyperloop" class="headerlink" title="开篇,马斯克们的Hyperloop"></a>开篇,马斯克们的Hyperloop</h1><p>我们先来看张图，下图上部分是现在的高铁，它是跑在露天的轨道上的，下图是Elon Musk’s 在正吹的<a href="https://hyperloop-one.com" target="_blank" rel="noopener">hyperloop</a>，类似于跑在真空管道里的未来高铁。相比跑在露天轨道里的高铁，跑真空管道里的高铁好处多了：快，节能，安全，比飞机便宜。。。<br>技术是可以自己进化的，相信类似hyperloop的”高铁+真空管道”的模式就是未来的一种交通出行方式。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/streaming%2Fstreaming-native-platform-0.jpg" alt="hyperloop"></p><p>那么HYPERLOOP跟本文又有什么关系呢？ 是不是有点扯远了？其实本文讲的就是类似给高铁加上真空管道的活，二者本质上是相同的。</p><p>##管道,Unix/Linux的设计哲学</p><p>在Linux或者Unix系统里,有时候我们为了查询某个信息，会输入类似如下的命令行：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">#cat *.log | grep –v ‘pipeline’ | sort –nr | head –n 10 | tail -5 | awk ‘&#123;print $2&#125;’ | wc –l  &gt; /dev/stdout<br></code></pre></td></tr></table></figure><p>这个命令行通过“|”来分隔多个命令，前面命令的输出是紧接着的后面命令的输入，命令之间通过“|”彼此相连，并且一个命令只做一件事情。这里的“|”就是管道，把一个程序的输出和另一个程序的输入连起来的一根管子。</p><p>在Unix/Linux里存在这样的管道命令设计哲学：</p><ul><li>程序是个过滤器</li><li>一个程序只做一件事并且做到最好</li><li>一个程序的输入是另外一个程序的输出</li></ul><p>下图体现了这样的管道设计哲学，应用之间通过管道相连相互作用：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/streaming%2Fstreaming-native-platform-1.PNG" alt="uniux linux pipeline 1"></p><p>管道所要解决的问题是：<code>高内聚，低耦合</code>。它以一种“链”的方式将这些程序组合起来，让这些程序组成一条工作流，而每个程序又只作一件事情，给定输入，经过各个程序的先后处理，最终得到输出结果，如下图所示：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/streaming%2Fstreaming-native-platform-2.PNG" alt="uniux linux pipeline 2"></p><p>Unix/Linux在<code>&quot;每个程序只做一件事并且做好，每个程序的输出是对另一个程序的输入，可组合性&quot;</code>方面是做的非常成功的。但是，UNIX/Linux也存在一些局限性，比如：<code>&quot;仅单机，只支持一对一通信，无容错，仅字节流,数据处理能力有限等&quot;</code>。意思是说 linux/unix的这些管道命令只能在一台机器上跑，没有分布式，并且只能支持一个命令和另外一个命令之间的一对一的输入输出，无法一对多或多对一；无容错，假如管道坏了数据就出错不能恢复；只支持字节流，不支持数据格式的多样性；处理的数据量有限。</p><p>因此，我们希望可以找到一个数据处理解决方案，这个方案在保留这些Unix/linux管道的设计哲学优点的同时还能克服其缺点。 幸运的是，我们通过Flink+Pravega打造的第三代“流原生”(stream native)式的大数据处理平台实现了这种设计思想。</p><h2 id="流原生-第三代大数据处理平台"><a href="#流原生-第三代大数据处理平台" class="headerlink" title="流原生,第三代大数据处理平台"></a>流原生,第三代大数据处理平台</h2><p>下图体现了“流原生”(stream native)式的设计哲学，Flink是“流原生”的计算，Pravega是“流原生”的存储管道，Flink + pravega 是“流原生”的大数据处理平台。数据从pravega管道输入经过map算子计算，输出中间计算结果到pravega的管道里，数据又从pravega的管道里读入到filter算子里，再经过计算，中间结果放到了pravega管道里，再最后的计算结果经过聚合算子的计算放到了目的地的pravega的管道里。这个过程体现了算子编排和管道式编程的设计哲学。在这里pravega起了大数据处理平台里的管道的作用。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/streaming%2Fstreaming-native-platform-3.PNG" alt="uniux linux pipeline"></p><p>在Unix/Linux中，系统提供管道和命令，用于从一个进程到另一个进程获取字节流。</p><p>在“流原生”处理平台上，Flink提供流处理服务，pravega提供流存储服务，数据源自pravega，被Flink算子们处理后输出到pravega，这是一种将事件从一个流处理作业转移到另一个流处理作业的机制。 Flink和Pravega 所遵循的流处理平台设计哲学是：</p><ul><li>每个算子都只做一件事，并且做到最好</li><li>每个算子的输出是另一个算子的输入</li><li>可组合</li><li>流式传输：数据是动态的，算子是静态的</li><li>算子可编排</li><li>Pravega是最好的Flink搭档</li><li>分布式，扩展到多台机器</li><li>可进化的编码/解码</li></ul><p>当前的流式处理平台一般是 Flink 加传统的存储类型，这种是”半流原生“式的大数据处理平台，计算是原生的流计算而存储却不是原生的流存储。<br>而Pravega就是专门给Flink们设计的原生流存储，它的数据传输方式类似于“管道”，不同于传统的块存储，文件存储以及对象存储，它是一个”管道式流存储“。</p><p>通过Flink + Pravega的组合可以实现 “流原生”(stream native)式的第三代大数据处理平台，未来已来。。。。。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>最后给大家留个思考题，“流原生”(stream native)的概念有了，Flink + Pravega 也有了，而且二者的代码都是开源的（flink.apache.org, pravega.io），那么怎么把这些开源的东西产品化？ 或者这个问题太伤脑筋，我们换个简单的问题：“今天中午到哪吃饭？”</p>]]></content>
      
      
      <categories>
          
          <category> streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> streaming </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>算命先生的阴阳五行学说与码农们的分布式系统设计理论</title>
      <link href="/2018/09/22/distributed-tradeoff/"/>
      <url>/2018/09/22/distributed-tradeoff/</url>
      
        <content type="html"><![CDATA[<h2 id="阴阳五行"><a href="#阴阳五行" class="headerlink" title="阴阳五行"></a>阴阳五行</h2><p>一说到阴阳五行就容易让人想到大街上的算命先生，然而阴阳五行学说却是中国古代解释世间万物的起源和多样性的哲学理论依据，是中国古代朴素的唯物论和自发的辩证法思想。</p><p>中国古代哲学的核心思想之一用“老子”的话来说就是：</p><blockquote><p>“道生一、一生二、二生三、三生万物，万物负阴而抱阳，冲气以为和。”。</p></blockquote><p>解读开来就是：</p><blockquote><p>“天道生阴阳，阴阳成五行，五行变化成万物，而万物的存在方式和相互关系一直在追求一种“和谐”。 </p></blockquote><p>“道”在阴阳的相互作用下，产生五行，五行之间相互作用产生世间万物的无穷变化，并且阴阳之间对立消长，五行之间相生相克，自此万物得以和谐发展。</p><p>借助于阴阳五行的核心要素以及由此而生的非核心要素关系把宇宙看成一个统一的整体，这样的整体：<code>循环平衡、相生相克、有刚有柔、和谐统一</code>。</p><p>五行讲的是:<code>“金 木 水 火 土”</code> 这五行,五行相生又相克。</p><p>五行相生是为：</p><blockquote><p>木头烧火——木生火；火烧木头成灰——火生土，土长期聚在一起生石头、石头里炼金——土生金，金销水——金生水，水又生土。</p></blockquote><p>五行相克是为：</p><blockquote><p>水克火，火克金，金克木，木克土，土克水。</p></blockquote><p>如下图，五行都是为“和”字而服务的，即平衡：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-tradeoff-1.PNG" alt="五行"></p><p>那么这些玄乎的哲学理论跟码农又有什么关系呢？对于本人这么个靠技术混饭吃卖身又卖艺的码农来说，这实在太重要，归纳成一个字就是”和”，对应到技术实现体系里就是一个理念 ”权衡“，英文叫<code>tradeoff</code>。<code>“tradeoff”</code>这词实在是太妙了，啥都可以往上套，比如你十一准备到哪旅游啦，中午到哪吃饭啦，买哪里的房子啦，准备追哪个姑娘做老婆啦…….，都需要 <code>tradeoff</code>。技术如此人生又何尝不如是。</p><h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p>通常来讲设计分布式系统的时候需要考虑的最重要的<code>核心要素</code>有五个，这里不是说其他要素就不重要，这是指经过<code>tradeoff</code>过的五个最重要的核心要素，如下图：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-tradeoff-2.PNG" alt="分布式系统要素"></p><ol><li><p><code>Capacity</code>，容量，其实这个词翻译成”能力“会更合适，指的是分布式系统里的CPU，内存，硬盘，网络，文件描述符，socket连接数，老板的预期，开发周期，成本预算之类的限制条件,以下所有的要素都受 “容量”的限制，这是前提条件，就比如一辆车最多能跑多快，一个人最多能跳多高都是受自身“容量/能力”的限制的；</p></li><li><p><code>Performant</code>, performance + conformant, performant这词也是造的，指的是合适的性能，分布式系统的IOPS，TPS, QPS，Latency,Jitter之类的性能指标要求，性能受限于容量，性能同时又影响了可靠性以及可用性；</p></li><li><p><code>Availability</code>，可用性，可用性通常指的是产品或服务在随机时间内调用时处于可服务状态的概率，通常被定义为正常运行时间除以总时间（正常运行时间加停机时间），比如 5个9，6个9，还有个厂家都喜欢的号称的9个9之类的，可用性受容量的限制同时也受可伸缩性的影响，可用性又影响了性能；</p></li><li><p><code>Reliability</code>，可靠性，一般指的是出保证不出故障的概率，比如，企业级产品 5个9是保底，可测试性和可维护性通常被定义为可靠性当中的一部分，可伸缩性影响了可靠性，而可靠性又影响了可用性，同时性能又影响了可靠性，可靠性也影响着性能。</p></li><li><p><code>Scalability</code>，可伸缩性，这里很容易跟“可扩展性”混淆，可伸缩性可以指的是集群处理越来越多或越来越少的工作的能力，或者是为了适应这种增长或减少而扩大或缩小其能力的能力。可伸缩性影响了可用性，也影响了性能与可靠性，受限于容量。</p></li></ol><p>当然还有另外一些由此而衍生的非核心要素，就不多做详细解释了，比如：</p><ul><li>Testability，可测试性</li><li>Security，安全性</li><li>Observability，可观测性</li><li>Predictability，可预测性</li><li>Extensibility，可扩展性</li><li>Maintainability，可维护性</li></ul><p>这些非核心要素虽然是非核心但是也不是说就不重要，是<code>开源产品与商业产品</code>差异的关键，关键在如何<code>tradeoff</code>。</p><h2 id="阴阳五行与分布式系统"><a href="#阴阳五行与分布式系统" class="headerlink" title="阴阳五行与分布式系统"></a>阴阳五行与分布式系统</h2><p>将阴阳五行理论与分布式系统设计理论结合起来解读就是：</p><p><code>分布式系统里的“道”就是“产品”，“阴阳“ 就是 ”功能“ 与 “非功能”，五行就是 ”容量、性能、可用性、可伸缩性以及可靠性“，阴阳五行衍生的一些其他关系对应分布式系统五要素衍生的一些其他要素。</code></p><p>用人话来讲就是 开发产品的时候需要考虑功能与非功能两个方面，而要保证产品质量又需要考虑”容量、性能、可用性、可伸缩性以及可靠性“这些核心要素，但是也不能忽略由此而生的一些非核心要素。</p><p>那么从这些理论到产品又需要怎么做才能落地呢？ 那自然是需要 <code>懂得如何把从这些概念性的、功能的、非功能的、这些核心的、非核心的要素进行设计实现成代码</code>，这就涉及到 “术”的层面了，“道”的层面可以通过看书看论文获得，而<code>“术”</code>的获得除了自身努力还得靠机会，而且每个人的悟性还不一样，这些个”术“以后有空慢慢讲。</p><p>最后给大家留一个思考题： 前面提过老子曰：<code>”道生一、一生二、二生三、三生万物，万物负阴而抱阳，冲气以为和。“</code>， 三之后就是万物，为什么不是 五、不是六、不是七之类的呢？为什么三之后就是万物了？</p><p>注：</p><ol><li>这个用五行解释分布式系统的观点，以前在一个业内微信群里提出并且聊过，所以这个解读的方式为本人原创非COPY.</li><li>个人愚钝，悟性有限，欢迎拍砖，砖多了我就拿回去砌墙。</li></ol><p>五行理论参考来源：</p><p>[1]. <a href="https://baike.sogou.com/v7556185.htm" target="_blank" rel="noopener">https://baike.sogou.com/v7556185.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> distributed </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>应该在同一个Kafka主题中放入几种事件类型吗？</title>
      <link href="/2018/09/21/streaming-put-several-event-types-kafka-topic/"/>
      <url>/2018/09/21/streaming-put-several-event-types-kafka-topic/</url>
      
        <content type="html"><![CDATA[<p>如果您采用Apache Kafka等流平台，有个很重要的问题是：将使用哪些主题？特别是，如果要将一堆不同的事件作为消息发布到Kafka，是将它们放在同一主题中，还是将它们拆分为不同的主题？</p><p>主题最重要的功能是允许使用者指定它想要使用的消息子集。在一个极端情况下，所有数据都放在一个主题中可不是一个好主意，因为这意味着消费者无法选择感兴趣的事件 - 给他们的只会是所有的内容。在另一个极端，拥有数百万个不同的主题也不是一个好事，因为Kafka中的每个主题都会消耗资源消耗，因此拥有大量的主题就会对性能不利。</p><p>实际上，从性能的角度来看，重要的是分区数量。但由于Kafka中的每个主题至少有一个分区，如果你有n个主题，那么就不可避免地至少有n个分区。不久之前，[Jun Rao撰写了一篇博文]，解释了拥有多个分区的成本（端到端延迟，文件描述符，内存开销，故障后的恢复时间）。根据经验，如果您关注延迟问题，您应该关注每个代理节点上的（数量级）数百个主题分区。如果每个节点有数万个甚至数千个分区，则延迟会受到影响。</p><p>该性能参数给设计主题的结构提供了一些指导：如果您发现自己有数千个主题，那么将一些细粒度，低吞吐量的主题合并到粗粒度主题中是明智之举，从而减少分区的扩散。</p><p>然而，性能问题并不是结束。在我看来，更重要的是您的主题结构的数据完整性和数据建模方面。我们将在本文的其余部分讨论这些内容。</p><h2 id="主题-相同类型的事件的集合？"><a href="#主题-相同类型的事件的集合？" class="headerlink" title="主题=相同类型的事件的集合？"></a>主题=相同类型的事件的集合？</h2><p>常见的想法（根据我所拥有的几个对话，并根据邮件列表）似乎是：将同类型的所有事件放在同一主题中，并针对不同的事件类型使用不同的主题。这种思路让人联想到关系数据库，其中表是具有相同类型（即同一组列）的记录的集合，因此我们在关系表和Kafka主题之间进行类比。</p><p>该<a href="https://www.confluent.io/confluent-schema-registry/" target="_blank" rel="noopener">融合模式的注册表</a>本质上强化了这种模式，因为它鼓励你在主题中的所有消息使用相同的Avro模式。该模式可以在保持兼容性的同时进化（例如，通过添加可选字段），但最终所有消息都必须符合某种记录类型。在我们介绍了更多背景之后，我们将在后面的帖子中再次讨论这个问题。</p><p>对于某些类型的流数据（例如记录的活动事件），要求同一主题中的所有消息都符合相同的模式是有意义的。但是，有些人正在使用Kafka来实现更多类似数据库的目的，例如事件溯源，或者在微服务之间交换数据。在这种情况下，我相信，它定义一个主题为一组具有相同模式的消息并不重要。更重要的是Kafka维护主题分区内的消息排序。</p><p>想象一下，您有一些事物（比如客户），并且该事物可能发生许多不同的事情：创建客户，客户更改地址，客户向其帐户添加新信用卡，客户进行客户支持查询，客户支付发票，客户关闭其帐户。</p><p>这些事件的顺序很重要。例如，我们期望在客户做任何动作之前创建客户，并且我们也期望在客户关闭其帐户之后不再发生任何其他事情。使用Kafka时，您可以通过将它们全部放在同一个分区中来保留这些事件的顺序。在此示例中，您将使用客户ID作为分区键，然后将所有这些不同的事件放在同一主题中。它们必须位于同一主题中，因为不同的主题意味着不同的分区，并且不会跨分区保留排序。</p><h2 id="排序问题"><a href="#排序问题" class="headerlink" title="排序问题"></a>排序问题</h2><p>如果你没有使用（比方说）不同的主题<code>customerCreated</code>，<code>customerAddressChanged</code>和<code>customerInvoicePaid</code>事件，然后这些议题的消费者可能会看到荒谬的事件顺序。例如，消费者可能会看到不存在的客户的地址更改（因为尚未创建，因为相应的<code>customerCreate</code>事件已被延迟）。</p><p>如果消费者暂停一段时间（可能是维护或部署新版本），则重新排序的风险尤其高。当消费者停止时，事件将继续发布，并且这些事件将存储在Kafka代理的选定主题分区中。当消费者再次启动时，它会消耗来自其所有输入分区的积压事件。如果消费者只有一个输入，那就没问题了：挂起的事件只是按照它们存储的顺序依次处理。但是，如果消费者有几个输入主题，它将选择输入主题以按任意顺序读取。它可以在读取另一个输入主题上的积压之前从一个输入主题读取所有挂起事件，或者它可以以某种方式交错输入。</p><p>因此，如果你把<code>customerCreated</code>，<code>customerAddressChanged</code>以及<code>customerInvoicePaid</code>事件在三个独立的主题，消费者可能会看到所有的<code>customerAddressChanged</code>事件，它看到任何之前<code>customerCreated</code>的事件。因此，消费者可能会看到一个<code>customerAddressChanged</code>客户的事件，根据其对世界的看法，尚未创建。</p><p>您可能想要为每条消息附加时间戳，并将其用于事件排序。如果要将事件导入数据仓库，您可以在事后对事件进行排序，这可能就可以了。但是在流进程中，时间戳是不够的：如果你得到一个具有特定时间戳的事件，你不知道你是否仍然需要等待一个时间戳较低的先前事件，或者所有之前的事件是否已到达而你是’准备好处理这个事件。依靠时钟同步通常会导致噩梦;</p><h2 id="何时分割主题，何时结合？"><a href="#何时分割主题，何时结合？" class="headerlink" title="何时分割主题，何时结合？"></a>何时分割主题，何时结合？</h2><p>鉴于这种背景，我将提出一些经验法则来帮助您确定在同一主题中放入哪些内容，以及将哪些内容拆分为单独的主题：</p><ol><li><p>最重要的规则是，  任何需要保持固定顺序的事件必须放在同一主题中（并且它们也必须使用相同的分区键）。最常见的是，如果事件的顺序与同一事物有关，则事件的顺序很重要。因此，根据经验，我们可以说关于同一事物的所有事件都需要在同一主题中。如果您使用事件排序方法进行数据建模，事件的排序尤为重要。这里，聚合对象的状态是通过以特定顺序重放它们来从事件日志中导出的。因此，即使可能存在许多不同的事件类型，定义聚合的所有事件也必须在同一主题中。</p></li><li><p>当您有关于不同事物的事件时，它们应该是相同的主题还是不同的主题？我想说，如果一个事物依赖于另一个事物（例如，一个地址属于一个客户），或者如果它们经常需要在一起，那么它们也可能会出现在同一个主题中。另一方面，如果它们不相关并由不同的团队管理，则最好将它们放在单独的主题中。它还取决于事件的吞吐量：如果一个事物类型具有比另一个事物类型高得多的事件，它们是更好地分成单独的主题，以避免压倒性的消费者只想要具有低写入吞吐量的事物（参见第4点）。但是，几个都具有低事件率的事物可以很容易地合并。</p></li><li><p>如果一个事件涉及多个事物怎么办？例如，购买涉及产品和客户，并且从一个帐户到另一个帐户的转移涉及至少那两个帐户。我建议最初将事件记录为单个原子消息，而不是将其分成几个消息。主题，最好以完全按照您收到的方式记录事件，并尽可能采用原始形式。您可以随后使用流处理器拆分复合事件 - 但如果您过早地将其拆分，则重建原始事件要困难得多。更好的是，您可以为初始事件提供唯一ID（例如UUID）; 以后，当您将原始事件拆分为每个涉及的事物的一个事件时，您可以将该ID转发，从而使每个事件的起源都可追溯。</p></li><li><p>查看消费者需要订阅的主题数量。如果几个消费者都阅读了一组特定的主题，这表明可能应该将这些主题组合在一起。如果将细粒度的主题组合成粗粒度的主题，一些消费者可能会收到他们需要忽略的不需要的事件。这不是什么大问题：消费来自Kafka的消息非常便宜，所以即使消费者最终忽略了一半的事件，这种过度消费的成本可能也不大。只有当消费者需要忽略绝大多数消息（例如99.9％是不需要的）时，我才建议从高容量流中分割低容量事件流。</p></li><li><p>Kafka Streams状态存储（KTable）的更改日志主题应与所有其他主题分开。在这种情况下，主题由Kafka Streams流程管理，不应与其他任何内容共享。</p></li><li><p>最后，如果上述规则都没有告诉您是否将某些事件放在同一主题或不同主题中，该怎么办？然后，通过将相同类型的事件放在同一主题中，通过所有方法将它们按事件类型分组。但是，我认为这条规则是最不重要的。</p></li></ol><h2 id="模式管理"><a href="#模式管理" class="headerlink" title="模式管理"></a>模式管理</h2><p>如果您使用的是数据编码（如JSON），而没有静态定义的模式，则可以轻松地将许多不同的事件类型放在同一主题中。但是，如果您使用的是基于模式的编码（如Avro），则需要更多地考虑在单个主题中处理多个事件类型。</p><p>如上所述，基于Avro的<code>Kafka Confluent Schema Registry</code>目前依赖于每个主题都有一个模式的假设（或者更确切地说，一个模式用于密钥，一个模式用于消息的值）。您可以注册新版本的模式，注册表会检查模式更改是向前还是向后兼容。这个设计的一个好处是，您可以让不同的生产者和消费者同时使用不同的模式版本，并且它们仍然保持彼此兼容。</p><p>更确切地说，当Confluent的Avro序列化程序在注册表中注册模式时，它会在主题名称下注册。默认情况下，该主题<code>&lt;topic&gt;-key</code>用于消息键和<code>&lt;topic&gt;-value</code>消息值。然后，模式注册表检查在特定主题下注册的所有模式的相互兼容性。</p><p>我最近对<a href="https://github.com/confluentinc/schema-registry/pull/680" target="_blank" rel="noopener">Avro序列化程序进行了修补</a>，使兼容性检查更加灵活。该补丁添加了两个新的配置选项:(<code>key.subject.name.strategy</code>定义如何构造消息键的主题名称），以及<code>value.subject.name.strategy</code>（如何构造消息值的主题名称）。选项可以采用以下值之一：</p><ul><li><p><code>io.confluent.kafka.serializers.subject.TopicNameStrategy</code>（默认值）：消息键的主题名称是<code>&lt;topic&gt;-key</code>，<code>&lt;topic&gt;-value</code>对于消息值。这意味着主题中所有消息的模式必须相互兼容。</p></li><li><p><code>io.confluent.kafka.serializers.subject.RecordNameStrategy</code>：主题名称是邮件的Avro记录类型的完全限定名称。因此，模式注册表会检查特定记录类型的兼容性，而不考虑主题。此设置允许同一主题中的任意数量的不同事件类型。</p></li><li><p><code>io.confluent.kafka.serializers.subject.TopicRecordNameStrategy</code>：主题名称是<code>&lt;topic&gt;-&lt;type&gt;</code>，<code>&lt;topic&gt;Kafka</code>主题名称在哪里，并且<type>是邮件的Avro记录类型的完全限定名称。此设置还允许同一主题中的任意数量的事件类型，并进一步将兼容性检查限制为仅当前主题。</type></p></li></ul><p>使用此新功能，可以轻松，干净地将特定事物的所有不同事件放在同一主题中。现在，可以根据上述条件自由选择主题的粒度，而不仅限于每个主题的单个事件类型。</p><h2 id="原文："><a href="#原文：" class="headerlink" title="原文："></a>原文：</h2><p>[1] <a href="https://www.confluent.io/blog/put-several-event-types-kafka-topic/" target="_blank" rel="noopener">https://www.confluent.io/blog/put-several-event-types-kafka-topic/</a>, Martin KleppmannMartin Kleppmann</p>]]></content>
      
      
      <categories>
          
          <category> streaming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> streaming </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega控制器服务之三</title>
      <link href="/2018/09/20/pravega-controller-service-3/"/>
      <url>/2018/09/20/pravega-controller-service-3/</url>
      
        <content type="html"><![CDATA[<h2 id="角色和责任"><a href="#角色和责任" class="headerlink" title="角色和责任"></a>角色和责任</h2><h3 id="流操作"><a href="#流操作" class="headerlink" title="流操作"></a>流操作</h3><p>控制器是所有流相关元数据的真实存储。Pravega客户端（EventStreamReaders和EventStreamWriters）与控制器一起确保流不变量在流上工作时得到满足和尊重。控制器维护流的元数据，包括段的整个历史。访问流的客户端需要联系控制器以获取有关段的信息。<br>客户端查询控制器以了解如何导航流。为此，控制器公开适当的API以获取活动段，后继者，前驱者和段信息以及Uris。这些查询使用存储和通过流存储接口访问元数据来提供这些查询服务。<br>Controller还提供了修改流的状态和行为的工作流程。这些工作流程包括创建，缩放，截断，更新，密封和删除。这些工作流程既可以通过直接API调用，也可以通过后台策略管理器（自动调整和保留）调用。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FRequestProcessing.png" alt="RequestProcessing"><br>请求处理流程</p><h2 id="创建流"><a href="#创建流" class="headerlink" title="创建流"></a>创建流</h2><p>创建流是作为任务框架上的任务来实现的。创建流工作流首先创建流状态设置为CREATING * 的初始流元数据。在此之后，它标识应该拥有的段容器并为此流创建段并同时调用create-segment。完成所有创建段后，创建流任务完成，从而将流移动到ACTIVE状态。所有故障都会以指数退避重试几次。但是，如果它无法完成任何步骤，则流将保持在CREATING状态。</p><h2 id="更新流"><a href="#更新流" class="headerlink" title="更新流"></a>更新流</h2><p>更新流被实现为序列化请求处理程序/并发事件处理器框架上的任务。更新流由显式API调用来调用到控制器中。它首先将更新请求事件发布到请求流中。之后，它尝试创建临时更新属性。如果它无法创建临时更新属性，则请求失败，并且会通知调用方由于与另一个正在进行的更新冲突而无法更新流。</p><p>事件由请求事件处理器选择。处理开始时，更新流任务期望找到要存在的临时更新流属性。如果找不到该属性，则通过将事件推回到内存中队列中来延迟更新处理，直到它认为事件已过期为止。如果在此期间找到要更新的属性，则在到期之前，处理该事件并执行更新流操作。现在处理开始，它首先将状态设置为UPDATING。在此之后，在元数据存储中更新流配置，然后通知段存储关于流的所有活动段关于策略的改变。现在状态重置为ACTIVE。</p><h3 id="缩放流"><a href="#缩放流" class="headerlink" title="缩放流"></a>缩放流</h3><p>可以通过显式API调用（称为手动缩放）调用缩放，也可以基于缩放策略自动执行缩放（称为自动缩放）。我们首先编写事件，然后通过为要创建的所需段创建新条目来更新段表。此步骤是幂等的，并确保如果正在进行现有的缩放操作，则此启动新的缩放的尝试失败。处理的开始类似于更新流中遵循的机制。如果更新元数据，则事件处理并继续执行任务。如果元数据未在期望的时间范围内更新，则事件被丢弃。</p><p>一旦缩放处理开始，它首先设置流状态SCALING。然后在段存储中创建新段。在成功创建新段之后，它使用对应于新时期的部分记录更新历史表，该新时期包含按比例显示的段列表。每个新的时期创建还创建新的根历元节点，在该节点下，来自该时期的所有事务的元数据驻留在该节点上。因此，当执行比例时，将存在与旧时期对应的节点，并且现在还将存在用于新时期的根节点。从这一点开始创建的任何事务都将针对新时期进行。现在，工作流尝试通过机会性地尝试删除旧时期来完成缩放。当且仅当其名下没有事务时，可以删除旧时期。一旦我们确定旧时期没有事务，我们就可以继续密封旧的段并完成规模。成功密封旧段后，历史表中的部分记录现已完成，从而完成了缩放工作流程。状态现在重置为ACTIVE。</p><h3 id="截断流"><a href="#截断流" class="headerlink" title="截断流"></a>截断流</h3><p>Truncate遵循类似的机制进行更新，并具有用于截断的临时流属性，用于为截断流提供输入。一旦截断工作流标识它可以继续，它首先将状态设置为TRUNCATING。然后截断工作流查看请求的流截断，并检查它是否大于或等于现有截断点，然后它才是截断的有效输入并且工作流程开始。截断工作流接受请求的流截断，并计算要作为此截断请求的一部分删除的所有段。然后它调用相应的段存储来删除已识别的段。删除后，我们称在截止流中截断的流中描述的截断段，如流截断中所述。在此之后，截断记录将使用新的截断点和已删除的段进行更新。状态重置为ACTIVE。</p><h2 id="密封流"><a href="#密封流" class="headerlink" title="密封流"></a>密封流</h2><p>可以通过对控制器的显式API调用来请求密封流。它首先将密封流事件发布到请求流中，然后尝试将流的状态设置为SEALING。如果事件被挑选并且没有找到流处于期望状态，则它通过将其重新发布在内存队列的后面来推迟密封流处理。一旦流被设置为密封状态，流的所有活动段都通过调用段存储来密封。在此之后，流在流元数据中被标记为SEALED。</p><h3 id="删除流"><a href="#删除流" class="headerlink" title="删除流"></a>删除流</h3><p>可以通过对控制器的显式API调用来请求删除流。请求首先验证流是否处于SEALED状态。只有密封的流才能被删除，并在请求流中发布这样的事件。当事件被处理，它会再次验证流状态，然后通过调用段存储来继续从一开始就删除属于该流的所有段。成功删除所有段后，将清除与此流对应的流元数据。</p><h2 id="流策略管理器"><a href="#流策略管理器" class="headerlink" title="流策略管理器"></a>流策略管理器</h2><p>如前所述，控制器负责执行的用户定义策略有两种类型，即自动缩放和自动保留。Controller不仅仅是流策略的存储，而是主动为其流实施这些用户定义的策略。</p><h3 id="缩放基础架构"><a href="#缩放基础架构" class="headerlink" title="缩放基础架构"></a>缩放基础架构</h3><p>缩放基础架构与段存储一起构建。当控制器在段存储中创建新段时，它会将用户定义的缩放策略传递给段存储。然后，段存储监视所述段的流量，并且如果违反了从策略确定的某些阈值，则向控制器报告。Controller通过在专用内部流中发布的事件接收这些通知。可以为段接收两种类型的流量报告。第一种类型标识是否应该按比例放大（拆分）段，第二种类型标识是否应按比例缩小段。对于符合条件进行缩放的段，控制器会立即在请求流中发布段缩放请求，以便处理请求事件处理器。但是，为了缩小规模，控制器需要等待至少两个相邻的段，才有资格进行缩小。为此，它只是在元数据存储中将该段标记为冷。如果有相邻的段标记为冷，控制器会将它们合并，并发布缩小请求。然后，在请求事件处理器上异步执行缩放处理请求。</p><h3 id="保留基础设施"><a href="#保留基础设施" class="headerlink" title="保留基础设施"></a>保留基础设施</h3><p>保留策略定义了应为给定流保留多少数据。这可以定义为基于时间或基于大小的。为了应用此策略，控制器定期收集流的流截断，并且如果策略指定，则有机会地对先前收集的流截断执行截断。由于这是需要为已定义保留策略的所有流执行的定期后台工作，因此迫切需要在所有可用的控制器实例之间公平地分配此工作负载。为实现这一点，我们依赖于将流转储到预定义集中，并在控制器实例之间分发这些集。这是通过使用zookeeper来存储此分发来完成的。在引导期间，每个控制器实例都尝试获取存储桶的所有权。拥有控制器监视桶下的所有流以保留机会。在每个周期里，控制器收集新的流截断并将其添加到所述流的保留集中。发布此消息后，它会查找存储在保留集中的候选流截断，这些流截断可以根据定义的保留策略进行截断。例如，在基于时间的保留中，选择早于指定保留期的最新流截断作为截断点。</p><h2 id="事务管理器"><a href="#事务管理器" class="headerlink" title="事务管理器"></a>事务管理器</h2><p>控制器扮演的另一个重要角色是事务管理器。它负责创建，提交和中止事务。由于控制器是我们集群中的核心大脑和机构，并且是关于流的真实的持有者，因此writer请求控制器执行关于事务的所有控制平面动作。从创建事务到提交或中止事务的时间起，控制器在为事务提供保证方面发挥着积极作用。控制器跟踪每个事务的指定超时，如果超时超过，它会自动中止事务。</p><p>控制器负责确保事务和潜在的并行规模操作相互配合，确保所有承诺都得到尊重和执行。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FTransactionManagement.png" alt="TransactionManagement"></p><p>事务管理图</p><p>客户端调用控制器进程来创建，ping提交或中止事务。这些请求中的每一个都在控制器上接收并由Transaction Utility模块处理，该模块实现用于处理每个请求的业务逻辑。</p><h3 id="创建事务"><a href="#创建事务" class="headerlink" title="创建事务"></a>创建事务</h3><p>writer与Controller交互以创建新事务。Controller Service将创建事务请求传递给事务工具Utility模块。模块中的create transaction函数执行以下步骤以创建事务：1。为事务生成唯一的UUID。2.它从元数据存储中获取流的当前活动的一组段，并从历史中获取其对应的时期标识符。3.它使用元数据存储接口在zookeeper中创建新的事务记录。4.然后，它请求段存储创建特定的事务段，这些段本质上链接到父活动段。<br>在创建事务时，控制器确保在我们尝试创建相应的事务段时不会密封父段。并且在事务的生命周期中，如果缩放开始，它应该等待旧时期事务在缩放之前完成，以便从旧时期密封段。</p><h3 id="提交事务"><a href="#提交事务" class="headerlink" title="提交事务"></a>提交事务</h3><p>在收到提交事务的请求后，Controller Service将请求传递给Transaction Utility模块。该模块首先尝试通过元数据存储来标记事务特定元数据记录中的提交事务。在此之后，它在内部提交流中发布提交事件。提交事务工作流在提交事件处理器上实现，从而异步处理。提交事务工作流检查提交事务的是否合格，如果为true，则执行提交工作流，无限期重试，直到成功为止。如果事务不符合提交条件（通常在旧时期仍处于活动状态时在新时期上创建事务时发生），则此类事件将重新发布到内部流中以便稍后选取。</p><p>成功提交事务后，事务的记录将从其时期根下被删除。然后，如果存在一个正在进行的缩放，则它呼叫尝试完成正在进行的缩放。试图完成缩放取决于删除旧时期的能力，当且仅当没有针对所述时期的未完成的活动事务时才能删除（有关更多细节，请参阅缩放工作流程）。</p><h3 id="中止事务"><a href="#中止事务" class="headerlink" title="中止事务"></a>中止事务</h3><p>可以通过应用程序明确请求中止，类似于提交。但是，如果事务超时，则也可以自动启动中止。控制器跟踪系统中每个事务的超时，并且每当超时过去时，或者在显式用户请求时，事务实用程序模块在其各自元数据中将事务标记为中止。在此之后，事件被中止事件处理器处理，并立即尝试中止事务。中止事务没有排序要求，因此它同时并跨流执行。</p><p>与提交一样，一旦事务中止，其节点将从其时期根目录中删除，如果存在持续的缩放，则尝试完成缩放流。</p><h3 id="Ping事务"><a href="#Ping事务" class="headerlink" title="Ping事务"></a>Ping事务</h3><p>由于控制器对于正在写入事务中的段的数据没有对数据路径的可见性，因此控制器不知道是否正在主动处理事务，并且如果超时过去，它可能会尝试中止事务。为了使应用程序能够控制事务的命运，控制器公开API以允许应用程序更新事务超时期限。这种机制称为ping，只要应用程序ping事务，控制器就会为各自的事务重置其计时器。</p><h3 id="事务超时管理"><a href="#事务超时管理" class="headerlink" title="事务超时管理"></a>事务超时管理</h3><p>控制器跟踪每个事务的超时。这是作为定时轮服务实现的。创建后，每个事务都会在创建它的控制器上注册到计时器服务中。可以在不同的控制器实例上接收事务的后续ping，并且基于通过zookeeper实现的所有权机制将定时器管理转移到最新的控制器实例。超时到期后，将尝试自动中止，如果能够成功将事务状态设置为中止，则启动中止工作流。</p><p>控制器监视超时的每个事务都会添加到此进程索引中。如果此类控制器实例失败或崩溃，则其他控制器实例将收到节点失败通知，并尝试从失败的实例中扫描所有未完成的事务，并从该点开始监视其超时。</p><h2 id="段容器到主机映射"><a href="#段容器到主机映射" class="headerlink" title="段容器到主机映射"></a>段容器到主机映射</h2><p>Controller还负责将段容器分配给段存储节点。维护此映射的责任落在单个控制器实例上，该实例是通过使用zookeeper的领导者选举选择的。当段存储节点被添加到/从集群中移除时，该领导控制器监视段存储节点的生命周期，并且跨可用的段存储节点执行段容器的重新分配。此分发映射存储在专用ZNode中。每个段存储周期性地轮询此znode以查找更改，如果找到更改，它将关闭并放弃它不再拥有的容器，并尝试获取分配给它的容器的所有权。</p><p><a href="http://pravega.io/docs/latest/controller-service/#controllerClusterListener" target="_blank" rel="noopener">这里</a>已经讨论了有关实现的细节，特别是关于如何存储和管理元数据的细节。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega控制器服务之二</title>
      <link href="/2018/09/20/pravega-controller-service-2/"/>
      <url>/2018/09/20/pravega-controller-service-2/</url>
      
        <content type="html"><![CDATA[<h2 id="流元数据"><a href="#流元数据" class="headerlink" title="流元数据"></a>流元数据</h2><p>客户端需要有关哪些段构成流的信息以开始其处理，并且他们从控制器存储在流存储中的时期信息中获取它。读取器客户端通常从流的头部开始，但它也可以选择从任意有趣的位置开始访问流。另一方面，writer总是附加到流的尾部。<br>客户端需要能够有效地查询和查找在以下三种场景中的任何一个段。为了启用这些查询，流存储提供API调用来获取这些段的初始集合、在特定时间获取段以及获取段的当前集合。</p><p>如前所述，流可以从一组段（epoch）转换到构成流的另一组段。如果至少一个段被密封，并且被一个或多个精确覆盖密封段的密钥空间的段替换，则流从一个时期移动到另一个时期。当客户在流上工作时，他们可能会遇到密封段的末端，因此需要找到新的段才能继续前进。为了使客户端能够查询下一个段，流存储库通过控制器服务公开有效查询，以查找任意段的直接后继和前驱。</p><p>为了启用上述服务查询，我们需要有效地存储这些段转换的时间序列，并将它们与时间进行索引。我们在一组表中存储有关流段的当前和历史状态的信息，这些表被设计为旨在针对上述查询进行优化。除了特定于段的元数据记录之外，流的当前状态包括此后描述的其他元数据类型。</p><h2 id="表"><a href="#表" class="headerlink" title="表"></a>表</h2><p>为了有效地存储和查询段信息，我们将段数据拆分为三个仅附加表，即段表，历史表和索引表。</p><ul><li><p>段表<br>segment-info：segmentid，time，keySpace-start，keySpace-end<br>控制器将段表存储在仅附加表中，第 i 行对应于段id i的元数据。值得注意的是，段表中的每一行都是固定大小的。当添加新的段时，它们将按严格增加的顺序分配新的段ID。因此，该表非常有效地创建新段并使用O（1）处理来查询段信息响应方面非常有效。</p></li><li><p>历史表<br>epoch：时间，历史中的片段列表<br>历史表在从一个时期过渡到另一个时期时存储一系列活动段。历史表中的每一行都存储一个epoch，该epoch捕获一组逻辑上一致的（如前面定义的）段，这些段组成流，并且在该epoch的生命周期内是有效的。此表旨在优化查询以查找在任意时间形成流的段集。有三种最常用的场景，我们希望有效地知道形成流的段集 - 初始的段集，当前的段和段的任意时间段。前两个查询在O（1）时间内非常有效地回答，因为它们对应于表中的第一行和最后一行。由于表中的行是按时间的增加顺序排序的，并且捕获了流段集变化的时间序列，因此我们可以很容易地执行二进制搜索来查找在任何任意时间对应于段集的行。</p></li></ul><ul><li>索引表<br>index：⟨time，offset-in-history-table⟩<br>由于历史行的长度是可变的，因此我们为索引表中的时间戳索引历史记录行。这使我们能够浏览历史表并执行二分查找以有效地回答查询以在任意时间获得段集。我们还对历史表执行二分查找以确定任何给定段的后继。</li></ul><h2 id="流配置"><a href="#流配置" class="headerlink" title="流配置"></a>流配置</h2><p>Znode，其中流配置被序列化并持久化。流配置包含需要强制实施的流策略。缩放策略和保留策略由应用程序在创建流时提供，并由控制器通过监视流中数据的速率和大小来强制执行。缩放策略描述了是否以及何时根据流中的传入流量条件自动缩放。该策略支持两种风格 -每秒事件的速率的流量和每秒的字节速率的流量。应用程序通过缩放策略将它们所需的流量指定到每个段，并选择所提供的值来计算确定何时缩放给定流的阈值。保留策略描述了需要保留到此流的pravega集群中的数据量。我们支持基于时间和基于大小的保留策略，其中应用程序可以选择是否希望通过选择适当的策略并提供其所需值来按大小或时间保留流中的数据。</p><h2 id="流状态"><a href="#流状态" class="headerlink" title="流状态"></a>流状态</h2><p>Znode捕获流的状态。它是一个枚举，包含来自创建，活动，更新，缩放，截断，密封和密封流的值。一旦激活，流在执行特定操作和活动之间转换，直到它被密封。转换映射在State 类中定义 ，允许和禁止各种状态转换。流状态描述了流的当前状态。它基于在流上执行的动作从ACTIVE转换到相应的动作。例如，在缩放期间，流的状态从ACTIVE转换为SCALING一旦缩放完成，它就会转换回ACTIVE。流状态用作屏障，以确保在任何时间点仅对给定流执行一种类型的操作。仅允许某些状态转换，并在状态转换对象中进行描述。只允许合法的状态转换，任何不允许转换的尝试都会导致适当的异常。</p><h2 id="截断记录"><a href="#截断记录" class="headerlink" title="截断记录"></a>截断记录</h2><p>这对应于最后用于截断给定流的流截断。所有流段查询都会叠加截断记录并返回严格大于或等于截断记录中的流截断的段。</p><h2 id="密封段记录"><a href="#密封段记录" class="headerlink" title="密封段记录"></a>密封段记录</h2><p>由于段表仅附加，因此在密封段时我们需要保留的任何其他信息都存储在密封段记录中。目前，它简单地包含了段号到其密封大小的映射。</p><h2 id="与事务相关的元数据记录："><a href="#与事务相关的元数据记录：" class="headerlink" title="与事务相关的元数据记录："></a>与事务相关的元数据记录：</h2><h3 id="活动事务"><a href="#活动事务" class="headerlink" title="活动事务"></a>活动事务</h3><p>每个新事务都是在此Znode下创建的。这将与每个事务相对应的元数据存储为ActiveTransactionRecord。事务完成后，将在全局完成事务znode节点下创建一个新节点，并从流特定活动事务节点下删除该节点。</p><h3 id="完成事务"><a href="#完成事务" class="headerlink" title="完成事务"></a>完成事务</h3><p>完成后，所有流的所有已完成事务都将在此单个znode下移动（通过提交或中止路径）。随后，我们可以根据我们认为合适的任何收集方案定期回收这些值。不过在这一点上，我们此时尚未实施任何计划。</p><h2 id="流存储缓存"><a href="#流存储缓存" class="headerlink" title="流存储缓存"></a>流存储缓存</h2><p>由于同一个控制器实例可以处理给定流的多个并发请求，因此每次通过查询zookeeper来读取该值是不合理的。因此，我们引入了每个流存储维护的内存缓存。它缓存每个流的检索元数据，使得缓存中每个流最多有一个数据副本。我们有两个内存缓存 - a）存储中多个流对象的缓存，b）流对象中流的缓存属性。</p><p>我们引入了操作上下文的概念，并且在任何新操作开始时创建了新的操作上下文。新操作上下文的创建使流的高速缓存实体无效，并且每当请求时都从存储中懒惰地检索每个实体。如果在操作过程中更新了值，则该值在缓存中再次无效，以便流上的其他并发读取/更新操作获取其后续步骤的新值。<br>流桶</p><p>为了启用某些场景，我们可能需要我们的后台工作人员定期处理群集中的每个流，以对它们执行某些特定操作。我们引入了一个桶的概念，以便在所有可用的控制器实例中分发此定期后台工作。为此，我们将每个流散列到一个预定义的存储桶中，然后在可用的控制器实例之间分配存储桶。<br>群集的桶数是群集生命周期的固定（可配置）值。<br>控制器实例将系统中的所有可用流映射到桶中，并在它们之间分配桶，以便所有长时间运行的后台工作可以在多个控制器实例之间均匀分布。每个桶对应于zookeeper中的唯一Znode。完全限定范围流名称用于计算散列以将流分配给桶。所有控制器实例在启动时都会尝试获取存储桶的所有权。在故障转移时，所有权都会转移，因为幸存的节点竞争以获取孤立桶的所有权。拥有存储桶的控制器实例负责与存储桶下所有节点相对应的所有长时间运行的调度后台工作。目前，这需要运行周期性工作流来捕获每个流的流截断（称为保留集）。</p><h2 id="保留集"><a href="#保留集" class="headerlink" title="保留集"></a>保留集</h2><p>每个流的一个保留集存储在相应的桶/流Znode下。当我们定期计算流截断时，我们会在此Znode下保留它们。在执行某些自动截断时，将从此集中清除不再有效的流截断。</p><h2 id="控制器群集Listener"><a href="#控制器群集Listener" class="headerlink" title="控制器群集Listener"></a>控制器群集Listener</h2><p>Pravega 集群中的每个节点都在集群Znode下作为短暂节点注册。这包括控制器和段存储节点。每个控制器实例在集群Znode上注册监视，以侦听集群更改通知。这些通知是关于节点添加和删除的。</p><p>一个控制器实例承担所有控制器实例的领导。此领导者控制器实例负责处理段存储节点更改通知。根据拓扑结构的变化，控制器实例会定期将段容器重新平衡为段存储节点映射。</p><p>所有控制器实例都侦听控制器节点更改通知。每个控制器实例都有多个子组件，用于实现故障转移 - 清除程序接口。目前有三个组件实现故障转移清除程序接口，即 TaskSweeper，EventProcessors和TransactionSweeper。每当识别出控制器实例已从群集中删除时，群集侦听器将调用所有已注册的故障转移清除程序，以便乐观地尝试清除先前由故障控制器主机拥有的所有孤儿工作。</p><h3 id="主机存储"><a href="#主机存储" class="headerlink" title="主机存储"></a>主机存储</h3><p>主机存储接口用于将Segment Container存储到Segment Store节点映射。它公开了像getHostForSegment这样的API，它计算了段ID的一致哈希值来计算所有者Segment Container。然后基于容器 - 主机映射，它将适当的URL返回给调用者。</p><h2 id="后台工作者"><a href="#后台工作者" class="headerlink" title="后台工作者"></a>后台工作者</h2><p>控制器进程有两种不同的机制/框架来处理后台工作。这些后台工作通常需要多个步骤和更新特定元数据根实体下的元数据，以及与一个或多个段存储的潜在交互。</p><p>首先，我们从一个简单的任务框架开始，该框架允许运行对给定资源（通常是流）拥有独占权的任务，并允许任务从一个控制器实例故障转移到另一个控制器实例。然而，这个模型限制了它的范围和锁定语义，并且没有固有的任务排序概念，因为多个任务可以竞争地同时获取资源上的工作权限（锁定），并且其中任何一个都可以成功。</p><p>为了克服这个限制，我们提出了一种新基础架构，称为Event Processor。事件处理器是经典的自己的狗食自己吃。它使用pravega流建造。这为我们提供了一个简洁的机制，以确保互斥和有序的处理。</p><h3 id="任务框架"><a href="#任务框架" class="headerlink" title="任务框架"></a>任务框架</h3><p>任务框架被设计为在每个资源上运行独占的后台处理， 以便在控制器实例失败的情况下，工作可以轻松地转移到另一个控制器实例并完成。框架本身并不保证幂等处理，并且如果需要，任务的作者必须处理它。任务模型被定义为只在给定资源上专门工作，这意味着没有其他任务可以在同一资源上并发运行。这是通过在zookeeper上实现的持久分布式锁实现的。任务的故障转移是通过遵循索引给定进程正在执行的工作的索引方案来实现的。因此，如果一个流程失败，另一个流程将扫描所有未完成的工作并尝试将所有权转移给自己。注意：控制器进程失败时，多个幸存的控制器进程可以同时尝试扫描孤立的任务。它们中的每一个都将在其主机索引中索引任务，但只有其中一个能够成功获取对资源的锁定，从而获得处理任务的权限。执行任务的参数被序列化并存储在资源下。</p><p>目前，我们仅将任务框架用于创建流任务。所有其他后台处理都是使用事件处理器框架完成的。</p><h2 id="事件处理器框架"><a href="#事件处理器框架" class="headerlink" title="事件处理器框架"></a>事件处理器框架</h2><p>事件处理器框架是后台工作子系统，它从内部流中读取事件并对其进行处理，因此称为事件处理器。我们系统中的所有事件处理器至少 提供一次 处理保证。并且在其基本风格方面，该框架还提供强大的顺序保证。但我们也有不同的事件处理器子类型，允许并发处理。</p><p>我们为不同类型的工作创建不同的事件处理器。目前，我们的系统中有三个不同的事件处理器，用于提交事务，中止事务和处理流特定请求，如扩展更新密封等。每个控制器实例都有一个每种类型的事件处理器。事件处理器框架允许为每个事件处理器创建多个读取器。跨控制器实例的特定事件处理器的所有读者共享相同的读取器组，这保证了跨控制器实例的互斥分配工作。每个读者都获得一个专用线程，在该线程中，它读取事件，调用其处理并在完成处理后更新其检查点。事件被发布在事件处理器特定的流中，并且基于使用作用域流名称作为路由密钥被路由到特定的段。</p><p>我们有两种类型的事件处理器，一种执行串行处理，这基本上意味着它会读取一个事件并启动它的处理，并等待它完成后再继续进行下一个事件。这为处理过程提供了强有力的顺序保证。处理完每个事件后的检查点。提交事务是使用事件处理器的这种基本风格实现的。处理这些事件的并行度上限为内容流中段数，下限为读者取器数量的限制。来自不同流的多个事件可以在同一段中出现，并且由于我们执行串行处理，串行处理的缺点是处理停顿或来自一个流的事件泛滥会对不相关流的延迟产生不利影响。</p><p>为了克服这些缺点，我们设计了Concurrent Event Processor作为串行事件处理器的叠加。顾名思义，并发事件处理器允许我们同时处理多个事件。这里读者线程读取一个事件，调度它的异步处理并返回读取下一个事件。在任何时间点同时处理的事件数量都有上限，并且当某个事件的处理完成时，允许获取更新的事件。这里的检查点方案变得更加复杂，因为我们希望保证至少一次处理。</p><p>但是，随着并发处理，顺序保证会被破坏。但是，重要的是要注意，我们只需要顺序保证来处理来自流而不是跨流的事件提供顺序保证。为了满足排序保证，我们将Concurrent Event Processor与Serialized Request Handler重叠，后者将来自内存队列中相同流的事件排队并按顺序处理它们。</p><p>提交事务处理是在专用串行事件处理器上实现的，因为我们需要提交顺序的强力保证，同时确保提交不会干扰流上其他类型请求的处理。</p><p>中止事务处理是在专用并发事件处理器上实现的，该处理器同时对来自跨流的事务执行中止处理。</p><p>对流的所有其他请求都在序列化请求处理程序上实现，该处理程序确保在任何给定时间正在处理每个流的一个请求，并且在请求处理期间存在排序保证。但是，它允许来自跨流的并发请求同时进行。实现缩放，截断，密封，更新和删除流等工作流程，以便在请求事件处理器上进行处理。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega控制器服务之一</title>
      <link href="/2018/09/20/pravega-controller-service-1/"/>
      <url>/2018/09/20/pravega-controller-service-1/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>控制器服务是Pravega的核心组件，用于实现控制平面。它充当集群中执行的各种操作的中央协调器和管理器，主要分为两类：</p><ol><li>流管理</li><li>集群管理</li></ol><p>控制器服务，此后简称为控制器，负责提供<a href="http://pravega.io/docs/latest/pravega-concepts/#streams" target="_blank" rel="noopener">流</a>的抽象，这是Pravega向应用程序公开的主要抽象。流包括一个或多个<a href="http://pravega.io/docs/latest/pravega-concepts/#stream-segments" target="_blank" rel="noopener">段</a>。每个段都是仅附加数据结构，用于存储字节序列。一个段本身与其他段的存在无关，并且不知道它与其对等段的逻辑关系。拥有和管理这些段的段存储没有任何流的概念。流是由Controller概念化的逻辑视图通过组合动态变化的一组段来满足一组预定义的逻辑不变量。控制器提供流抽象并协调流上的所有生命周期操作，同时确保抽象保持一致。</p><p>控制器在流的生命周期中起着核心作用：创建，修改，<a href="http://pravega.io/docs/latest/pravega-concepts/#autoscalingthenumber-of-stream-segments-can-vary-over-time" target="_blank" rel="noopener">缩放</a>和删除。它通过维护每个流的元数据并在必要时对段执行必要的操作来完成这些操作。例如，作为流生命周期的一部分，可以创建新的段并密封现有的段。控制器决定何时执行这些操作，使得流继续可用并且对访问它们的客户端是一致的。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>控制器服务由一个或多个无状态工作节点实例组成。每个新的控制器实例都可以独立启动，并成为pravega集群的一部分，它只需要指向相同的Apache Zookeeper。对于高可用性，建议是每个群集具有不止一个控制器服务实例。</p><p>每个控制器实例都能够独立工作，并使用一个共享的持久存储作为控制器服务所拥有和管理的所属状态的真实来源。目前使用Apache ZooKeeper作为持久存储来保存所有的元数据。每个实例包括各种子系统，其负责对不同类别的元数据执行特定操作。这些子系统包括不同的API端点，元数据存储句柄，策略管理器和后台工作程序。</p><p>控制器暴露两个端点，这些端点可用于与控制器服务交互。第一个端口用于为pravega客户端提供编程访问，并使用gRPC实现为RPC。另一个端点用于管理操作，并作为REST端点实现。</p><h2 id="流管理"><a href="#流管理" class="headerlink" title="流管理"></a>流管理</h2><p>控制器拥有并管理流的概念，并负责维护每个流的元数据和生命周期。具体而言，它负责创建，更新，缩放，截断，密封和删除流。<br>流管理可以大致分为三类：</p><ol><li><p>流抽象<br>流可以被视为一系列动态变化的段集，其中流从一组一致的段转换到下一个。Controller是创建和管理此流抽象的地方。控制器决定流何时以及如何从一种状态转换到另一种状态，并负责执行这些转换，同时保持流的状态一致且可用。这些转换是受控制器强制执行的用户定义策略的支配。因此，作为流管理的一部分，控制器还会执行策略管理器的角色，以实现保留和缩放等策略。</p></li><li><p>自动策略管理<br>控制器负责通过主动监视流的状态来存储和实施用户定义的流策略。目前，我们有两个用户可以定义的策略，即<a href="https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/ScalingPolicy.java" target="_blank" rel="noopener">缩放策略</a>和 <a href="https://github.com/pravega/pravega/blob/master/client/src/main/java/io/pravega/client/stream/RetentionPolicy.java" target="_blank" rel="noopener">保留策略</a>。缩放策略描述了流是否以及在何种情况下应自动缩放其段数。保留策略描述了有关在流中保留多少数据的策略。</p></li><li><p><a href="http://pravega.io/docs/latest/pravega-concepts/#transactions" target="_blank" rel="noopener">事务</a>管理<br>实现事务需要操作段。对于每个事务，Pravega创建一组事务段，这些事务段稍后在提交时合并到流段上或在中止时丢弃。控制器执行事务管理器的角色，负责在给定流上创建和提交事务。在创建事务时，控制器还跟踪事务超时并中止超时已过期的事务。事务管理的细节可以在文档的后面找到。</p></li></ol><h2 id="集群管理"><a href="#集群管理" class="headerlink" title="集群管理"></a>集群管理</h2><p>控制器负责管理段存储集群。控制器管理段存储节点的生命周期，因为它们被添加到群集/从群集中删除，并在可用的段存储节点上执行段容器的重新分发。</p><h2 id="系统图"><a href="#系统图" class="headerlink" title="系统图"></a>系统图</h2><p>下图显示了控制器进程的主要组件。我们接下来将详细讨论该图表的元素。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FControllerSystemDiagram.png" alt="ControllerSystemDiagram"></p><p>控制器流程图</p><h1 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h1><h3 id="服务端点"><a href="#服务端点" class="headerlink" title="服务端点"></a>服务端点</h3><p>控制器公开了两个端口：客户端控制器API和管理API。客户端控制器通信被实现为RPC，RPC公开API以执行所有与流相关的控制平面操作。除此控制器外，还公开了一个作为REST实现的管理API集。</p><p>每个端点都对Controller Service后端子系统执行适当的调用，该控制器服务子系统 具有对控制器拥有和管理的实体进行各种创建，读取，更新和删除（CRUD）操作的实际实现。</p><h3 id="GRPC"><a href="#GRPC" class="headerlink" title="GRPC"></a>GRPC</h3><p>客户端控制器通信端点实现为gRPC 接口。完整的API列表可以在<a href="https://github.com/pravega/pravega/blob/master/shared/controller-api/src/main/proto/Controller.proto" target="_blank" rel="noopener">此处</a>找到 。这暴露了Pravega客户端（读者，写者和流管理器）使用的API，并启用了流管理。此API启用的请求包括创建，修改和删除流。底层gRPC框架提供同步和异步编程模型。我们在客户端控制器交互中使用异步模型，以便客户端线程不会阻止来自服务器的响应。</p><p>为了能够附加和读取来自流，writer和reader的数据，查询控制器以在使用流时获得活动的段集，后继段和前置段。对于事务，客户端使用特定的API调用来请求控制器创建和提交事务。</p><h3 id="REST"><a href="#REST" class="headerlink" title="REST"></a>REST</h3><p>对于管理，控制器实现并公开REST接口。这包括用于流管理的API调用以及主要处理范围创建和删除的其他管理API。我们使用swagger来描述我们的REST API。<a href="https://github.com/pravega/pravega/tree/master/shared/controller-api/src/main/swagger" target="_blank" rel="noopener">这里</a>可以找到swagger的yaml文件。</p><h3 id="控制器服务"><a href="#控制器服务" class="headerlink" title="控制器服务"></a>控制器服务</h3><p>Controller服务是控制器端点（gRPC和REST）后面的后端层。提供控制器API调用所需的所有业务逻辑都在此处实现。该层包含所有其他子系统的句柄，如各种存储实现（流存储，主机存储和检查点存储）和后台处理框架（任务框架，事件处理器框架）。存储是提供对Controller管理的各种类型元数据的访问的接口。后台处理框架用于执行异步处理，该异步处理通常实现涉及元数据更新和对分段存储的请求的工作流。</p><h3 id="流元数据存储"><a href="#流元数据存储" class="headerlink" title="流元数据存储"></a>流元数据存储</h3><p>流是动态改变的段序列，其中路由键空间的区域映射到开放段。随着流的段的集合发生变化，路由密钥空间到段的映射也会发生变化。<br>如果1）映射到集合中的段的关键空间区域的并集覆盖整个密钥空间，则该组段是一致的。2）密钥空间区域之间没有重叠。例如，假设集合S = { S 1，S 2，S 3 }，使得： - 区域[0,0.3]映射到区段S 1 - 区域[0.3,0.6]映射到区段S 2 - 区域[0.6 ，1.0）映射到段S 3<br>S是一致的段集。</p><p>随着时间的推移，流会经历转换。流以初始的段集合开始，这些初始段集合在创建时由流配置确定，并且随着对流执行缩放操作时转换为新的段集。在任何给定时间点构成流的每一段被认为属于一个时期。因此，流以初始时期开始，该初始时期是时期0，并且在每次转换时，它在其时期中向前移动以描述流中的段的生成的变化。</p><p>控制器维护流存储关于构成给定流的所有时期以及它们如何转换的信息。该存储旨在优化存储和查询与段及其相互关系相关的信息。<br>除了时期信息之外，它还保留了一些额外的元数据，例如状态及其策略以及流上的持续事务。</p><p>控制器的各种子组件通过定义良好的接口访问每个流的存储元数据 。我们目前有两个具体的流存储接口实现：内存和zookeeper支持的存储。两者共享一个公共的基础实现，该实现依赖于流对象，为所有特定于流的元数据提供特定于存储类型的实现。流存储的基本实现创建并缓存这些流对象。</p><p>流对象实现存储/流接口。具体的流实现特定于存储类型的，并负责实现存储特定的方法，以提供一致性和正确性。我们有一个提供乐观并发的所有存储类型的通用基础实现。此基类封装了针对支持Compare和Swap（CAS）的所有具体存储的流存储查询的逻辑。我们目前使用zookeeper作为我们的底层存储，它也支持CAS。我们在流特定的znodes（ZooKeeper数据节点）下以分层方式存储所有流元数据。</p><p>对于基于ZooKeeper的存储，我们将元数据组织到不同的组中，以支持针对此元数据的各种查询。所有特定于流的元数据都存储在作用域/流根节点下。针对该元数据的查询包括（但不限于）查询在不同时间点形成流的段集，段特定信息，段前驱和后继。有关流元数据存储公开的API的详细信息，请参阅流元数据接口。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>flink 集群与部署之kubernetes篇</title>
      <link href="/2018/09/20/flink-deployment-kubernetes/"/>
      <url>/2018/09/20/flink-deployment-kubernetes/</url>
      
        <content type="html"><![CDATA[<p>本页介绍如何在Kubernetes上部署Flink作业和会话群集。</p><h2 id="设置Kubernetes"><a href="#设置Kubernetes" class="headerlink" title="设置Kubernetes"></a>设置Kubernetes</h2><p>请参照<a href="https://kubernetes.io/docs/setup/" target="_blank" rel="noopener">Kubernetes的设置指南</a>来部署Kubernetes集群。如果您想在本地运行Kubernetes，我们建议使用<a href="https://kubernetes.io/docs/setup/minikube/" target="_blank" rel="noopener">MiniKube</a>来部署集群。</p><blockquote><p>注意：如果使用MiniKube，请确保<code>minikube ssh &#39;sudo ip link set docker0 promisc on&#39;</code>在部署Flink群集之前执行。否则，Flink组件无法通过Kubernetes服务自行引用。</p></blockquote><h2 id="Kubernetes上的Flink会话群集"><a href="#Kubernetes上的Flink会话群集" class="headerlink" title="Kubernetes上的Flink会话群集"></a>Kubernetes上的Flink会话群集</h2><p>Flink会话群集作为长期运行的Kubernetes部署来执行，请注意，可以在会话群集上运行多个Flink作业。在部署了集群之后，每个作业都需要提交到群集。</p><p>一个基本的部署在Kubernetes上的Flink会话群集一般会有三个组件：</p><ul><li>一个运行JobManager的deployment或job</li><li>一个TaskManagers池 deployment</li><li>一个公开JobManager的REST和UI端口的service</li></ul><h2 id="在Kubernetes上部署Flink会话群集"><a href="#在Kubernetes上部署Flink会话群集" class="headerlink" title="在Kubernetes上部署Flink会话群集"></a>在Kubernetes上部署Flink会话群集</h2><p>使用会话群集的资源定义，采用kubectl命令启动群集：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl create -f jobmanager-service.yaml<br>kubectl create -f jobmanager-deployment.yaml<br>kubectl create -f taskmanager-deployment.yaml<br></code></pre></td></tr></table></figure><p>然后，您可以通过kubectl proxy按以下方式访问Flink UI ：</p><p>第一步，保证kubectl proxy在终端中运行</p><p>第二步，在浏览器里输入 <code>http://localhost:8001/api/v1/namespaces/default/services/flink-jobmanager:ui/proxy</code></p><p>如果要终止Flink会话群集，可以使用如下命令：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl delete -f jobmanager-deployment.yaml<br>kubectl delete -f taskmanager-deployment.yaml<br>kubectl delete -f jobmanager-service.yaml<br></code></pre></td></tr></table></figure><h2 id="Kubernetes上的Flink作业集群"><a href="#Kubernetes上的Flink作业集群" class="headerlink" title="Kubernetes上的Flink作业集群"></a>Kubernetes上的Flink作业集群</h2><p>Flink作业集群是运行单个作业的专用集群，这项作业是打包在flink镜像里的，因此，不需要提交额外的作业对象，步骤如下：</p><h3 id="创建特定于作业的镜像"><a href="#创建特定于作业的镜像" class="headerlink" title="创建特定于作业的镜像"></a>创建特定于作业的镜像</h3><p>Flink作业集群镜像需要包含启动集群的作业的用户代码jar。因此，需要为每个作业构建专用的容器镜像。请按照这些<a href="https://github.com/apache/flink/blob/master/flink-container/docker/README.md" target="_blank" rel="noopener">说明</a>构建Docker镜像。</p><h3 id="在Kubernetes上部署Flink作业集群"><a href="#在Kubernetes上部署Flink作业集群" class="headerlink" title="在Kubernetes上部署Flink作业集群"></a>在Kubernetes上部署Flink作业集群</h3><p>要在Kubernetes上部署作业集群，请按照这些<a href="https://github.com/apache/flink/blob/master/flink-container/kubernetes/README.md#deploy-flink-job-cluster" target="_blank" rel="noopener">说明</a>进行操作。</p><h2 id="高级群集部署"><a href="#高级群集部署" class="headerlink" title="高级群集部署"></a>高级群集部署</h2><p>GitHub上提供了早期版本的<a href="https://github.com/docker-flink/examples" target="_blank" rel="noopener">Flink Helm chart</a>。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="会话群集资源定义"><a href="#会话群集资源定义" class="headerlink" title="会话群集资源定义"></a>会话群集资源定义</h2><p>部署使用的最新镜像 <code>flink:latest</code> 可在<a href="https://hub.docker.com/r/_/flink/" target="_blank" rel="noopener">Docker Hub</a>上找到。该镜像是用这个工具 <code>https://github.com/docker-flink/docker-flink</code> 构建的</p><p>jobmanager-deployment.yaml</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs undefined">&quot;<br>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  name: flink-jobmanager<br>spec:<br>  replicas: 1<br>  template:<br>    metadata:<br>      labels:<br>        app: flink<br>        component: jobmanager<br>    spec:<br>      containers:<br>      - name: jobmanager<br>        image: flink:latest<br>        args:<br>        - jobmanager<br>        ports:<br>        - containerPort: 6123<br>          name: rpc<br>        - containerPort: 6124<br>          name: blob<br>        - containerPort: 6125<br>          name: query<br>        - containerPort: 8081<br>          name: ui<br>        env:<br>        - name: JOB_MANAGER_RPC_ADDRESS<br>          value: flink-jobmanager<br>&quot;<br></code></pre></td></tr></table></figure><p>taskmanager-deployment.yaml</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs undefined">&quot;<br>apiVersion: extensions/v1beta1<br>kind: Deployment<br>metadata:<br>  name: flink-taskmanager<br>spec:<br>  replicas: 2<br>  template:<br>    metadata:<br>      labels:<br>        app: flink<br>        component: taskmanager<br>    spec:<br>      containers:<br>      - name: taskmanager<br>        image: flink:latest<br>        args:<br>        - taskmanager<br>        ports:<br>        - containerPort: 6121<br>          name: data<br>        - containerPort: 6122<br>          name: rpc<br>        - containerPort: 6125<br>          name: query<br>        env:<br>        - name: JOB_MANAGER_RPC_ADDRESS<br>          value: flink-jobmanager<br>&quot;<br></code></pre></td></tr></table></figure><p>jobmanager-service.yaml</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs undefined">&quot;<br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: flink-jobmanager<br>spec:<br>  ports:<br>  - name: rpc<br>    port: 6123<br>  - name: blob<br>    port: 6124<br>  - name: query<br>    port: 6125<br>  - name: ui<br>    port: 8081<br>  selector:<br>    app: flink<br>    component: jobmanager<br>&quot;<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>flink 集群与部署之docker篇</title>
      <link href="/2018/09/20/flink-deployment-docker/"/>
      <url>/2018/09/20/flink-deployment-docker/</url>
      
        <content type="html"><![CDATA[<h2 id="作者标注"><a href="#作者标注" class="headerlink" title="作者标注"></a>作者标注</h2><p>经过验证， 到当前版本为止 flink-1.7 snapshot，构建 flink docker镜像需要采用这个flink docker 构建工具 <code>https://github.com/docker-flink/docker-flink</code>，按照<a href="https://github.com/apache/flink/tree/master/flink-container" target="_blank" rel="noopener">flink官方代码库</a>里的构建出来的flink镜像有些功能不能用，比如 flink-standalone模式，report metrics等。</p><h2 id="Docker设置"><a href="#Docker设置" class="headerlink" title="Docker设置"></a>Docker设置</h2><p>Docker Hub上有关于Apache Flink的Docker镜像，可用于部署flink群集。Flink镜像库还包含用于创建容器映像以部署flink工作集群的一些工具以及说明。</p><h2 id="Flink-session群集"><a href="#Flink-session群集" class="headerlink" title="Flink session群集"></a>Flink session群集</h2><p>Flink会话群集可用于运行多个业务。在部署后，每个业务都需要提交到集群才能跑起来。</p><h2 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h2><p>该<a href="https://hub.docker.com/_/flink/" target="_blank" rel="noopener">Flink镜像库</a>托管在docker hub，提供了flink1.2.1以及之后的版本镜像。</p><p>注意： Docker镜像是由个人提供的社区项目，它们并不是Apache Flink PMC的官方版本（作者标注：所以需要用这个个人的<a href="https://github.com/docker-flink/docker-flink" target="_blank" rel="noopener">构建工具</a>，而不是官方代码库里的构建工具）。</p><h2 id="Flink作业集群"><a href="#Flink作业集群" class="headerlink" title="Flink作业集群"></a>Flink作业集群</h2><p>Flink作业集群是运行单个作业的专用集群，这是镜像内容的一部分，因此，不需要额外的工作。</p><h2 id="Docker镜像-1"><a href="#Docker镜像-1" class="headerlink" title="Docker镜像"></a>Docker镜像</h2><p>Flink作业集群镜像需要包含启动集群的作业的用户代码jar。因此，需要为每个作业构建专用的容器镜像。该flink-container模块包含一个build.sh脚本，可用于创建此类镜像。有关详细信息，请参阅<a href="https://github.com/apache/flink/blob/master/flink-container/docker/README.md" target="_blank" rel="noopener">说明</a>。（作者注：这个是官方的构建方式，试过有问题，比如跑 flink-standalone再 report metrics）</p><h2 id="Flink与Docker-Compose"><a href="#Flink与Docker-Compose" class="headerlink" title="Flink与Docker Compose"></a>Flink与Docker Compose</h2><p>Docker Compose是一种很方便的用于在本地启动一组Flink Docker容器的方式。</p><p>GitHub上提供了<a href="https://github.com/docker-flink/examples/blob/master/docker-compose.yml" target="_blank" rel="noopener">集群部署实例</a>和<a href="https://github.com/apache/flink/blob/master/flink-container/docker/docker-compose.yml" target="_blank" rel="noopener">作业群集示例</a>的配置文件。</p><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><p>启动集群</p><p><code>$docker-compose up</code></p><p>以deamon的方式启动集群</p><p> <code>$docker-compose up -d</code></p><p>集群扩展 N 个 TaskManagers</p><p><code>$docker-compose scale taskmanager=&lt;N&gt;</code></p><p>销毁集群</p><p><code>$docker-compose kill</code></p><p>当拉起一个Flink群集后，您可以访问 <code>http：// localhost：8081</code>的Web UI ，在界面里您还可以将作业提交到群集。</p><p>如果要通过命令行将作业提交到会话群集，必须将JAR复制到JobManager容器里并从那里执行作业。</p><p>例如：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ JOBMANAGER_CONTAINER=$(docker ps --filter name=jobmanager --format=&#123;&#123;.ID&#125;&#125;)<br>$ docker cp path/to/jar &quot;$JOBMANAGER_CONTAINER&quot;:/job.jar<br>$ docker exec -t -i &quot;$JOBMANAGER_CONTAINER&quot; flink run /job.jar<br></code></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega段容器</title>
      <link href="/2018/09/19/pravega-segment-containers/"/>
      <url>/2018/09/19/pravega-segment-containers/</url>
      
        <content type="html"><![CDATA[<h2 id="Pravega集群中的段容器"><a href="#Pravega集群中的段容器" class="headerlink" title="Pravega集群中的段容器"></a>Pravega集群中的段容器</h2><p>本文档描述了我们如何管理Pravega集群中Segment Containers生命周期的高级设计。</p><h2 id="平衡段容器"><a href="#平衡段容器" class="headerlink" title="平衡段容器"></a>平衡段容器</h2><h2 id="段容器"><a href="#段容器" class="headerlink" title="段容器"></a>段容器</h2><p>在本文档中，我们将段容器称为容器。对于给定的部署，容器的总数是固定的。每个容器只能由一个Pravega主机拥有，并且群集中的所有容器都应该在任何时刻都是运行着的。</p><h2 id="Pravega主机"><a href="#Pravega主机" class="headerlink" title="Pravega主机"></a>Pravega主机</h2><p>pravega主机是pravega服务的一个实例，它拥有并执行一组容器。</p><h2 id="检测活动的Pravega主机"><a href="#检测活动的Pravega主机" class="headerlink" title="检测活动的Pravega主机"></a>检测活动的Pravega主机</h2><p>启动时每个pravega主机都会使用临时节点向Zookeeper注册。只要zookeeper从Pravega主机接收到适当的心跳，临时的节点就会出现在zookeeper中。我们依靠这些临时的节点来检测哪些pravega主机当前在群集中是活动的。</p><h2 id="监控Pravega集群"><a href="#监控Pravega集群" class="headerlink" title="监控Pravega集群"></a>监控Pravega集群</h2><p>每个Pravega Controller都运行一项服务，监控zookeeper上的临时节点并检测集群中所有活动的pravega主机。在检测到对集群成员资格的任何更改时，我们会验证所有容器并将其重新映射到可用的pravega主机集。此信息以原子方式保存在HostStore中。它存储为单个blob，并包含主机拥有的主机到容器集的映射。</p><p>我们使用zookeeper来确保只有一个pravega控制器监视集群，以避免多个同时写入HostStore的写入器。这将避免竞争条件，并允许状态更快地收敛。</p><h2 id="再平衡频率"><a href="#再平衡频率" class="headerlink" title="再平衡频率"></a>再平衡频率</h2><p>当从群集中添加或删除pravega主机时，会发生容器所有权的重新平衡。由于这是一项代价高昂的操作，我们需要防止经常这样做。目前，我们确保在任何2次重新平衡操作之间保持配置的最小时间间隔。在最坏的情况下，这将按比例增加在集群中执行所有权更改所花费的时间。</p><h2 id="所有权变更通知"><a href="#所有权变更通知" class="headerlink" title="所有权变更通知"></a>所有权变更通知</h2><p>每个pravega主机都有一个长期运行的Segment Manager服务。这会不断轮询/监视HostStore，以便对容器所有权进行任何更改。在检测到自身的所有权变更（由map中的主机密钥标识）时，段管理器会相应地触发添加和删除容器。</p><h2 id="确保容器在另一台主机上启动之前在一台主机上停止"><a href="#确保容器在另一台主机上启动之前在一台主机上停止" class="headerlink" title="确保容器在另一台主机上启动之前在一台主机上停止"></a>确保容器在另一台主机上启动之前在一台主机上停止</h2><p>我们目前计划在Pravega主机上使用保守超时，以确保容器在另一个节点上停止之前不会启动。这需要进一步检阅/讨论。</p><h2 id="检测和处理容器启动-停止故障"><a href="#检测和处理容器启动-停止故障" class="headerlink" title="检测和处理容器启动/停止故障"></a>检测和处理容器启动/停止故障</h2><p>任何容器启动/停止故障都被视为本地故障，我们希望Pravega主机尽可能在本地处理这些情况。Pravega Controller控制器不需要通过在不同主机上运行来纠正这一点。这需要进一步检阅/讨论。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega wire protocol</title>
      <link href="/2018/09/19/pravega-wire-protocol/"/>
      <url>/2018/09/19/pravega-wire-protocol/</url>
      
        <content type="html"><![CDATA[<h2 id="流服务线协议"><a href="#流服务线协议" class="headerlink" title="流服务线协议"></a>流服务线协议</h2><p>本页描述了为流服务提出的有线协议。有关服务的描述，请参阅父页面。</p><h2 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h2><p>数据通过线路以独立的“消息”发送，这些消息是“请求”（从客户端到服务器的消息）或“回复”（响应请求并返回到客户端）。</p><p>所有请求和回复都有一个带有两个字段的8字节头，（所有数据都以BigEndian格式写入）：1。消息类型 -​​ 一个整数（4个字节）标识消息类型，这决定了后面的字段。（注意，协议可以通过添加新类型来扩展）2。长度 - 无符号整数4个字节（消息应该&lt;2 ^ 24，但高位保持为零）。此点前面的数据字节数是此消息的一部分。（可能为零，表示没有数据）</p><p>字段的其余部分是特定于消息类型。下面列出了一些重要信息。</p><h1 id="一般信息"><a href="#一般信息" class="headerlink" title="一般信息"></a>一般信息</h1><h2 id="部分消息-请求-应答"><a href="#部分消息-请求-应答" class="headerlink" title="部分消息 - 请求/应答"></a>部分消息 - 请求/应答</h2><ol><li>开始/中间/结尾 - 枚举（1个字节）</li><li>数据部分消息是通过线路发送时被分解的消息。（出于任何原因）。通过按依次读取部分消息并将它们组合成一个整体来重建整个消息。在完成上一个部分消息之前尝试启动新的部分消息是无效的。</li></ol><h2 id="KeepAlive-请求-回复"><a href="#KeepAlive-请求-回复" class="headerlink" title="KeepAlive - 请求/回复"></a>KeepAlive - 请求/回复</h2><ol><li>数据 - 消息长度的未解释数据。（通常为0字节）</li></ol><h1 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h1><h2 id="读段-请求"><a href="#读段-请求" class="headerlink" title="读段 - 请求"></a>读段 - 请求</h2><ol><li>要读取的段 - 字符串（2字节长度，后跟Java的修改后的UTF-8的多个字节）</li><li>读取偏移量 - 长（8个字节）</li><li>建议的回复长度 - int（4字节）<ul><li>这是客户端想要的数据量。他们不一定会获得那么多。</li></ul></li></ol><h2 id="段读-回复"><a href="#段读-回复" class="headerlink" title="段读 - 回复"></a>段读 - 回复</h2><ol><li>读取的段 - 字符串（长度为2个字节，后跟Java的修改后的UTF-8的多个字节）</li><li>从中读取的偏移量 - 长（8个字节）</li><li>在Tail - 布尔值（1位）</li><li>在EndOfSegment - （1位）</li><li>数据 - 二进制（消息中的剩余长度）</li></ol><p>客户端请求以特定偏移量从特定流中读取，然后以SegmentRead消息的形式接收一个或多个应答。这些包含他们请求的数据（假设它存在）。在合适的答复中，服务器可能决定给客户端提供比其要求更多或更少的数据。</p><h2 id="追加"><a href="#追加" class="headerlink" title="追加"></a>追加</h2><h2 id="设置追加-请求"><a href="#设置追加-请求" class="headerlink" title="设置追加 - 请求"></a>设置追加 - 请求</h2><ol><li>ConnectionId - UUID（16字节）标识此appender。</li><li>要追加的段。 - 字符串（长度为2个字节，后跟Java的Modified UTF-8的多个字节）</li></ol><h2 id="追加设置-答复"><a href="#追加设置-答复" class="headerlink" title="追加设置 - 答复"></a>追加设置 - 答复</h2><ol><li>可以附加到的段。 - 字符串（长度为2个字节，后跟Java的Modified UTF-8的多个字节）</li><li>ConnectionId - UUID（16字节）标识请求的appender。</li><li>ConnectionOffsetAckLevel - Long（8字节）此connectionId在此段上接收和存储的最后一个偏移量（如果是新的话，则为0）</li></ol><h2 id="BeginAppendBlock-请求"><a href="#BeginAppendBlock-请求" class="headerlink" title="BeginAppendBlock - 请求"></a>BeginAppendBlock - 请求</h2><p>仅在SetupAppend成功完成后才有效。</p><ol><li>ConnectionId - UUID（16字节）</li><li>ConnectionOffset - Long（8字节）到目前为止通过此连接写入此段的数据</li><li>EndAppendBlock消息之前的数据长度 - 整数（4个字节） </li></ol><h2 id="EndAppendBlock-请求"><a href="#EndAppendBlock-请求" class="headerlink" title="EndAppendBlock-请求"></a>EndAppendBlock-请求</h2><ol><li>ConnectionId - UUID（16字节）</li><li>ConnectionOffset - Long（8字节）到目前为止通过此连接写入的数据</li><li>块长度 - （4个字节）写入的块的总大小。（注意，这可能多于或少于BeginAppendBlock和此消息之间的字节数）</li></ol><h2 id="事件-请求"><a href="#事件-请求" class="headerlink" title="事件 - 请求"></a>事件 - 请求</h2><p>仅在块内有效</p><ol><li>数据</li></ol><h2 id="数据附加-答复"><a href="#数据附加-答复" class="headerlink" title="数据附加 - 答复"></a>数据附加 - 答复</h2><ol><li>ConnectionId - UUID（16字节）</li><li>ConnectionOffsetAckLevel - 长（8字节）此连接之前所有数据成功存储在此段上的最高偏移量</li></ol><h3 id="追加客户段时"><a href="#追加客户段时" class="headerlink" title="追加客户段时"></a>追加客户段时</h3><ol><li>建立与其认为正确的主机的连接。</li><li>发送安装追加请求。</li><li>等待追加设置回复。</li></ol><p>然后它可以1.发送BeginEventBlock请求2.发送尽可能多的消息以适应块3.发送EndEventBlock请求</p><p>在发生这种情况时，服务器将定期向其发送DataAppended回复acking消息。请注意，给定的TCP连接可以有多个“追加”设置。这允许客户端在生成多个段时共享连接。</p><p>客户端可以通过在其BeginAppendBlock消息中指定一个较大的值来优化其附加，因为块内的事件不需要单独处理。</p><p>EndEventBlock消息指定追加块的大小而不是BeginAppendBlock消息。这意味着不需要事先知道块中数据的大小。如果客户端正在生成小消息流，这将非常有用。它可以开始一个块，写入许多消息，然后当它结束块时，它可以在EndAppendBlock消息之后写入部分消息，然后是剩余的部分消息。这样可以避免在块中的所有消息上都有标题，而不必在其进程中将它们缓冲在ram中。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega数据面段存储服务之三</title>
      <link href="/2018/09/18/pravega-segment-store-service-3/"/>
      <url>/2018/09/18/pravega-segment-store-service-3/</url>
      
        <content type="html"><![CDATA[<h2 id="与Controller集成"><a href="#与Controller集成" class="headerlink" title="与Controller集成"></a>与Controller集成</h2><p>关于如何将段容器映射到主机以及使用什么规则从一个迁移到另一个的实际方法超出了本文的范围。这里，我们只描述段存储服务如何与控制器交互，以及它如何基于外部事件管理段容器的生命周期。</p><h2 id="段容器管理"><a href="#段容器管理" class="headerlink" title="段容器管理"></a>段容器管理</h2><p>Segment Store Service的每个实例都需要Segment Container Manager。此组件的作用是管理分配给该节点（服务）的Segment Containers的生命周期。它履行以下职责：</p><ul><li>连接到Controller服务端客户端（即，仅处理Segment Container事件的客户端，而不是Streams的管理，并侦听与其自身实例相关的所有与Container相关的更改。</li><li>当它收到需要为特定Container Id启动Segment Container的通知时，它会启动引导此类对象的过程。在操作完成且没有错误之前，它不会通知请求客户端成功。</li><li>当它收到需要停止特定Container Id的Segment Container的通知时，它会启动关闭它的过程。在操作完成且没有错误之前，它不会通知请求客户端成功。</li><li>如果Segment Container意外关闭（无论是在Start期间还是在正常操作期间），它将不会尝试在本地重新启动它; 相反，它会通知控制器这个事实。</li></ul><h2 id="存储抽象"><a href="#存储抽象" class="headerlink" title="存储抽象"></a>存储抽象</h2><p>段存储不是专门针对TIER-1或TiR-2的实现而设计的。相反，所有这些组件都已经抽象出来并定义得很好，可以针对任何标准文件系统（Tier-2）或仅追加日志系统（Tier-1）实现。</p><p>第1层存储的可能候选者：</p><ul><li>Apache BookKeeper（首选，适配器完全实现为Pravega的一部分）</li><li>非持久性，非复制性解决方案：</li><li>内存中（只部署单个节点——Pravega成为二级存储的易失性缓冲区；在进程崩溃或系统重新启动的情况下，数据丢失是不可避免的和不可恢复的）。<ul><li>这仅用于单元测试。</li></ul></li><li>本地文件系统（仅单节点部署——Pravega成为二级存储的半持久缓冲区；在完全节点失败的情况下，数据丢失是不可避免的和不可恢复的）</li></ul><p>二级存储的可能候选者：</p><ul><li>HDFS（可实施）</li><li>扩展S3（可实现）</li><li>NFS（通用FileSystem）（可用实现）</li><li>内存中（单节点部署——有限的用途；在进程崩溃或系统重新启动的情况下，数据丢失是不可避免的和不可恢复的）<ul><li>这仅用于单元测试。</li></ul></li></ul><p>关于Tier-2 Truncation的注释：</p><ul><li><p>Segment Store支持在特定偏移量处的Segment截断，这意味着，一旦该请求完成，那么在该偏移量以下的偏移量将不可用于读取。<br>上面这只是一个元数据更新操作，但是这也需要由Tier-2支持，以便从其中物理删除截断的数据。</p></li><li><p>如果Tier-2实现从具有偏移量保存的文件开始就不支持截断（即，在偏移50处截断长度为100的段，则删除偏移0..49，但是偏移量50-99可用并且没有向下移动），然后Segment Store在通用的Tier-2实现之上提供了一个包装器，它可以做到这一点.</p></li><li><p>所述RollingStorage tier-2 分割segment成多个段组块并暴露它们作为一个单一的段到上层。已截断的段块将自动删除。这不是一个非常精确的应用程序（因为它在很大程度上依赖于规定粒度的翻转策略），但对于那些真正的第2层实现不提供我们需要的功能的情况，它是一个实用的解决方案。</p></li></ul><h2 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h2><p>以下是Pravega Segment Store Service中数据流动的几个示例。</p><h2 id="追加"><a href="#追加" class="headerlink" title="追加"></a>追加</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FSegment.Store.Appends.png" alt="Segment.Store.Appends"></p><p>上图描绘了这些步骤（注意步骤编号可能不匹配，但顺序相同）：</p><ol><li>Segment Store接收带有params的附加请求：Segment Name，Payload和AttributeUpdates。</li><li>Segment Store确定给定Segment的ContainerId，并验证Segment Container是否在本地注册。如果不是，则返回适当的错误代码。</li><li>Segment Store将请求委托给相应的Segment Container实例。<ul><li>Segment Container验证Segment是否属于Segment Container并且Segment实际存在。如果不是，则返回适当的错误代码。<ul><li>在此过程中，它还会获得现有的段ID或分配新段（通过使用段映射器组件）。</li></ul></li><li>Segment Container StreamSegmentAppendOperation使用输入数据创建a 并将其发送到Durable Log。</li></ul></li></ol><ol start="4"><li><p>持久日志采用追加操作并根据持久日志部分中描述的算法对其进行处理:</p><ul><li>将其放入其操作队列中。</li><li>操作处理器从队列中拉出所有操作。</li><li>操作处理器使用数据框构建器来构建具有其操作的数据框架。</li><li>Data Frame Builder将数据帧异步写入持久数据日志。</li><li>完成后，以下内容并行完成：<ul><li>元数据已更新。</li><li>操作被添加到内存操作日志和读取索引中。</li><li>触发操作的呼叫被激活。</li></ul></li><li>上述过程是异步的，这意味着操作处理器将具有多个未受控制的数据帧（未示出）。它将跟踪每一个的变化并根据需要应用或回滚。<br>此过程适用于Segment Store支持的每个操作。所有修改操作都通过操作处理器并具有类似的路径。</li></ul></li></ol><h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FSegment.Store.Reads.png" alt="Segment.Store.Reads"></p><p>上图描绘了这些步骤（注意步骤编号可能不匹配，但顺序相同):</p><ol><li>段存储接收带有参数的读取请求：段名称，读取偏移量，最大长度。<ul><li>Segment Store确定给定Segment的ContainerId，并验证它是否是给定Segment Container的 Leader 。如果不是，则返回适当的错误代码。</li><li>Segment Store将请求委托给Segment Container实例。</li></ul></li><li><p>Segment Container验证Segment是否属于该Container并且它实际存在。如果没有，它会向客户端返回适当的错误代码。</p><ul><li>在此过程中，它还会获得现有的段ID或分配新段（通过使用段映射器组件）。</li></ul></li><li><p>段容器将请求委托给其读取索引，该索引按照“ 读取索引”部分中的描述处理读取，方法是从存储中发出读取（对于不在缓存中的数据），并根据需要查询/更新缓存。</p></li></ol><h2 id="与Tier-2（存储写入器）同步"><a href="#与Tier-2（存储写入器）同步" class="headerlink" title="与Tier-2（存储写入器）同步"></a>与Tier-2（存储写入器）同步</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FSegment.Store.SyncTier2.png" alt="Segment.Store.SyncTier2"></p><p>上图描绘了这些步骤（注意步骤编号可能不匹配，但顺序相同）：</p><ol><li>该存储写入的主循环是子组件触发所有这些操作</li><li>从持久日志中读取下一个操作（在每个循环之间，Writer会记住上次处理的操作的序列号是什么）</li><li>处理所有操作，并将其添加到内部段聚合器（每个段一个聚合器）</li><li>符合条件的段聚合器被刷新到存储（资格取决于每个聚合器中收集的数据量，以及是否有排队的Seal，Merge或Truncate操作）<ul><li>每次遇到Append操作时，可能需要访问Read Index以获取追加的内容</li></ul></li><li>在对storage的每次成功修改（写入/密封/连接/截断）之后，都会更新Container Metadata以反映更改。</li><li>该持久日志被截断（如果符合条件）。</li></ol><h2 id="容器启动（正常-恢复）"><a href="#容器启动（正常-恢复）" class="headerlink" title="容器启动（正常/恢复）"></a>容器启动（正常/恢复）</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FSegment.Store.Recovery.png" alt="Segment.Store.Recovery"></p><p>上图描绘了这些步骤（注意步骤编号可能不匹配，但顺序相同）：</p><ol><li>容器管理器接收在该段存储服务的这个实例中启动容器的请求。<ul><li>它创建，注册和启动Container。</li></ul></li><li>该容器启动持久日志组件。</li><li>持久日志启动恢复过程（由Recovery Executor协调）。</li><li>Recovery Executor从持久数据日志中读取所有数据帧。</li><li>读取数据帧中的反序列化操作将添加到“ 内存操作日志”中。</li><li>所述容器的元数据是由的方式更新操作元数据更新器（同运算处理器内使用的）。</li><li>该读取索引填充了那些适用于IT运营的内容。</li><li>该容器启动存储写入器。<ul><li>该存储写入的主循环开始从处理操作持久化日志，以及在第一次遇到一个新的segment时，它将其内容（和元数据）与存储中存在的实际情况调和。</li></ul></li><li>在Durable Log和Storage Writer都启动后，Container已准备好开始接受新的外部请求。</li></ol>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega数据面段存储服务之二</title>
      <link href="/2018/09/18/pravega-segment-store-service-2/"/>
      <url>/2018/09/18/pravega-segment-store-service-2/</url>
      
        <content type="html"><![CDATA[<h2 id="日志操作"><a href="#日志操作" class="headerlink" title="日志操作"></a>日志操作</h2><p>一个日志操作是在可持久化的日志里序列化的基本单元。它本身并不代表一个动作，而是几个可序列化操作的基础（我们可以序列化多种类型的操作，而不仅仅是Appends）。每个操作都是外部操作（表示更改段）或内部触发（如元数据维护操作）的结果。<br>每个日志操作都具有以下元素： <code>- SequenceNumber</code>：分配给此条目的唯一序列号（请参阅容器元数据下的更多信息）。</p><p>这些是各种类型的日志操作：</p><ul><li>存储操作表示需要应用于底下第2层存储的操作：</li><li>StreamSegmentAppendOperation：表示对特定段的附加。</li><li>CachedStreamSegmentAppendOperation：与StreamSegmentAppendOperation相同，但这是供内部使用的（它不是具有实际的数据有效负载，而是指向缓存中可以从中检索数据的位置）。</li><li>StreamSegmentSealOperation：处理后，它会在内存中的元数据中设置一个标记，以便不再接收附加内容。当Storage Writer处理它时，它会在第2层存储中将Segment标记为只读。</li><li>StreamSegmentTruncateOperation：截断特定偏移处的段。这会导致segment的StartOffset发生变化。</li><li>MergeTransactionOperation：表示要将事务合并到其父段中。</li><li>元数据操作是指示容器元数据更改的辅助操作。它们可能是外部操作的结果（我们之前收到了一个我们从未知道的段的请求，因此我们必须为其分配一个唯一的ID）或者对整个元数据进行快照（这有助于恢复和清理第1层）存储）。元数据操作的目的是减少故障转移恢复所需的时间（如果需要）：</li><li>StreamSegmentMapOperation：将Id映射到段名称。</li><li>TransactionMapOperation：将Id映射到事务及其父段。</li><li>UpdateAttributesOperation：更新segment上的任何属性。</li><li>MetadataCheckpoint包括元数据的整个快照。这在恢复期间非常有用 - 它包含到目前为止的所有元数据，这是之后所有操作的基础。</li></ul><h2 id="可持久化日志"><a href="#可持久化日志" class="headerlink" title="可持久化日志"></a>可持久化日志</h2><p>该可持久化日志是处理所有操作日志的核心组成部分。所有操作（由Container创建）都会添加到持久日志中，该日志按照接收顺序处理它们。它由一些其他组件组成，在不影响数据完整性的情况下，所有这些组件的唯一目标都是致力于尽快处理所有输入操作。</p><p>可持久化日志中的信息流:</p><ol><li>所有收到的操作都被添加到操作队列中 （调用方接收一个当操作持久地完成时将完成的未来）</li><li>该运算处理器选取目前在队列中可用的所有操作（如果队列为空，则等到至少一个操作被添加）。</li><li><p>所述操作处理器运行作为一个连续的环（在后台线程中），并且具有四个主要阶段。</p><ul><li>从操作队列中出列所有未完成的操作（如上所述）</li><li>预处理操作（验证它们是否正确并且不会导致意外行为，分配偏移量（如果需要），分配序列号等）</li><li>写操作的数据帧生成器，其序列化和包装的操作数据帧。数据框完成后（完整或不再需要添加操作），数据框构建器将数据框发送到_可持久化的数据日志。请注意，操作可能跨越多个DataFrame - 目标是通过尽可能大的写入（但也考虑到每次写入可能有最大大小限制）来充分利用持久数据日志吞吐量容量。</li></ul></li></ol><ol start="4"><li><p>当持久数据日志中的DataFrame持久存在时，操作处理器会对迄今为止完全写入的所有操作进行后处理（将它们添加到内存中结构，更新索引等）并完成与它们相关的未来。</p></li><li><p>Operation Processor异步工作，因为它在开始另一个数据帧并将其发送到持久数据日志之前不等待写入特定的数据帧。因此，多个DataFrame可能正在运行（但是以特定的顺序），并且操作处理器依赖于持久数据日志中的某些排序保证（如果特定DataFrame被攻击，那么在此之前的所有DataFrame也以正确的顺序被成功提交）。</p><ul><li>操作处理器不执行任何写入限制（将其留给持久数据日志实现），但它控制发送给它的数据帧的大小。</li></ul></li></ol><h2 id="截断"><a href="#截断" class="headerlink" title="截断"></a>截断</h2><p>根据提供的配置，Durable Log会自动添加一种特殊的操作，命名为MetadataCheckpointOperation。此操作在由操作处理器处理时，收集整个Container Metadata的快照，并将其序列化为Durable Data Log。此特殊操作标记一个截断点 - 日志操作流中的一个位置，我们可以在其中发出截断操作。非常重要的是，在每次截断之后，在日志中找到的第一个操作是一个 MetadataCheckpointOperation，因为没有先前的操作来重建元数据，这是能够处理后续操作的唯一方法。</p><p>注意：不应将持久数据日志（第1层）截断与段截断相混淆。它们用于不同的目的，适用于不同的目标。</p><h2 id="操作处理器"><a href="#操作处理器" class="headerlink" title="操作处理器"></a>操作处理器</h2><p>操作处理器是处理日志输入操作的可持久化的日志的子组件。其目的是基于每个操作的内容来验证、持久化和更新元数据和其他内部结构。<br>操作元数据更新器</p><p>操作元数据更新器是持久日志的子组件，负责基于元数据的当前状态验证操作，以及在成功提交操作之后更新元数据。在内部，它有各种机制来应对失败，并且它可以回滚失败情况下的某些变化。</p><h2 id="持久数据日志"><a href="#持久数据日志" class="headerlink" title="持久数据日志"></a>持久数据日志</h2><p>持久数据日志是一个外部组件的抽象层，提供仅附加语义。它是一个向日志提供非常快速附加的系统，它保证了写入数据的持久性和一致性。读取性能不是一个很重要的因素，因为我们不直接从该组件读取数据 - 我们只在需要恢复持久日志的内容时才从该组件读取日志数据。</p><p>如上所述，日志操作被序列化为数据框架（如果需要的话，单个操作能够跨越多个这样的框架），然后这些数据框架被序列化为持久数据日志的条目。这仅用作故障安全，并且我们只需要在需要执行恢复时才需要读回这些框架（在这种情况下，我们需要按照接收它们的相同顺序反序列化它们中包含的所有日志操作）。</p><h2 id="内存操作日志"><a href="#内存操作日志" class="headerlink" title="内存操作日志"></a>内存操作日志</h2><p>内存中操作日志包含提交（和复制）的日志操作，其顺序与添加到持久数据日志的顺序完全相同。虽然持久数据日志包含一系列数据帧（其中包含操作的序列化），但是内存日志包含实际的操作，这些操作可以在整个持久日志（以及存储写入器）中使用。</p><p>内存日志本质上是在接收操作时命令的日志操作链。我们总是在一端添加，然后从另一端移除。当我们截断持久数据日志时，内存日志也被截断在同一位置。</p><h2 id="读取索引"><a href="#读取索引" class="headerlink" title="读取索引"></a>读取索引</h2><p>读取索引有助于段容器在任意偏移量下执行从流读取。虽然耐用日志按照接收的顺序记录（并且只能重放）数据，但是Read Index可以以随机方式访问数据。读取索引由多个片段读取索引（每个活片段之一）构成。</p><p>段读取索引是一种数据结构，用于提供从内存的读取，以及从第2层存储中提取数据，并在数据尚未可用时提供未来读取（尾部读取）。当接收到读取请求时，段读取索引返回一个读取迭代器，只要读取请求参数尚未满足，该迭代器将返回数据。迭代器要么从存储器中取出立即可用的数据，要么从第2层存储器中请求数据（并将其带到存储器中），要么如果到达段当前末端，返回Future并在添加新数据时完成（从而提供尾随或未来读取）。</p><p>段读索引的核心是条目的排序索引（由它们的起始偏移量索引），用于在需要时定位所请求的数据。索引本身由定制的平衡二进制搜索树（更确切地说，是AVL树）实现，其目标是最小化内存使用而不牺牲插入或访问性能。条目本身不包含数据，而是一些少量的元数据，这些元数据用于定位缓存中的数据并确定使用模式（对缓存撤出很有好处）。</p><h2 id="高速缓存"><a href="#高速缓存" class="headerlink" title="高速缓存"></a>高速缓存</h2><p>缓存是一个组件，其中所有数据（无论是从新添加的还是从第二存储中提取的）都被存储。它是一个完全由读取索引管理的密钥值存储。</p><p>缓存被定义为抽象层，并且有两个实现：</p><ul><li>内存实现（通过哈希图）。目前只用于单元测试。</li><li>内存与磁盘混合，由ROCKSDB提供支持。这是首选的（默认的）实现，因为它不受可用堆空间或机器RAM的限制，其性能与内存大小成正比。</li></ul><h2 id="存储写入"><a href="#存储写入" class="headerlink" title="存储写入"></a>存储写入</h2><p>Pravega绝不是数据的最终安放位置，也不是存储服务。Tier-2 Storage是我们希望数据长期存在的地方而Pravega仅用于存储非常短的尾部（使用第1层存储），足够快速追加并将它们聚合成更大的块以便提交给第2层存储。为了执行此操作，它需要另一个组件（存储写入），该组件按照接收顺序从持久日志中读取数据，对其进行聚合，并将其发送到第2层存储。</p><p>就像持久日志一样，每个段容器都有一个存储写入器。每个写入器从内存操作日志中读取日志操作（通过持久日志中的read()方法公开），按照它们被处理的顺序。它通过它的序列号来跟踪最后一个读项目。此状态不被持久化，并且在恢复时，它可以从可用的持久日志的开始开始。</p><p>Storage Writer可以处理任何存储操作（附加，密封，合并），并且由于Pravega是在第2层中修改此类数据的唯一参与者，因此它可以不受约束地应用它们。它有几种机制可以检测和恢复可能的数据丢失或外部参与者同时修改数据。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega数据面段存储服务之一</title>
      <link href="/2018/09/18/pravega-segment-store-service-1/"/>
      <url>/2018/09/18/pravega-segment-store-service-1/</url>
      
        <content type="html"><![CDATA[<p>Pravega Segment Store Service是Pravega的核心子系统，它提供了创建，删除和修改/访问其内容的功能，是管理流段的主要入口。Pravega客户端与Pravega stream controller互动以确定需要使用哪些段（对于流），流控制面和客户端一起处理段存储服务从而操作这些流段。<br>Segment Store Service背后的基本思想是，它将输入的数据缓存在一个非常快速且持久化的append only介质（第1层存储）中，并将其与高吞吐量（但不一定是低延迟）存储系统（第2层存储）同步，同时将多个小流段合并到大的流段里。</p><p>Pravega Segment Store Service可提供以下保证：</p><ul><li>流段长度不受限制，仅具有附加语义，但支持任意偏移读取</li><li>无论底层第2层存储系统的性能如何，执行小型附加时都不会降低吞吐量</li><li>多个并发写入到同一个段</li><li>在单个写入的上下文中保证顺序，但是来自多个并发写入的附加数据行为将按照接收它们的顺序来添加（附加是原子的而不交错其内容）。</li><li>并发写入和读取段，写入和读取之间的延迟相对较低。</li></ul><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>在本文档的其余部分中，我们将使用以下术语：</p><ul><li>流段或段：连续的字节序列。类似于没有大小限制的文件。这是Stream的一部分，限制是暂时的并且是横向的（根据key值）。Streams的 范围 和如何将Stream Segments映射到此Streams超出了本文档需要阐述的内容。</li><li>第2层存储或永久存储：数据的最终存储位置。</li><li>第1层存储：快速附加存储，用于在将数据刷到第2层存储之前持久缓冲输入的append only数据。</li><li>缓存：键值本地缓存，不期望持久性。</li><li>Pravega Segment Store服务或Segment Store：本文档描述的服务。</li><li>事务：与段相关的一系列附加操作，如果持久化，它将在段中构成连续的字节范围。这用于摄取非常大的记录或用于累积可能或可能不会持久存储到段中的数据（但其如何使用以后才能确定）。</li></ul><p>请注意，在Pravega级别，事务适用于整个流。在本文档中，事务适用于单个段。</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>段储存由以下几部分组成：</p><ul><li>Pravega Node：运行Pravega进程的主机。</li><li>流段容器（或段容器）：流段的逻辑分组。Segments到Containers的映射是确定性的，不需要任何持久存储; 段通过hash函数（基于段的名称）映射到容器。</li><li>持久数据日志适配器（或DurableDataLog）：第1层存储的抽象层。</li><li>存储适配器：第2层存储的抽象层。</li><li>缓存：用于追加数据缓存的抽象层。</li><li>Streaming Client：可用于与Pravega Segment Store通信的API。</li><li>Segment Container Manager：可用于确定Pravega节点上Segment Containers生命周期的组件。该组件用于启动或停止Segment Containers， 而这些段容器是基于外部协调服务（例如Pravega控制器）的。</li></ul><p>首先段存储通过将数据写入快速存储（最好是SSD）上的日志层（持久数据日志），并在数据被持久存储后立即返回给客户端。随后，这些写入的数据被合并成更大的数据块并在后台刷新到第2层存储。已经确认（并且在第1层中）但尚未在第2层中的附加数据存储在缓存中（除了第1层）。一旦将此类数据写入第2层存储，它可能会也可能不会保留在缓存中，具体取决于许多因素，例如缓存利用率/压力和访问模式。<br>有关上述每个组件的更多详细信息，请参阅“ 组件”部分（下面）。</p><h2 id="系统图"><a href="#系统图" class="headerlink" title="系统图"></a>系统图</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2FSegment.Store.Components.png" alt="Segment.Store.Components"></p><p>在此图中，我们显示了Segment Store的主要组件（为简单起见，仅描绘了一个Segment Container）。显示所有Container组件和它们之间的主要链接（它们如何相互交互）。所述容器的元数据组件未示出。<br>更详细的图表可以在数据流部分（下面）中找到。</p><h1 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h1><h2 id="段容器"><a href="#段容器" class="headerlink" title="段容器"></a>段容器</h2><p>段容器是段的逻辑组，负责跨越这些段的所有操作。Segment Container由多个子组件组成：</p><ul><li>段容器元数据：一组段的元数据，用于描述每个段的当前状态（第2层中的数据量，第1层中的数量，是否密封等），以及关于每个容器的其他错误信息。</li><li>可持久化日志：Container将其接收的每个操作都写入此日志，并仅在日志显示已被接受且持久化存储时才返回。</li><li>读索引：可从中读取数据的内存索引。Container将所有读取请求委托给它，它负责从当前所在的任何位置（Cache，Tier-1 Storage或Tier-2 Storage）获取数据。</li><li>缓存：用于存储仅在第1层（尚未存在于第2层）中的附加数据，以及支持读取的数据块。</li><li>Storage Writer：处理持久日志操作并将它们应用于第2层存储（按接收顺序）。此组件也是将多个操作合并在一起的组件，以获得更好的后端吞吐量。</li></ul><h2 id="段容器元数据"><a href="#段容器元数据" class="headerlink" title="段容器元数据"></a>段容器元数据</h2><p>段容器元数据对其组件的良好运行和同步至关重要。此元数据在所有组件之间共享，它分为两个级别：容器范围的元数据和每个段的元数据。每个服务都有不同的用途，如下所述。<br>容器元数据<br>每个Segment Container都需要保存一些影响容器内所有操作的通用元数据：</p><ul><li>操作序列号：持久日志分配的最大序列号。每次通过持久日志接收并成功处理新操作时，此数字都会递增（其值永远不会减少或以其他方式回滚，即使操作未能保存）。</li><li>操作序列号保证严格单调递增（没有两个操作具有相同的值，并且操作将始终具有比之前的所有操作更大的序列号）。</li><li>Epoch：每次成功恢复（Container Start）时会增加的数字。该值可以持续递增并作为恢复的一部分进行存储，并且可以用于许多场景（例如作为HDFS的第2层防护，HDFS不能为此提供良好的原生保护机制）。</li><li>活动段元数据：有关每个活动段的信息（请参阅下一节）。如果Segment最近有活动（读取或写入）并且当前已加载到内存中，则它处于活动状态。如果这个段有一段时间内未使用，或者当前有太多个段处于活动状态，那么通过将段的元数据刷新到第2层存储并且将段的元数据从内存中淘汰，从而可以使得这个段变为非活动状态。</li><li>第1层元数据：在该点之前的所有操作已经持久存储到第2层，可用于准确截断第1层存储日志的各种信息。</li><li>检查点：通过将容器元数据的整个快照（包括活动段）序列化到第1层存储来定期对容器元数据打检查点。检查点充当第1层的截断点，这意味着它包含通过之前所有已处理的操作对Container进行的所有更新，因此我们不再需要这些操作来重建元数据。如果我们在Checkpoint上截断Tier-1，那么我们可以使用来自Tier-2和此Checkpoint的信息来重建先前元数据中的内容，而不依赖于Tier-1中之前的任何操作。</li></ul><h2 id="段元数据"><a href="#段元数据" class="headerlink" title="段元数据"></a>段元数据</h2><p>每个段容器都需要保留每个段的元数据，用于在处理每个段的操作时跟踪每个段的状态。元数据可以是易失性的（可以在恢复时完全重建），并且包含当前正在使用的每个段的以下属性：</p><ul><li>Name 段的名称。</li><li>Id：内部分配的唯一段ID。这用于指代Segments，它比段的名称更受欢迎。此ID在段的生命周期内是不会改变的，这意味着即使段变为非活动状态，将来重新激活也会将其映射到相同的Id。</li><li>StartOffset（也称为TruncationOffset）：可用于读取的数据的最低偏移量。非截断段的Start Offset将等于0，而后续Truncate操作将增加（但永不减少）此数字。</li><li>StorageLength：第2层存储中存在的数据的最高偏移量。</li><li>Length：第1层存储中已提交数据的最高偏移量。</li><li>LastModified：上次处理（和确认）附加的时间戳。</li><li>IsSealed：segment是否已关闭追加数据（此值可能尚未应用于Tier-2存储）。</li><li>IsSealedInStorage：Segment是否已关闭追加数据（并且这已在第2层存储中保留）。</li><li>IsMerged：此段是否已合并到另一个段中（但尚未在第2层存储中保留）。这仅适用于事务。一旦合并持续到第2层，事务段就不再存在（因此IsDeleted将成为现实）。</li><li>IsDeleted：segment是否被删除或最近是否已合并到另一个segment中。这仅适用于最近删除的segment，而不适用于从未存在过的segment。<br>对于任何segment，以下内容  始终为true：</li><li>StorageLength &lt;= Length</li><li>StartOffset &lt;= Length</li></ul>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega reader groups设计</title>
      <link href="/2018/09/17/pravega-reader-groups/"/>
      <url>/2018/09/17/pravega-reader-groups/</url>
      
        <content type="html"><![CDATA[<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>可以将一组读者组合在一起，以便可以并行读取流中的事件集。读者群组称为读者群。Pravega保证流中的每个事件都由读者组中的一个读者读取。<br>ReaderGroup中的每个Reader都分配了零个或多个段。分配给Segment的Reader是ReaderGroup中唯一一个从该Segment读取事件的Reader,这是Pravega向读者提供事件传递的顺序保证的基本机制,读者将按照他们发布到segment中的顺序接收事件。<br>这种机制存在以下几个挑战：</p><ul><li>如何维护ReaderGroup中哪个Reader的映射分配哪个Segment </li><li>如何在Segments拆分和合并时管理上述映射 </li><li>如何在将读者添加到ReaderGroup </li><li>当读者通过显式操作离开ReaderGroup或reader因网络中断或Reader进程失败而变得不可用时，如何管理上述映射</li></ul><p>为了解决这些问题，我们可以使用[[StateSynchronizer | StateSynchronizer-design]]使读者能够相互协调。</p><h2 id="如何使用一致的复制状态来解决问题"><a href="#如何使用一致的复制状态来解决问题" class="headerlink" title="如何使用一致的复制状态来解决问题"></a>如何使用一致的复制状态来解决问题</h2><p>每个reader中都创建了表示ReaderGroup元数据的一致复制状态对象,此ReaderGroup元数据包括：</p><ul><li>在线读者的映射表，他们拥有的segment可以接管的segment中的位置列表。</li><li>每次ReaderGroup中的读者更改时，都可以更新状态。</li><li>类似地，每当其中一个读者开始从一个新段读取时，它就可以更新复制状态。</li></ul><p>这允许所有读者了解ReaderGroup中的所有其他读者以及他们拥有的哪些片段。</p><p>假设这样的信息：</p><ul><li><p>新读者可以知道哪些片段可读取,（因为无状态）处理合并的段变得容易，因为到达其合并前段的末尾的最后一个读者知道它可以自由地获得新段的所有权。</p></li><li><p>读者可以看到他们的相对负载以及他们相对于他们小组中其他读者的进展情况，并且如果事情失衡，他们可以决定转移segment。</p></li><li><p>这允许读者直接采取行动，以确保所有事件都被读取，而无需一些外部跟踪器。</p></li></ul><h2 id="ReaderGroup的API"><a href="#ReaderGroup的API" class="headerlink" title="ReaderGroup的API"></a>ReaderGroup的API</h2><p>可以将用于管理ReaderGroup的外部API添加到StreamManager对象。它们包括：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">ReaderGroup createReaderGroup（String name，Stream stream，ReaderGroupConfig config）;<br>ReaderGroup getReaderGroup（String name，Stream stream）;<br>void deleteReaderGroup（ReaderGroup group）;<br></code></pre></td></tr></table></figure><p>创建ReaderGroup时，它会创建一个由读者共享的[[StateSynchronizer | StateSynchronizer-design]]。要加入ReaderGroup，读者只需在其配置中指定它：</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">ReaderConfig cc = new ReaderConfig（props）;<br>Reader &lt;T&gt; reader = a_stream.createReader（“my_reader_id”，“my_reader_group”，cc）;<br></code></pre></td></tr></table></figure><p>当读者加入组时，他们使用状态来确定要读取的segment。当他们关闭时，他们会更新状态，以便其他读者可以接管他们的segment。</p><h2 id="故障检测器"><a href="#故障检测器" class="headerlink" title="故障检测器"></a>故障检测器</h2><p>我们仍然需要某种心跳机制来判断读者是否还在线。问题大大简化，因为它不需要生成集群视图或管理任何状态。该组件只需要检测失败并调用<code>void readerOffline(String readerId, Position lastPosition);</code>ReaderGroup上的api</p><p>为保持一致性，故障检测器不应将仍在处理事件的主机声明为死机，这样做可能会违反恰好一次处理保证。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h2 id="新读者"><a href="#新读者" class="headerlink" title="新读者"></a>新读者</h2><ol><li>当读者加入组时，其在线状态将添加到共享状态</li><li>其他读者会收到共享状态的更新。</li><li>当具有超过平均段数的读者看到新读者时，它可以通过将该段的位置写入共享状态来放弃段。</li><li>新的读者可以通过记录它在共享状态下执行此操作来接管一个段。</li><li>新读者可以从它从所拾取的段的共享状态读取的位置开始读取。</li><li>多个读者之间没有同时上线的比赛，因为只有其中一个可以成功声明任何给定片段的所有权。</li></ol><h2 id="合并段"><a href="#合并段" class="headerlink" title="合并段"></a>合并段</h2><ol><li>当读者到达其片段的末尾时，它会将此信息记录在共享状态中。</li><li>当所有合并在一起的段完成后，读者可以声明对以下段的所有权。</li></ol><p>关于拥有者是谁，因为它存储在共享状态中没有歧义。不存在遗忘某个段的风险，因为任何读者都可以通过查看共享状态并声明它们来查看哪些段可用。</p><h2 id="读者离线"><a href="#读者离线" class="headerlink" title="读者离线"></a>读者离线</h2><ol><li>当读者离线时，readerOffline（）方法将由读者本身在正常关闭（在close方法内部）或通过活动检测器调用。在任何一种情况下，读者的最后位置都会被传入。</li><li>最后一个位置写入状态。</li><li>其他读者在更新本地状态时会看到这一点。</li><li>他们中的任何一个都可以通过记录他们在状态对象中的意图来决定接管旧读者所拥有的一个或多个片段。</li><li>状态更新后，新读者将被视为该segment受众群的所有者，并可随意阅读。</li></ol><h2 id="如果读者没有及时了解会发生什么"><a href="#如果读者没有及时了解会发生什么" class="headerlink" title="如果读者没有及时了解会发生什么"></a>如果读者没有及时了解会发生什么</h2><p>具有过期状态的读者可以从其现有段中读取而不受干扰。唯一的缺点是，如果有可用的话，它们不会给另一个读者带来负担。但是，因为他们必须写入共享状态才能从他们尚未拥有的任何段开始读取，所以他们必须在转移到新段之前获取最新信息。</p><h2 id="可用性和延迟的影响"><a href="#可用性和延迟的影响" class="headerlink" title="可用性和延迟的影响"></a>可用性和延迟的影响</h2><p>读取和更新状态对象可以与读取并行发生，因此可能没有可见的延迟影响。如果Pravega以包含ReaderGroup信息的段落下并且保持离线足够长时间以使读者耗尽其现有段中的所有事件的方式失败，则流将无法读取。当然，如果Pravega以这种方式失败，那么至少某些部分流也会受到直接影响，并且无法读取任何事件。这种故障模式将表现为读者的延迟，类似于他们到达流尾部时会发生的情况。</p><p>这比使用外部系统来管理这种协调更为可取，因为这需要添加可能以不同方式失败的新组件，而不是进一步依赖我们需要高度可用的小组。在网络分区的情况下，这尤其值得注意。如果网络被分开，与Pravega服务器位于分区同一侧的任何reader都可以继续工作。如果我们要利用外部服务，那么该服务可能被切断，即使他们可以与Pravega交互，读者也可能无法取得进展。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>分布式系统中DHT算法改进</title>
      <link href="/2018/09/16/distributed-dht-update/"/>
      <url>/2018/09/16/distributed-dht-update/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>通常，分布式存储系统以及分布式缓存系统习惯采用分布式哈希（DHT）算法来实现数据的分区分配（路由）以及负载均衡，普通的分布式hash算法通过增添虚拟节点，对物理的热点区间进行划分，将负载分配至其他节点，从而达到负载均衡的状态，但是这并不能保证集群的负载就一定很是的均衡。</p><p>而一种改进过的一致性Hash算法，即带边界因子的一致性Hash算法，其严格控制每个节点的负载从而能获得更好的负载均衡效果[1][2]。</p><h2 id="普通的DHT算法"><a href="#普通的DHT算法" class="headerlink" title="普通的DHT算法"></a>普通的DHT算法</h2><p>假设有8个Object，通过下图的DHT算法:</p><ol><li>object 0,1,2映射到了虚拟节点vNode0 ： object 0,1,2 –&gt; vNode0</li><li>Object 3,4,5 映射到了vNode1：object 3,4,5 –&gt; vNode1</li><li>Object 6映射到 vNode2：object 6 –&gt; vNode2</li><li>Object 7映射到 vNodeN：object 7 –&gt; vNodeN</li></ol><p><img src="http://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-DHT-1.png" alt="distributed-DHT-1"></p><p><img src="http://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-DHT-2.png" alt="distributed-DHT-2"></p><p>很明显，Vnode0和vNode1 都落了三个 object，而 vNode2和vNodeN 都只落了 1个Object，这里的DHT算法负债均衡因子并不是很好。</p><h2 id="带负载边界因子的DHT算法"><a href="#带负载边界因子的DHT算法" class="headerlink" title="带负载边界因子的DHT算法"></a>带负载边界因子的DHT算法</h2><p>假设有8个Object，通过如下图的DHT with bounded loads算法:</p><p><img src="http://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-DHT-3.png" alt="distributed-DHT-3"></p><p><img src="http://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fdistributed-DHT-4.png" alt="distributed-DHT-4"></p><p>第一轮映射：</p><ol><li>object 0,1,2 需要映射到了虚拟节点vNode0，但是vNode0的权重因子是 2，因此只完成了 object 0,1 –&gt; vNode0， object 2不能映射到节点 vNode0；</li><li>Object 3,4,5 需要映射到了虚拟节点vNode1：但是vNode1的权重因子是 2，因此只完成了 object 3,4 –&gt; vNode1， object 5不能映射到节点 vNode1；</li><li>Object 6映射到 vNode2：object 6 –&gt; vNode2</li><li>Object 7映射到 vNodeN：object 7 –&gt; vNodeN</li></ol><p>第二轮映射：</p><ol><li>Object 2 映射到 vNode1，但是vNode1权重因子=0， 不能被接收，继续往下一个节点走，发现vNode2 权重因子是2,还剩权重因子1，可以被映射，因此 object 2–&gt;vNode2</li><li>Object 5 映射到 vNode2，但是vNode2现在的权重因子=0， 不能被接收，继续往下一个节点走，发现vNodeN 权重因子是2,还剩权重因子1，可以被映射，因此 object 5–&gt;vNodeN</li></ol><p>最终的映射结果是:</p><ol><li>object 0,1映射到了虚拟节点vNode0 ： object 0,1 –&gt; vNode0</li><li>Object 3,4 映射到了vNode1：object 3,4 –&gt; vNode1</li><li>Object 2,6映射到 vNode2：object 2,6 –&gt; vNode2</li><li>Object 5,7映射到 vNodeN：object 5,7 –&gt; vNodeN</li></ol><p>很明显，Vnode0，vNode1，vNode2, vNodeN 每个节点都分到2个 object，<br>显然带负载边界因子的DHT算法负债均衡比普通的DHT算法来的好。</p><p>这些节点的负载因子可以从IO，CPU，MEM，Disk，Network等输入因子计算出来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://research.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html" target="_blank" rel="noopener">https://research.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html</a></p><p>[2] <a href="https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed" target="_blank" rel="noopener">https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed</a></p>]]></content>
      
      
      <categories>
          
          <category> distributed </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>是时候把分布式系统的理论指导从CAP升级到PACELC</title>
      <link href="/2018/09/16/distributed-cap-pacelc/"/>
      <url>/2018/09/16/distributed-cap-pacelc/</url>
      
        <content type="html"><![CDATA[<h2 id="从-CAP到-PACELC"><a href="#从-CAP到-PACELC" class="headerlink" title="从 CAP到 PACELC"></a>从 CAP到 PACELC</h2><p>CAP理论是当前分布式系统设计的理论指导，而PACELC理论是CAP理论的扩展，分布式系统设计的理论依据是时候从CAP理论扩展为PACELC理论, PACELC在wiki上的定义是:</p><blockquote><p>It states that in case of network partitioning (P) in a distributed computer system, one has to choose between availability (A) and consistency (C) (as per the CAP theorem), but else (E), even when the system is running normally in the absence of partitions, one has to choose between latency (L) and consistency (C).</p></blockquote><p>简单来说这里的意思就是：</p><blockquote><p>如果有分区partition (P)，系统就必须在availability 和consistency (A and C)之间取得平衡; 否则else (E) 当系统运行在无分区情况下,系统需要在 latency (L) 和 consistency (C)之间取得平衡</p></blockquote><p>CAP理论认为以下三者不能同时满足：</p><ul><li><p>一致性(Consistency): 所有的节点在同一时刻看到同样的数据。</p></li><li><p>可用性(Availability):  节点失效不会影响系统的读写。</p></li><li><p>分区容忍性(Partition Tolerance): 系统能支持网络分区，即使分区之间的消息丢失系统也正常工作。</p></li></ul><p>根据业务场景的不同，不同的分布式系统会根据自身业务的需求在CAP三者中进行权衡， CAP理论的意义是一种在分布式系统设计时权衡的因素，而非绝对的三者必舍其一，并且在CAP理论中是没有提到系统的时延（Latency）的，而访问时延（Latency）却是很重要的可用性(Availability)因素。</p><p>因此重新定义一个新的模型PACELC，添加了系统中的Latency，如下图：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fcap-pacelc.png" alt="cap-pacelc"></p><p>当前分布式系统设计指导理论应当用PACELC理论替代CAP理论，理由如下：</p><ol><li><p>PACELC更能满足实际操作中分布式系统的工作场景是更好的工程实现策略；</p></li><li><p>当partition (P)存在的场景下，需要在availability 和consistency (A and C)之间获得权衡，当时实际上分布式系统中绝大多数时间里partition (P)是不存在的，那么就需要在latency (L) 和 consistency (C)之间取得权衡。</p></li><li><p>availability在不存在partition (P)的场景下跟 latency关联,在partition (P)时跟reliable指标关联。</p></li><li><p>PACELC 可以在 latency vs consistency之间获得平衡</p></li><li><p>CAP 理论忽略了 一致性和时延之间的权衡</p></li></ol><p>PACELC建立在CAP之上，二者都描述了在一致性(Consistency)，可用性(Availability)和分区容忍性(Partition Tolerance)之间的限制和权衡。而PACELC更进一步描述了即使在没有Partition的场景下，也存在Latency和Consistency之间的权衡，从而为分布式系统的Consistency模型提供了一个更为完整的理论依据。</p><p>要保证系统的高可用（high availability）那么就必须复制数据，而分布式系统进行数据复制复制，就会出现在Consistency和Latency之间做个权衡的要求。</p><p>举个栗子，如下图所示，</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed%2Fconsistency-latency.png" alt="consistency-latency"></p><ol><li><p>在强一致性复制场景下，需要三副本都下盘才能返回ok给client端，Master向 Slave 复制数据，Latancy的限制是 20ms，有时候，slave 2 硬盘或网络出现故障，Master 往 Slave 复制数据的时延超过 20ms了，这个时候如果还一致等待 slave 2 返回结果再notify 给client就会出现性能和时延抖动，而且这种抖动是经常发生的长尾效应。</p></li><li><p>依据PACELC理论，我们可以在 consistency和Latency之间做个权衡，比如 slave 2 节点的时延超过 20ms了，就不等待slave 2 返回，master 和 slave 1 返回结果给client即可，如果 slave 2 出现 超时的 次数超过 5次那么就认为 这个节点可能出现故障，打个故障标签，进行后续的处理。</p></li></ol><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><p>[1] <a href="https://en.wikipedia.org/wiki/PACELC_theorem" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/PACELC_theorem</a></p><p>[2] CAP理论与分布式系统设计，S先生</p>]]></content>
      
      
      <categories>
          
          <category> distributed </category>
          
      </categories>
      
      
        <tags>
            
            <tag> distributed </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega stateSynchronizer</title>
      <link href="/2018/09/16/pravega-statesynchronizer/"/>
      <url>/2018/09/16/pravega-statesynchronizer/</url>
      
        <content type="html"><![CDATA[<h2 id="StateSynchronizer的设计"><a href="#StateSynchronizer的设计" class="headerlink" title="StateSynchronizer的设计"></a>StateSynchronizer的设计</h2><p>StateSynchronizer提供了一种方法，通过这种方法可以支持多个进程同时对一份数据进行写入和读取，并且使用了一种乐观检查的方法来保证数据的一致性。</p><p>这项工作保证每个进程都有一份数据的副本。所有的数据更新都是通过StateSynchronizer写入，它将这些数据附加到Pravega的段里。通过从段里消费数据来跟踪数据的最新变化，并且使用了有条件追加数据的方法提供了一致性保证。<br>这样可确保更新的过程只有在有最新数据时才可以继续执行更新。最后，为了防止段数据无节制地增长，我们使用了一种重写最新数据的简单方法，并截断旧数据，以便可以删除它。</p><p>当大多数更新与存储的总数据大小相比较小时，此模型运行良好，因为它们可以写成小的增量。与任何乐观并发系统一样，当众多进程都试图同时尝试更新相同的信息时，工作状态最差。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>这里有一个同步集合内容的具体示例，此外我们还有一个示例，即同步一组主机的成员资格。</p><p>想象一下，许多进程同时共享一个映射表的场景。这可以通过StateSynchronizer创建来完成，这将有助于协调对映射表的更改。每个客户端在内存中都有自己的一份映射表副本，可以通过将映射表生成器传递给StateSynchronizer来更新。每次尝试更新时，更新都会先记录到段中。除非传递给进程的映射表与已记录到段中的映射表一致，否则更新将失败。如果发生这种情况，则使用最新状态调用生成器以再次尝试。因此，更新的顺序由它们写入段的顺序定义。</p><h2 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h2><p>为此，我们使用了Pravega Segment Store Service的两个功能。</p><h3 id="条件追加"><a href="#条件追加" class="headerlink" title="条件追加"></a>条件追加</h3><p>附加方法可以指定追加期望的偏移量，如果追加数据失败，则不执行任何操作而是返回失败给客户端。</p><h3 id="截断段"><a href="#截断段" class="headerlink" title="截断段"></a>截断段</h3><p>截断段删除给定偏移之前的所有数据（此操作不会影响现有偏移量），对于低于此值的偏移量的任何读取都将失败，并且在此偏移下的任何数据都可以删除。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega常见问题</title>
      <link href="/2018/09/16/pravega-faq/"/>
      <url>/2018/09/16/pravega-faq/</url>
      
        <content type="html"><![CDATA[<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p><strong>什么是Pravega？</strong> Pravega是一个开源存储原语，为连续和无界数据实现Streams。</p><p><strong>“Pravega”是什么意思？</strong> “Pravega”是梵文中的一句话，指的是“速度快”的意思。</p><p><strong>Pravega与Kafka和Kinesis等系统类似吗？</strong> Pravega是以企业级存储为目标从头开始构建的流存储系统，支持恰好一次性，持久化等功能.Pravega是一个理想的流存储系统，专门用于流式数据的存储，比如来自实时应用的数据和物联网数据。</p><p><strong>我怎样才能参与这个开源系统？</strong> 开源加速了颠覆性创新。当Pravega创建时，毫无疑问，将它作为开源是有道理的。我们欢迎经验丰富的新开发人员的贡献。查看Github中的代码。有关如何参与的更多细节可以在这里找到。</p><p><strong>我该如何开始使用pravega？</strong>有关更多信息，请阅读入门指南，并访问一些示例应用程序的sample-apps repo。</p><p><strong>如果遇到问题，我在哪里可以获得帮助？</strong> 不要犹豫！如果您需要任何帮助，请联系邮件列表上的开发人员和社区。有关详细信息，请参阅加入社区。</p><p><strong>Pravega支持恰好一次语义吗？</strong> 绝对的支持。有关Pravega如何支持语义的讨论，请参阅主要功能。</p><p><strong>Pravega如何与Apache Flink等流处理器配合使用？</strong> Pravega的很多功能特性使其成为流处理器的理想选择。首先，通过flink connector, Pravega支持开箱即用。更加重要的是，Pravega支持恰好一次语义，使得开发精确的流处理应用变得更加容易。恰好一次语义，持久化和事务的这些特性的组合使得Pravega成为了Flink很好的合作伙伴，通过pravega可以提供端到端的一致性和恰好一次的语义。</p><p><strong>如何在流处理器和Flink之间进行自动缩放？</strong> 自动缩放是Pravega的一项基本功能，其流中的段数根据数据的摄取率的变化而变化。如果负载更高，速度更快，Pravega会通过添加段来增加流的容量。当数据速率或系统负载下降时，Pravega可以减少流的容量。当Pravega扩展和缩小流的容量时，如Flink的应用程序可以观察到此变化并且通过添加或减少使用流的作业实例的数量来响应。有关自动缩放的更多讨论，请参阅主要功能中的“Auto Scaling”部分。</p><p><strong>Pravega提供哪些一致性保证？</strong> Pravega提供了几项保证。持久化 - 一旦客户端确认数据，Pravega保证这个数据是受到保护的。排序 - 具有相同路由密钥的事件将始终按其编写顺序读取。恰好一次 - 写给Pravega的数据不会重复。</p><p><strong>为什么支持一致性和持久化对Pravega等存储系统如此重要？</strong> 主要是因为它使构建应用更容易。一致性和持久性是支持恰好一次语义的关键。如果没有恰好一次语义，就很难构建容错性应用程序，以确保一致性产生准确的结果。有关一致性和持久性保证的讨论，请参阅主要功能 .Pravega支持恰好一次的语义。</p><p><strong>Pravega支持事务吗？</strong> 是的。Pravega API允许应用程序在流上创建事务并将数据写入事务。数据被持久存储，就像写入Pravega的任何其他数据一样。当应用程序选择时，它可以提交或中止事务。提交事务时，事务中的数据将原子地附加到流中。有关Pravega事务支持的更多详细信息，请参见此处。</p><p><strong>Pravega是否支持跨不同路由键的事务？</strong> 是的。Pravega的事务本身就是一个流; 它可以有1个或多个段，写入事务的数据被放入与数据路由键关联的段中。提交事务时，事务数据将附加到流中的相应段。</p><p><strong>我是否需要安装HDFS才能使用Pravega？</strong> 是的。通常，您将为Pravega部署HDFS以用作其第2层存储。但是，对于简单的测试/开发环境，Pravega的所谓standAlone版本实现了自己的模拟HDFS。有关详细信息，请参阅Running Pravega指南。</p><p><strong>Pravega支持哪些第2层存储系统？</strong> Pravega旨在支持各种类型的第2层存储系统。目前，我们已将HDFS作为第2层存储。</p><p><strong>Pravega提供了哪些分布式计算原语？</strong> Pravega提供了一个名为StateSynchronizer的API结构。使用StateSynchronizer，开发人员可以使用Pravega在多个进程之间构建同步共享状态。此原语可用于构建各种分布式计算解决方案，如共享配置，领导者选举等。有关详细信息，请参阅主要功能中的“分布式计算原语”部分。</p><p><strong>Pravega推荐什么硬件配置？</strong> 对比控制面，数据面Segment Store 的要求更高，最少需要1GB内存和2核CPU，存储10GB起。控制面资源消耗少点，推荐的配置是1 CPU和0.5 GB内存起。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega关键特性二</title>
      <link href="/2018/09/16/pravega-key-features-2/"/>
      <url>/2018/09/16/pravega-key-features-2/</url>
      
        <content type="html"><![CDATA[<h2 id="自动缩放"><a href="#自动缩放" class="headerlink" title="自动缩放"></a>自动缩放</h2><p>与静态分区的系统不同，Pravega可以动态扩展单个数据流以适应数据摄取率的变化。</p><p>想象一下物联网的应用场景，数百万台设备为数千个数据流提供这些设备的相关信息。</p><p>想象一下Flink作业的管道，它处理这些流以从所有原始IoT数据中获取业务价值：预测设备故障，优化通过这些设备的服务交付，或在与这些设备交互时定制客户的体验。如果没有组件能够随着数据速率增加和减少的情况下自动伸缩的情况下，大规模地构建这样的应用程序是很困难的。</p><p>使用Pravega，可以轻松地弹性地和独立地扩展数据的摄取，存储和处理 - 协调数据管道中每个组件的扩展。</p><p>Pravega对自动缩放的支持始于Streams被划分为StreamSegments的想法。流可以具有一个或多个流段; 回想一下，Stream Segment是Stream的一个分区，与一系列路由密钥相关联。</p><p>写入Stream的任何数据都将写入与数据路由密钥关联的Stream Segment。写入者使用应用程序有意义的路由密钥，如customer-id，timestamp，machine-id等，以确保将类似数据组合在一起。 </p><p>流段是Pravega Streams中基本的并行度单位，具有多个流段的流可以支持更多的数据并行写入; 多个写入者将数据写入不同的流段，可能涉及群集中的所有Pravega服务器。在Reader端，Stream Segments的数量表示可能的最大读取并行度。如果Stream具有N个流段，则具有N个读取器的ReaderGroup可以并行地从流中消费数据。增加Stream Segments的数量，可以增加ReaderGroup中的Readers数量，以增加处理来自该Stream的数据的规模。当然，如果Stream Segments的数量减少，那么可以相应地减少读取器的数量。</p><p>Stream可以配置为更多数据写入Stream，那么就增加StreamSegments的数量，在数据量下降时就缩小segments规模。我们将此配置称为Stream的服务级别目标或SLO。Pravega监控输入到Stream的数据速率，并使用SLO从流中添加或删除流段。通过拆分流段来增加流段的数量。通过合并两个流段来减少流段的数量。请参阅  AutoScaling：有关Pravega如何管理StreamSegments的更多详细信息，Stream Segments的数量可能会随时间变化。</p><p>协调Pravega中Streams的自动缩放和应用程序缩小（在工作中）。这一点是可以实现的。使用Pravega提供的元数据，应用程序可以配置其应用程序组件的扩展; 例如，驱动Flink作业的实例数。或者，您可以使用Cloud Foundry，Mesos / Marathon，Kubernetes或Docker堆栈等软件来部署应用程序的新实例，以响应Pravega级别增加的并行性，或者减少Pravega规模以响应速率降低时终止实例数据的摄取。</p><h2 id="分布式计算原语"><a href="#分布式计算原语" class="headerlink" title="分布式计算原语"></a>分布式计算原语</h2><p>Pravega非常适合分布式应用，例如微服务; 它可以用作数据存储机制，用于微服务之间的消息传递和其他分布式计算服务，例如领导者选举。</p><p>State Synchronizer是Pravega API的一部分，它是在集群中以一致性和乐观并发性共享状态的基础。状态同步器基于Pravega中的基本条件写操作，因此只有当数据出现在Stream中的给定位置时才会写入数据。如果条件写入操作不满足条件，则失败。</p><p>因此，状态同步器是一种强大的同步原语，可用于群集中的共享状态，成员资格管理，领导者选举和其他分布式计算方案。</p><h2 id="写效率"><a href="#写效率" class="headerlink" title="写效率"></a>写效率</h2><p>Pravega写入延迟大约为毫秒级，无缝扩展以处理来自数千个并发客户端的高吞吐量读取和写入，使其成为物联网和其他时间敏感型应用的理想选择。</p><p>流是轻量级的，Pravega可以支持数百万个流，这可以避免静态配置流和需要预先分配少量固定数量的流以及管理或限制流资源。</p><p>Pravega中的写操作的时延是很低的，可以做到在10ms以下返回结果返回给Writer。此外，优化写入可以使的I / O吞吐量只受到网络带宽的限制; 持久性机制不是瓶颈。Pravega使用Apache BookKeeper来持久化所有写操作。BookKeeper可以非常有效地保留和保护数据。由于数据在写入操作被Writer确认之前受到保护，因此数据始终是持久的。正如我们在下面讨论的那样，数据持久性是存储原语的基本特征。为了进一步提高效率，对BookKeeper的写入通常涉及来自多个流段的数据，因此将数据保存到磁盘的成本可以通过多次写入操作来分摊。Pravega避免了持久化数据与性能的权衡问题。</p><p>读取也很有效，读取器可以在Stream的尾部或Stream历史的任何部分读取Stream。与一些基于日志的系统不同，它使用相同类型的存储进行尾部读写以及读取历史数据，而Pravega使用两种类型的存储。Stream的尾部位于所谓的第1层存储中。如上所述，写入由Apache BookKeeper实现。尾部读取由Pravega管理的内存缓存提供。事实上，BookKeeper仅在故障恢复方案中提供读取功能，Pravega Server已经崩溃并且正在恢复。BookKeeper的这种使用正是它的设计目标：快速写入，偶尔读取。Stream的历史部分在所谓的第2层存储中，针对具有高吞吐量的低成本存储进行了优化。Pravega使用高效的内存预读缓存，也获利于Streams通常以大的连续块读取的场景，并且HDFS非常适合那些大型，高吞吐量的读取。值得注意的是，尾部读取不会影响写入的性能。</p><h2 id="无限保留"><a href="#无限保留" class="headerlink" title="无限保留"></a>无限保留</h2><p>Streams中的数据可以在应用程序需要时保留，受限于可用数据量，这在第2层中使用云存储是无限制的.Pravega提供了一个方便的API来访问实时和历史数据。使用Pravega，可以有效地处理批量和实时应用程序; 另一个原因是Pravega是Kappa架构的优秀存储原语。</p><p>如果保留旧数据有价值，为什么不保留它？例如，在机器学习示例中，您可能希望定期更改模型并针对尽可能多的历史数据训练模型的新版本，以产生更准确的模型预测能力。使用Pravega自动分层，保留大量历史数据不会影响尾部读写的性能。</p><p>流的大小不受单个服务器的存储容量限制，而是仅受存储群集或云提供商的存储容量的限制。随着存储成本的降低，删除数据的经济动机也随之消失</p><h2 id="存储效率"><a href="#存储效率" class="headerlink" title="存储效率"></a>存储效率</h2><p>使用Pravega构建数据处理管道，结合批处理，实时和其他应用程序，而无需为管道的每个步骤复制数据。</p><p>考虑以下数据处理环境，它结合了使用Spark，Flink和/或Storm的实时处理; Haddoop批量; 某种基于Lucene的搜索机制，如弹性搜索全文搜索; 也许一个（或几个）NoSQL数据库支持微服务应用程序。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fpipeline.separate.replicas.png" alt="pipeline.separate.replicas"></p><p>使用传统方法，每个系统将单独摄取和复制一组源数据，例如来自IoT应用的传感器数据。您最终将获得pub / sub系统中受保护数据的3个副本，HDFS中的3个副本，Lucene中的3个副本，NoSQL数据库中的3个副本。当我们考虑以千兆字节为单位测量源数据时，由中间件类别分隔的数据复制成本变得非常昂贵。</p><p>考虑使用Pravega和适用于Pravega存储的中间件的相同管道：</p><p> <img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fpipeline.single.source.png" alt="pipeline.single.source"></p><p>使用Pravega，数据在一个地方被摄取和保护; Pravega为整个管道提供单一的事实来源。此外，由于大量数据存储在使用擦除编码的第2层中以有效地保护数据，因此数据的存储成本大大降低。</p><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>使用Pravega，您无需在性能，持久化和一致性之间达成妥协。Pravega提供持久的流数据存储，具有强大的一致性、顺序保证和出色的性能。</p><p>持久化是基本的存储原始要求。可能丢失数据的存储不是可靠的存储，基于这种存储的系统不能满足生产要求。</p><p>一旦确认写入操作，即使发生故障，数据也不会丢失。这是因为Pravega总是在写入操作返回到Writer之前将数据保存在受保护的持久存储中。</p><p>使用Pravega，Stream中的数据受到保护。可以将Stream视为记录系统，就像处理存储在数据库或文件中的数据一样。</p><h2 id="事务支持"><a href="#事务支持" class="headerlink" title="事务支持"></a>事务支持</h2><p>开发人员使用Pravega Transaction来确保将一组事件原子地写入流中。</p><p>Pravega事务是Pravega的Writer API的一部分。数据可以直接通过API写入Stream，或者应用程序可以通过Transaction写入数据。使用事务，Writer可以立即保留数据，然后决定是将数据附加到Stream还是放弃。</p><p>使用事务，仅在提交事务时才将数据写入Stream。提交事务时，写入事务的所有数据都以原子方式附加到Stream。由于事务的实现方式与Stream Segments相同，因此写入事务的数据与直接写入Stream的数据一样耐用。如果放弃了事务（例如，如果Writer崩溃），则中止事务并丢弃所有数据。当然，如果出现表明Writer应该丢弃数据的情况，应用程序可以选择通过API中止事务。</p><p>事务是将Flink工作链接在一起的关键。当Flink作业使用Pravega作为接收器时，它可以开始一个Transaction，如果它成功完成处理，则提交Transaction，将数据写入其基于Pravega的接收器。如果作业由于某种原因失败，则事务超时并且不写入数据。重新启动作业时，接收器中没有需要管理或清理的“部分结果”。</p><p>结合事务和Pravega的其他主要功能，可以将Flink工作链接在一起，让一个工作的基于Pravega的接收器成为下游Flink工作的来源。这使得整个Flink作业管道能够恰好具有一次端到端，保证了数据处理的顺序。</p><p>当然，跨多个Streams的事务可以与事务协调，因此Flink作业可以使用2个或更多基于Pravega的接收器为下游Flink作业提供源输入。此外，应用程序逻辑可以将Pravega事务与外部数据库（如Flink的检查点存储）进行协调。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega关键特性一</title>
      <link href="/2018/09/16/pravega-key-features-1/"/>
      <url>/2018/09/16/pravega-key-features-1/</url>
      
        <content type="html"><![CDATA[<p>本系列介绍了Pravega的一些关键特性。如果您已经熟悉Pravega的核心概念，那么这些概念对于理解本文会有所帮助。</p><h2 id="Pravega设计原则"><a href="#Pravega设计原则" class="headerlink" title="Pravega设计原则"></a>Pravega设计原则</h2><p>Pravega旨在支持新一代流式应用：这些应用处理大量的连续到达的数据，并且这些应用还对迟到的数据、无序到达的数据和发生故障时生成的数据进行准确的分析。有几个开源工具可以让开发人员构建这样的应用场景，例如Apache Flink，Apache Beam，Spark Streaming等。迄今为止，这些流式应用使用Apache Kafka，Apache ActiveMQ，RabbitMQ，Apache Cassandra和Apache HDFS等系统来摄取和存储数据。在pravega里，我们设想将摄取和存储这两个概念统一起来，因而pravega的工作重点是摄取和存储流数据。</p><p>Pravega从存储的角度处理流应用。它使这些流式应用能够连续不断的摄取流数据并永久地保存下来。作为分析历史数据的一部分，可以以低延迟（毫秒级）的方式访问这样的流数据，也可以提前几个月，甚至几年分析这样的历史数据。</p><p>Pravega的设计结合了使用Lambda架构构建流应用时的经验教训，也参考了大规模部署流应用时的挑战，这些应用始终以容错方式提供准确的结果。Pravega架构提供强大的持久化和一致性保证，为构建流式应用提供坚实的基础。</p><p>在Lambda架构里，开发人员使用复杂的中间件组合，其中包括批处理中间件以及Storm，Samza，Kafka等连续处理中间件。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Flambda.png" alt="lambda"></p><p>在lambda体系结构中，批处理用于提供准确但可能过时的数据分析，第二条路径在摄取数据时处理数据，原则上结果是无效的，这证明了第一条路径的合理性。使用此方法，应用程序逻辑上有两个副本，因为速度层的编程模型与批处理层中使用的编程模型不同。Lambda架构很难在生产中维护和管理。因此，这种大数据处理架构一直在对用户失去吸引力。最近，一种不同类型的大数据体系结构越来越受到关注，此架构不依赖于批处理数据路径。这种架构称为Kappa架构。</p><p>Kappa架构风格是针对Lambda架构太过于复杂的一种改进，依赖于专为流式传输而设计的组件，支持更强大的语义并提供快速准确的数据分析能力，Kappa架构是一种更简单的方法：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fkappa.png" alt="kappa"></p><p>在Kappa架构里只有一个数据路径要执行，而应用程序逻辑的实现只需要维护一个，而不是两个。借助适当的工具，为需要快速而准确地处理流数据的需求而构建，使得物联网，联网汽车，金融，风险管理，在线服务等领域设计和运行大数据应用变得更加简单。通过合适的工具，可以构建这样的流水线并为需要高容量和低延迟的大数据应用提供服务。</p><p>Kappa架构里流式应用通常需要处理多个阶段，任何实用的流分析系统都必须能够以数据流水线的形式适应各阶段的组合：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fpipeline.png" alt="pipeline"></p><p>对于数据管道，重要的是要考虑端到端的保证而不是每个组件的保证。例如，一个阶段保证exactly once语义而另一个阶段又不保证，这样是不行的。Pravega的目标是实现数据管道的设计和实现，并提供端到端的强一致保证。</p><p>Pravega - 为流世界重新设想的存储</p><p>Pravega引入了一个新的存储原语，即流，可以匹配无限数据的连续处理。在Pravega中，流是一个命名的，持久的，仅附加的和无限制的字节序列。有了这个原语，以及本文档中讨论的关键特性，Pravega是Flink等流处理引擎的最佳拍档。基于Pravega的关键特性，我们认为它将成为新一代面向流的中间件的基础存储原语。</p><h1 id="让我们来看看Pravega的主要特色。"><a href="#让我们来看看Pravega的主要特色。" class="headerlink" title="让我们来看看Pravega的主要特色。"></a>让我们来看看Pravega的主要特色。</h1><h2 id="Exactly-once语义"><a href="#Exactly-once语义" class="headerlink" title="Exactly once语义"></a>Exactly once语义</h2><p>恰好一次语义，我们的意思是Pravega确保数据不会重复，并且尽管失败也不会丢失任何事件。当然，这个声明附带了许多警告，就像任何其他承诺完全一次语义的系统一样，但是我们不要在这里深挖细节。一个重要的考虑因素是，一次性语义是Pravega的自然组成部分，是pravega从零开始设计时就考虑的首要目标之一。</p><p>为了实现一次性语义，Pravega Streams具有持久性，有序性，一致性和事务性。我们在下面的单独部分讨论持久性和事务性。</p><p>通过排序，我们的意思是Reader按照写入的顺序观测数据。在Pravega中，数据是与应用定义的路由密钥一起写入的，Pravega根据路由密钥提供顺序保证。两个具有相同密钥的数据总是被Reader按照它们写入的顺序读取。Pravega的排序保证允许数据被重放（例如，当应用程序崩溃时）并且重放的结果是相同的。</p><p>通过一致性，我们的意思是所有的reader都可以看到给定路由密钥的相同有序数据视图，即使失败也是如此。“大多数一致”的系统不足以构建准确的数据处理。</p><p>提供“至少一次”语义的系统可能会出现重复。在这样的系统中，数据生产者可能在某些情况下两次写入相同的数据。在Pravega中，写入是幂等的，由于重新连接而导致的重写不会造成数据重复。请注意，我们不保证来自源的数据是否已包含重复项。源数据对Pravega是不透明，pravega不会尝试删除源数据里的重复项。</p><p>然而，我们并没有将我们的注意力限制在写的完全一次语义上。我们还提供并正在积极致力于扩展功能，这些功能可实现数据管道的端到端的一次性。强一致性保证了Praveg的数据分析引擎的语义，如Flink实现了这种端到端的保证。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega相关术语</title>
      <link href="/2018/09/16/pravega-terminology/"/>
      <url>/2018/09/16/pravega-terminology/</url>
      
        <content type="html"><![CDATA[<h3 id="以下是与Pravega相关的术语表："><a href="#以下是与Pravega相关的术语表：" class="headerlink" title="以下是与Pravega相关的术语表："></a>以下是与Pravega相关的术语表：</h3><table><thead><tr><th style="text-align:left">术语</th><th style="text-align:left">定义</th></tr></thead><tbody><tr><td style="text-align:left">Pravega</td><td style="text-align:left">Pravega是一个开源存储原语，为连续和无界数据实现流式存储,Pravega是Flink最好的拍档。</td></tr><tr><td style="text-align:left">流</td><td style="text-align:left">一种可持久化，弹性，append-only，无限制的字节序列，具有良好的性能和强一致性。</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">流通过名称和范围来标识。</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">流由一个或多个数据流段组成。</td></tr><tr><td style="text-align:left">流段</td><td style="text-align:left">流的碎片</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">流中流段的数量会随着负载和缩放策略的变化而变化</td></tr><tr><td style="text-align:left"></td><td style="text-align:left">当没有发生缩放事件时，事件写入流中，具有相同路由密钥的事件存储在同一流段中，并且这些事件全部都是有序的. 当发生缩放事件时，与事件之后使用相同路由密钥K写入的事件相比，在缩放事件之前使用给定路由密钥K写入的流段数量发生了变化，给定路由密钥K写入的事件被存储在不同的流段中。流段可与Reader Groups相结合，流段的数量是并发读取流的最大的Reader数量。</td></tr><tr><td style="text-align:left">Scope</td><td style="text-align:left">流名称的命名空间。在一个scope内的流的名称必须是独一无二的。</td></tr><tr><td style="text-align:left">事件</td><td style="text-align:left">Stream中的字节集合。 事件与路由密钥相关联。</td></tr><tr><td style="text-align:left">路由密钥</td><td style="text-align:left">用于将消息路由到Reader的事件的属性。两个具有相同路由密钥的事件将以与它们所写的完全相同的顺序被Reader所读取</td></tr><tr><td style="text-align:left">Reader</td><td style="text-align:left">从一个或多个Streams读取数据的应用程序。</td></tr><tr><td style="text-align:left">Writer</td><td style="text-align:left">将数据写入一个或多个Streams的应用程序。</td></tr><tr><td style="text-align:left">Pravega Java客户端库</td><td style="text-align:left">应用程序用于与Pravega交互的 Java库</td></tr><tr><td style="text-align:left">ReaderGroup</td><td style="text-align:left">一个或多个Reader的命名集合，它们并发读取Stream。 Pravega为Reader分配了Stream  Segments，确保一个Stream Segments至少对应一个Reader，并且保证reader之间的平衡。</td></tr><tr><td style="text-align:left">位置</td><td style="text-align:left">Stream中的偏移量，代表了Reader的恢复点。如果Reader崩溃，可以从这个位置来恢复Reader，以便从这个故障点恢复对流的持续处理。</td></tr><tr><td style="text-align:left">1层存储</td><td style="text-align:left">短期，低延迟的数据存储，可确保写入Streams的数据的持久性。 第1层的当前实现使用   Apache Bookkeeper。 第1层存储保留了Pravega中最新的流。随着第1层中的数据老化，它将从第1 层移到第2层。</td></tr><tr><td style="text-align:left">2层存储</td><td style="text-align:left">Pravega存储的一部分，基于比较便宜的磁盘介质，如HDFS，DellEMC的Isilon或DellEMC的弹性云存储。</td></tr><tr><td style="text-align:left">PravegaServer</td><td style="text-align:left">Pravega的一个组件，其实现了Pravega数据面API，用于读取和写入Streams等操作。Pravega的数据面，也称为Segment Store，由一个或多个Pravega  Server实例组成。</td></tr><tr><td style="text-align:left">Segment Store</td><td style="text-align:left">PravegaServer的集合，它们聚合形成Pravega集群的数据面。</td></tr><tr><td style="text-align:left">Controller</td><td style="text-align:left">Pravega的一个组件，其实现了Pravega控制面API，用于创建和检索有关Streams的信息。 Pravega的控制面由Zookeeper协调的一个或多个Controller实例组成。</td></tr><tr><td style="text-align:left">Auto Scaling</td><td style="text-align:left">一个Pravega概念，基于缩放策略，它允许流中流段的数量随时间的推移而改变</td></tr><tr><td style="text-align:left">缩放策略</td><td style="text-align:left">一个流的配置项，它确定一个流中流段的数量如何随着时间的改变而改变 。  缩放策略有三种，Stream在任何给定时间都有其中一种。 - 固定数量的流段- 根据写入流的每秒字节数更改流段数 - 根据写入流的每秒的事件数更改流段数</td></tr><tr><td style="text-align:left">缩放事件</td><td style="text-align:left">缩放事件有两种类型：Scale-Up Event和Scale-Down Event。一个缩放事件触发自动缩放。放大事件是一种情况，其中负载的增加导致一个或多个流段被分割，从而增加流中的流段的数量。缩小事件是负载减少导致一个或多个流段合并的情况，从而减少流中的流段数。</td></tr><tr><td style="text-align:left">事务</td><td style="text-align:left">Stream写操作的集合，以原子的方式应用于Stream。事务中的所有字节要么都成功写入Stream，要么都没写入。</td></tr><tr><td style="text-align:left">状态同步器</td><td style="text-align:left">在Pravega之上构建的抽象，使用Pravega 段来支持复制状态的实现，以支持状态转换。状态同步器允许在多个进程之间共享一段数据，具有很强的一致性和乐观的并发性</td></tr><tr><td style="text-align:left">checkpoint</td><td style="text-align:left">一种事件，表示reader group内的所有reader都要持久化它们的状态。</td></tr><tr><td style="text-align:left">StreamCut</td><td style="text-align:left">StreamCut代表了流中的一致性位置。它包含一组针对单个流的段和偏移对，这些对表示给定时间点上的完整密钥空间。偏移总是指向事件边界，因此将没有偏移指向不完整事件。</td></tr></tbody></table><a id="more"></a> ]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega基本概念四 - 架构，小结</title>
      <link href="/2018/09/16/pravega-concepts-4/"/>
      <url>/2018/09/16/pravega-concepts-4/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>下图描述了Pravega的物理结构图：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fpravega.arch.new.png" alt="pravega.arch.new"></p><p>Pravega的架构符合软件定义存储（SDS）的语义，其控制面与数据面是分开的，Pravega数据面的集合统称为Segment Store。</p><p>controller实例组构成了Pravega的控制面，提供了创建、更新和删除Streams、检索有关Streams的信息、监控Pravega集群的健康状况、收集指标等的功能。为了实现高可用，通常有多个（建议至少3个）controller实例同时提供服务。 </p><p>Segment store实现Pravega的数据面。PravegaServers提供了在Streams中读写数据的API。Pravega中的数据存储由两层组成：第1层存储，提供短期、低延迟的数据存储，保证写入Streams。第2层存储提供数据的持久性、流数据的长期存储。Pravega使用 Apache Bookkeeper  实施第1层存储，并且支持使用HDFS，戴尔EMC的Isilon或戴尔EMC的弹性云存储（ECS）来实施第2层存储。第1层存储通常在Pravega集群内运行。第2层存储通常部署在Pravega集群之外。</p><p>分层存储对于提供快速访问Stream数据的组合非常重要，但也允许Streams存储大量数据。第1层存储会保留最近的Stream数据。随着第1层存储中的数据老化，它将进入第2层存储。</p><p>Pravega使用Apache Zookeeper作为Pravega集群中组件的协调机制。 </p><p>Pravega首先被构建为数据存储原语。Pravega经过精心设计，可充分利用软件定义存储，因此Pravega中存储的数据量仅受数据中心总存储容量的限制。就像您所期望的所有存储系统一样，一旦将数据写入Pravega，数据就会被持久存储。如果没有遇到连数据中心都被毁坏的灾难，Pravega中存储的数据永远不会丢失。</p><p>Pravega提供了一个用Java编写的客户端库，用于构建客户端应用程序，例如使用Flink作为分析应用程序。Pravega Java客户端库通过自我定制的TCP协议管理应用程序与Pravega之间的交互。</p><h2 id="概念小结"><a href="#概念小结" class="headerlink" title="概念小结"></a>概念小结</h2><p>Pravega中的概念总结如下：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fputting.all.together.new.png" alt="putting.all.together.new"></p><p>Pravega的客户端是writer和reader。writer将事件写入流中。Reader读取Stream中的事件。Reader被分组到ReaderGroups中以并行读取Stream。</p><p>Controller是服务端组件，用于管理Pravega的控制面。使用ControllerAPI创建、更新和列出流。</p><p>Pravega Server是一个服务端组件，用于实现读、写和其他数据面操作。</p><p>Streams是Pravega中的基本存储原语。Streams包含一组名为Events的数据元素。事件被writer附加到Stream的“尾部”。reader可以从Stream中的任何位置读取事件。</p><p>Stream被划分为一组Stream Segments。流中的stream segments数可以随时间变化。事件基于路由码写入到一个stream segment中。对于读取Stream的任何ReaderGroup，每个Stream Segment都分配给该ReaderGroup中的一个Reader。</p><p>每个流段都存储在Tier1和Tier2存储的组合中。Segment的尾部存储在Tier1中，提供低延迟的读写操作。Segment的其余部分存储在Tier2中，提供具有水平可扩展性和低成本的高吞吐量读取访问。</p><h2 id="关于分层存储的注意事项"><a href="#关于分层存储的注意事项" class="headerlink" title="关于分层存储的注意事项"></a>关于分层存储的注意事项</h2><p>为了实现Streams的有效实现，Pravega基于分层存储模型。事件存储于低延迟/高IOPS存储（第1层存储）和更高吞吐量存储（第2层存储）中。从API的角度来看，writer和reader对分层存储模型无需知晓。</p><p>Pravega基于仅附加日志数据结构。正如所观察到的，Log中实际上有三种数据访问机制：</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fanatomy.of.log.png" alt="anatomy.of.log"></p><p>所有写入活动以及大部分读取活动都发生在日志的尾部。写入被附加到日志中，并且许多客户端希望以到达日志的速度读取数据。这两种数据访问机制主要是需要低延迟 - 写入器的低延迟写入和读者对发布数据的近实时访问。</p><p>并非所有Reader都从日志的尾部读取; 一些reader想要从日志中的任意位置开始阅读。这些读取称为追赶读取。传统上访问历史数据是通过批量分析作业完成的，通常使用HDFS和Map / Reduce。但是，对于新的流应用程序，您只需访问日志即可访问历史数据和当前数据。一种方法是将所有历史数据存储在SSD中，就像我们使用尾部数据一样，但这可能会成本较高并迫使客户通过删除历史数据来节省成本。Pravega提供了一种机制，允许客户在日志的历史部分使用经济高效，高度可扩展的高吞吐量存储，这样他们就不必决定何时删除历史数据。基本上，如果存储足够便宜，为什么不保留所有的历史数据？</p><p>第1层存储用于快速持久地写入Streams，并确保从Stream的尾部读取尽可能快。第1层存储基于开源Apache BookKeeper。虽然不是必需的，但通常我们假设第1层存储通常在更快的SSD或甚至非易失性RAM上实现。</p><p>第2层存储提供高度可扩展，高吞吐量的经济高效存储。我们希望此层通常部署在机械磁盘上。Pravega异步迁移事件从第1层到第2层，以反映Stream数据的不同访问模式。第2层存储基于HDFS模型。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega基本概念三 - ReaderGroup，Transactions，State Synchronizers</title>
      <link href="/2018/09/16/pravega-concepts-3/"/>
      <url>/2018/09/16/pravega-concepts-3/</url>
      
        <content type="html"><![CDATA[<h2 id="ReaderGroup以及Checkpoint"><a href="#ReaderGroup以及Checkpoint" class="headerlink" title="ReaderGroup以及Checkpoint"></a>ReaderGroup以及Checkpoint</h2><p>Pravega为应用提供了在ReaderGroup上初始化Checkpoint的功能。使用Checkpoint的意图是通过使用一种特殊事件（检查点事件）来确保每个Reader能保存原来的使用状态，ReaderGroup中的每个Reader都可以创建一个“时间点”，借助这个“时间点“可以为reader提供状态的持久化以及保证这个状态一致性的功能。检查点完成后，应用可以通过恢复checkpoint(检查点)将ReaderGroup中的所有Reader恢复成这个检查点所代表的一致状态。</p><h2 id="Transactions（事务）"><a href="#Transactions（事务）" class="headerlink" title="Transactions（事务）"></a>Transactions（事务）</h2><p>Pravega支持事务。事务的想法是，Writer可以“批处理”一堆event并将它们作为一个处理单元提交到Stream中。这在某些场景是很有用的，例如，使用Pravega作为Flink作业的接收器。Flink作业可以不断产生一些数据的处理结果，并使用事务功能来持久化累积的处理结果。当在某个时间窗口（例如）的末尾时，Flink作业可以提交事务，因此使的处理结果可用于数据的下游处理，或者在出现错误的场景下，退出事务并且放弃整个处理结果。</p><p>Pravega的事务和类似方法（例如Kafka的生产者方式批处理）之间的关键区别在于能否持久化。当事件回到Writer时，添加到事务的事件是持久的。但是，在Writer提交Transaction之前，Reader不会看到事务中的事件。事务很像流; 事务与多个流段相关联。将事件发布到事务中时，事件本身将附加到事务的流段。假设Stream有5个段，当在该流上创建事务时，那么表示该事务也有5个段。当事件发布到事务中时，它被路由到相同编号的段，就像它被发布到Stream本身一样（如果事件将被放置在“真实”流中的段3中，那么它将出现在事务的段3中）。提交事务时，每个事务的段将自动附加到实际流中的相应段。如果流被中止，则事务其所有段以及发布到事务中的所有事件都将被从Pravega中删除。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Ftrx.commit.new.png" alt="trx.commit.new"></p><p>在提交事务之前，发布到事务中的事件永远不会被Reader看到。有关使用事务的更多详细信息，请参阅  使用Pravega：事务。</p><h2 id="State-Synchronizers（状态同步器）"><a href="#State-Synchronizers（状态同步器）" class="headerlink" title="State Synchronizers（状态同步器）"></a>State Synchronizers（状态同步器）</h2><p>Pravega是一个流存储; 但是Pravega也提供了在分布式计算环境中作为协调器的功能（类似zookeeper,etcd）。Pravega的State Synchronizer功能属于后一种。</p><p>状态同步器使用Pravega Stream为在集群中运行的多个进程之间共享的状态提供同步机制，从而使得更加的容易构建分布式应用。使用State Synchronizer，开发人员可以使用Pravega来读取状态，并且借助一致性和乐观锁功能对这些共享状态进行更改。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fstate.synchronizer.png" alt="state.synchronizer"></p><p>状态同步器可用于维护和保存应用的配置副本，状态同步器还可用于存储一个数据或具有数千个不同键值对的映射数据。实际上，Pravega本身就在内部使用State Synchronizer来管理分布在整个网络中的ReaderGroups和Readers的状态。</p><p>应用开发人员以类似于创建Writer的方式在Stream上创建State Synchronizer。状态同步器保留共享状态的本地副本，以便为应用提供快速访问数据的功能。对共享状态的任何修改都将通过StateSynchronizer写入Stream，以便跟踪对共享状态的所有更改。每个应用实例都使用状态同步器通过将更新提取到共享状态并修改数据的本地副本来保持最新的更改。通过状态同步器对共享状态的追加样式进行维护，保持一致性，确保仅对共享状态的最新版本进行更新。</p><p>状态同步器还支持可以“压缩”，压缩和删除旧的状态更新，以便只有最新版本的状态保留在后备流中。此功能可帮助应用开发人员确保共享状态不会未经检查。</p><p>在存储的总数据大小比较小时，状态同步器对共享状态的更新可以工作得最好，允许将这些数据写为小的增量。与任何乐观并发系统一样，当许多进程都试图同时更新同一条数据时，状态同步器并不是最佳状态。</p><p>更多有关使用状态同步器的详细信息，请参阅  使用Pravega：状态同步器。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega基本概念二 - Events, Segments, AutoScaling, Ordering</title>
      <link href="/2018/09/16/pravega-concepts-2/"/>
      <url>/2018/09/16/pravega-concepts-2/</url>
      
        <content type="html"><![CDATA[<h2 id="Stream-segments"><a href="#Stream-segments" class="headerlink" title="Stream segments"></a>Stream segments</h2><p>如下图，Stream由Stream Segments组成，而流段是流的分片或分区。</p><p><img src="http://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fstream.segment.png" alt="stream.segment"></p><h2 id="Event-event组成了stream-segment"><a href="#Event-event组成了stream-segment" class="headerlink" title="Event: event组成了stream segment"></a>Event: event组成了stream segment</h2><p>Stream Segment是事件（Event）的容器，event组成了流段，event存储在stream segment里，当事件被写入流时，会进行hash计算生成一个路由码，根据这个路由码，事件会被路由到一个流段中。Pravega使用一致性哈希算法将事件分配给流段。事件路由码被哈希后会生成一个“密钥空间”。然后密钥空间会被划分为多个分区，这些分区又对应于多个流段，采用一致性哈希算法可以确定将事件分配给哪个流段。</p><h2 id="AutoScaling：流段数量可变"><a href="#AutoScaling：流段数量可变" class="headerlink" title="AutoScaling：流段数量可变"></a>AutoScaling：流段数量可变</h2><p>当了流中的I/O负载上升或下降时，Stream中stream segments的数量会随着I/O负载增长或收缩 ，我们将此特性称之为AutoScaling。</p><p>参考下图，图中体现了路由码和时间之间的关系。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fsegment.split.merge.overtime.new.png" alt="segment.split.merge.overtime.new"></p><p>流在时间t0开始，具有可配置数量的段。如果写入流的数据速率不变，则段的数量不会改变。然而，在时间t1，系统注意到摄取率的增加并且选择将段1分成两部分。我们称之为Scale-up事件。在t1之前，具有哈希到密钥空间上部的路由密钥（值0.5-0.99）的事件将被放置在段1中，而散列到密钥空间下部的值（值0-0.49）将是放置在段0中。在t1之后，段1被分成段2和段3.段1被密封，它不再接受写入。此时，具有路由密钥0.7及以上的事件被写入段3，而在0.5和0.69之间的事件将被写入段2。  </p><p>我们还在时间t2看到另一个Scale-up事件，因为段0的路由键范围被分成段5和段4.此时，段0被封闭，因此它不接受进一步的写入。</p><p>覆盖密钥空间的连续范围的段也可以合并。在时间t3，段2和段5被合并到段6中以适应流上的负载的减少。</p><p>创建Stream时，会使用Scaling Policy配置Stream，该策略确定Stream如何响应其负载变化。目前有三种扩展策略：</p><ol><li><p>固定，流段的数量不随负载而变化</p></li><li><p>基于大小，当写入流的每秒数据字节数增量超过某个目标速率时，流段的数量增加。如果它低于某个流速时，会减少流段数。</p></li><li><p>基于事件的，与基于大小的扩展策略类似，不同之处在于它使用事件数而不是字节数。</p></li></ol><h2 id="Events-Stream-Segments-and-AutoScaling"><a href="#Events-Stream-Segments-and-AutoScaling" class="headerlink" title="Events, Stream Segments and AutoScaling"></a>Events, Stream Segments and AutoScaling</h2><p>我们之前提到当一个event被写入Stream的一个段中时，有时候需要AutoScaling，Stream Segments可以被看作是基于路由密钥和时间的事件的分组。在任何给定时间，在给定值的Routing Key内发布到Stream的event都将出现在同一个Stream Segment中。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Frk.segment.new.png" alt="rk.segment.new"></p><p>还值得强调的是，事件仅写入活动的流段。密封的段不接受写入。在上图中，在“现在”时，只有流段3,6和4处于活动状态，并且这三个流段覆盖了整个密钥空间。  </p><h2 id="Stream-Segments-and-ReaderGroups"><a href="#Stream-Segments-and-ReaderGroups" class="headerlink" title="Stream Segments and ReaderGroups"></a>Stream Segments and ReaderGroups</h2><p>流段对于理解ReaderGroup的工作方式非常重要。</p><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fsegment.readergroup.png" alt="segment.readergroup"></p><p>Pravega给ReaderGroup中的每个Reader分配了零个或多个Stream Segments用于读取数据。Pravega尝试平衡每个Reader分配的Stream Segments的数量。在上图中，Reader B1从2个Stream Segments中读取，而Reader Group中的每个其他读者只有1个Stream Segment可供读取。Pravega确保每个Stream Segment都会被配置给这个Stream的ReaderGroup中的一个Reader读取。当Reader被添加到ReaderGroup或Reader崩溃并从ReaderGroup中删除时，Pravega会重新分配流段，以便在Reader之间平衡流段。</p><p>流中的流段数确定了ReaderGroup中Reader的并行度的上限 - 流段越多，我们可以使用Stream的Reader的并行集合就越多。在上图中，Stream1有4个Stream Segments。这意味着最大的有效ReaderGroup将包含4个读者。上图中名为“B”的ReaderGroup并不是最优的。如果将另外一个Reader添加到ReaderGroup，则每个Reader将有1个Stream Segment进行处理，从而最大化读取并行性。但是，ReaderGroup中的读者数量增加到4以上，至少有一个读者不会被分配一个流段。</p><p>如果上图中的Stream1经历了Scale-Down事件，将Stream Segments的数量减少到3，那么所描绘的Reader Group B将具有理想数量的Readers。</p><p>借助AutoScaling特性，Pravega开发人员无需预先使用固定的流段个数配置其Streams - Pravega可以动态确定正确的流段个数。借助此特性，Pravega Streams可以自动增长或收缩以匹配数据输入的行为。任何Stream的大小仅受Pravega集群可用的总存储容量的限制; 如果您需要更大的流，只需向群集添加更多存储空间即可。</p><p>应用程序可以响应Stream中segments数量的变化，调整ReaderGroup中的Readers数量，以便在资源允许时保持最佳读取并行度。例如，在Flink应用程序中，这很有用，允许Flink增加或减少并行处理Stream的任务的数量，因为随着时间的推移会缩放事件。</p><h2 id="顺序保证Odering-Guarantees"><a href="#顺序保证Odering-Guarantees" class="headerlink" title="顺序保证Odering Guarantees"></a>顺序保证Odering Guarantees</h2><p>流包括可以随时间变化的一组段，在键区空间区域重叠的段具有已定义的顺序。</p><p>写入流的事件将写入单个段，并且相对于该段的事件具有顺序性，段内事件的存在和位置是强一致性的。</p><p>可以为Reader分配多个并行段（来自键空间的不同部分）。从多个段读取的Reader将交错段的事件，但每个段事件的顺序关联了一个段中。具体来说，如果s是一个片段，s的事件e~1和e~2使得e~1在e~2之前， Reader读取e~1和e~2，然后Reader将在e~2之前读取e~1。</p><p>这导致以下顺序保证：</p><ol><li><p>具有相同路由密钥的事件按其编写顺序被消费。</p></li><li><p>发送到特定段的具有不同路由键的事件将始终以相同的顺序被可见，即使Reader备份并重新读取它们也是如此。</p></li><li><p>如果某个事件已经被其Writer激活或已被Reader读取，则可以保证它将在所有后续读取的同一位置继续存在，直到被删除为止。</p></li><li><p>如果有多个读者读一个流并且他们都回到任何给定点，他们将永远不会看到关于该点的任何重新排序。（永远不会发生所选点之前读取的事件现在又再次出现的情况，反之亦然。）</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pravega 基本概念一 - Streams, Events, Writers, Readers, ReaderGroups</title>
      <link href="/2018/09/16/pravega-concepts-1/"/>
      <url>/2018/09/16/pravega-concepts-1/</url>
      
        <content type="html"><![CDATA[<p>Pravega是一个开源的流式存储系统，为连续和无界数据实现流式存储。本章节将分为一个系列讲述Pravega的一些基本概念。</p><h2 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h2><p>Pravega将数据组织到Streams中。Stream是一种持久，有弹性，append-only，无限制的字节序列，具有良好的性能和强大的一致性。Pravega Stream这个语义与当前很流行的面向消息的中间件（如RabbitMQ或Apache Kafka）中的“topic”类似但更灵活。</p><p>Pravega Streams是基于append-only的日志数据结构。通过使用append-only日志，Pravega可以快速将数据提取到可持久化的存储中，也可以支持流处理，工作流引擎，面向事件的应用程序，比如Flink，发布/订阅消息，NoSQL数据库（如时间序列数据库）（TSDB）等。</p><p>当开发人员在Pravega中创建Stream时，他/她会先给Stream定义一个名称，例如“IoTSensorData”或“WebApplicationLog20170330”。Stream的名称可以帮助其他开发人员了解存储在Stream中的数据类别。值得注意的是，Pravega Stream名称是在一个Scope内组织的。Scope是一个字符串，用于数据分类，它会向开发人员传达某种含义，比如“FactoryMachines”或“HRWebsitelogs”。Scope用作Stream名称的命名空间 - 所有Stream名称在Scope中都是唯一的。因此，Stream通过其Stream名称和Scope的组合唯一标识。Scope可用于按租户（在多租户环境中），按组织中的部门，按地理位置或开发人员选择的任何其他分类来命名。</p><p>Pravega的Stream大小无限制 – Pravega本身不会对Stream中可以有多少event或Stream中存储的总字节数施加以任何的限制。Pravega的设计原则是支持从几台机器的小规模到整个数据中心的大规模。</p><p>为了处理Stream中潜在的大量数据，Pravega Streams分为Stream Segments。Segment是stream中的Shard或Partition。我们稍后将在本文档中详细介绍Stream Segments。Stream Segments是一个重要的概念，但在我们深入了解Stream Segments之前，我们还需要介绍一些其他概念。</p><p>应用程序（例如从IoT传感器读取的Java程序）将数据写入Stream的尾部（前端）。应用程序（如Flink）也可以从Stream中的任何位置读取数据。多个应用程序还可以并行读写相同的Stream。弹性且可扩展地支持大量的Streams，支持大规模的数据和应用程序是Pravega的核心设计思想。在详细介绍reader和writer时，我们将会介绍应用程序如何读取和写入Streams。</p><h2 id="Event"><a href="#Event" class="headerlink" title="Event"></a>Event</h2><p>Pravega的客户端API允许应用程序根据event在Pravega中读取和写入数据。event是Stream中的一组字节。event可以像来自IoT传感器的温度读数只包含少量的字节一样简单，该传感器由时间戳，度量标识符和值组成。event可以是与用户点击网站相关联的Web日志数据，也可以是表示为一组字节的任何事物。应用程序使用标准Java序列化器和反序列化器来理解event，允许它们使用类似的技术在Pravega中读取和写入对象，以便从文件中读取和写入对象。</p><p>每个event都有一个路由码。路由码允许Pravega和应用程序开发人员推断哪些event是关联的。路由码只是开发人员用于将类似event组合在一起的字符串。路由码通常是从event中自然发生的数据派生出来的，例如“customer-id”或“machine-id”，但它也可能是一些人工字符串。路由码也可以类似于日期（按时间将event组合在一起），或者路由码可能是IoT传感器ID（按机器对event进行分组）。路由码对于定义Pravega 保证的精确读写语义非常重要，稍后我们将详细介绍。</p><h2 id="Writer-Reader-and-ReaderGroups"><a href="#Writer-Reader-and-ReaderGroups" class="headerlink" title="Writer Reader and ReaderGroups"></a>Writer Reader and ReaderGroups</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega%2Fproducer.consumer.client.new.png" alt="producer.consumer.client.new"></p><p>Pravega提供了一个用Java编写的客户端库，它为Writer和Reader应用程序实现了一个方便的API。Pravega Java客户端库封装了用于Pravega客户端和Pravega之间通信的协议。</p><p>Writer是一个创建event并将其写入Stream的应用。所有数据都可以通过append到Stream的尾部（前面）来写入。</p><p>Reader是一个从Stream读取event的应用。读者可以从Stream中的任何一点读取。一些Reader从Stream的尾部读取event。这些events将会尽可能快地被发送给这些reader。一些Reader从Stream的头部读取（称为追赶读取）。应用开发人员可以控制Reader开始读取的Stream中的位置。Pravega具有Position的概念，它表示Reader当前所在的Stream中的位置。位置对象可以用作恢复机制-应用保留Reader最后成功读取的位置，如果读失败了就从这个保存的位置从新开始读。</p><p>使用这种持久化Position对象的模式，Reader被组织成ReaderGroups。ReaderGroup是一个命名的Reader集合，它们一起并行读取给定Stream的事件。当通过Pravega数据平面API创建Reader时，开发人员包含它所属的ReaderGroup的名称。我们保证发布到Stream的每个事件都被发送到ReaderGroup中的一个Reader。ReaderGroup中可能有1个Reader，也可能有多个Reader。同一个Stream可以同时被不同的ReaderGroup读取。</p><p>您可以将ReaderGroup视为“复合阅读器”或“分布式阅读器”，它允许分布式应用程序并行读取和处理流数据，以便协调的ReaderGroup可以使用大量的流数据。在ReaderGroup中，并行处理流数据的Flink是ReaderGroup的一个很好的用例。</p><p>更多关于Pravega Reader和Writer如何使用的详细信息，请参阅  使用Pravega：基本Reader和Writer章节。</p><p>我们需要更详细地讨论Reader，ReaderGroup和Streams之间的关系以及Pravega提供的顺序保证。但是，我们需要先描述一下segment是什么。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega入门</title>
      <link href="/2018/09/16/pravegage-usage/"/>
      <url>/2018/09/16/pravegage-usage/</url>
      
        <content type="html"><![CDATA[<h2 id="Pravega入门"><a href="#Pravega入门" class="headerlink" title="Pravega入门"></a>Pravega入门</h2><p>了解Pravega最好方法就是自己动手部署一个，然后跑一把Pravega示例。</p><p>部署Pravega其实很简单，以下是步骤：</p><p>Java版本：Java 8</p><h3 id="下载Pravega"><a href="#下载Pravega" class="headerlink" title="下载Pravega"></a>下载Pravega</h3><p>可以从 <a href="https://github.com/pravega/pravega/releases" target="_blank" rel="noopener">https://github.com/pravega/pravega/releases</a> 下载Pravega编译好的发行版。如果您想自己构建Pravega，也可以自己下载代码并运行:</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$./gradlew distribution<br></code></pre></td></tr></table></figure><p>多细节可以查看Pravega <a href="https://github.com/pravega/pravega/blob/master/README.md" target="_blank" rel="noopener">README.md</a>。</p><p>解压:</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ tar xfvz pravega-0.1.0.tgz<br></code></pre></td></tr></table></figure><p>然后以standalone模式运行Pravega，这种模式会在本地机器上启动Pravega的所有组件。注意：这仅用于测试/演示目的，请勿在生产环境中使用！更多内容请 <a href="http://pravega.io/docs/latest/deployment/deployment/" target="_blank" rel="noopener">查看</a></p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ cd pravega-0.1.0<br>$ bin / pravega-standalone<br></code></pre></td></tr></table></figure><p>执行这个命令即可拉起一个本地化的pravega 集群， 这样就可以跑pravega。</p><h3 id="Pravega“Hello-World”示例"><a href="#Pravega“Hello-World”示例" class="headerlink" title="Pravega“Hello World”示例"></a>Pravega“Hello World”示例</h3><p>Pravega为示例维护一个单独的github库：https：//github.com/pravega/pravega-samples</p><p>Pravega依赖关系会自动从maven中心拉下来。注意：示例还可以使用本地编译的Pravega。有关这方面的更多信息，请参阅<a href="https://github.com/pravega/pravega/blob/master/README.md" target="_blank" rel="noopener">README.md</a>中maven发布的注释。</p><p>下载Pravega-Samples git库</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ git clone https://github.com/pravega/pravega-samples<br>$ cd pravega-samples<br></code></pre></td></tr></table></figure><p>生成示例程序</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ ./gradlew installDist<br></code></pre></td></tr></table></figure><p>运行示例“HelloWorldWriter”，将“hello world”消息作为事件写入Pravega stream。</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples<br>$ bin/helloWorldWriter<br></code></pre></td></tr></table></figure><p>HelloWorldWriter的输出</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">...<br>Writing message: &apos;hello world&apos; with routing-key: &apos;helloRoutingKey&apos; to stream &apos;examples / helloStream&apos;<br>...<br></code></pre></td></tr></table></figure><p>若想使用不同的参数运行HelloWorldWriter，更多信息请参阅pravegs-samples中的readme.md文件</p><p>运行示例“HelloWorldReader”</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples<br>$ bin/helloWorldReader<br></code></pre></td></tr></table></figure><p>示例HelloWorldReader的输出</p><figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs undefined">...<br>Reading all the events from examples/helloStream<br>...<br>Read event &apos;hello world&apos;<br>No more events from examples/helloStream<br>...<br></code></pre></td></tr></table></figure><p>有关HelloWorldReader的更多详细信息，请参阅pravega-samples中的readme.md文件</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pravega overview</title>
      <link href="/2018/09/15/pravega-overview/"/>
      <url>/2018/09/15/pravega-overview/</url>
      
        <content type="html"><![CDATA[<p>当前的大数据处理系统无论是Lamda架构还是Kappa架构都面临一个问题，即：“计算是原生的流计算，而存储却不是原生的流存储” 。</p><p>作为存储业界首屈一指的领导者，DELL EMC的存储专家们重新思考了这一基本的数据处理和存储规则，为这一场景重新设计了一种新的存储类型，即原生的流存储，命名为”Pravega”，在梵语里是“Good Speed”的意思。</p><p>Pravega是一种流数据存储系统，其具有可持久化，弹性，数据只追加，字节序列无限制，性能良好和强一致性的特点。并且根据Apache 2.0许可证开源，DELLEMC的存储专家们相信这一颠覆性的技术应该开源出来与开源社区一起拥有与推动，Pravega相应的介绍以及代码可以从pravega.io获得。</p><h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><ul><li><p>正好一次 – 不管是客户端、服务端还是网络出现了故障，Pravega都能确保每个事件都只被传递和处理正好一次（exactly-once）。</p></li><li><p>自动伸缩 – 不同于静态分区系统只有固定大小的存储空间，当数据采集速率发生变化时Pravega可以根据场景自动调整空间大小以自动适应数据规模的变化。</p></li><li><p>分布式计算原语 – Pravega具有和zookeeper一样的选主功能，支持进程间传递消息，支持数据存储，非常适用于分布式计算场景。</p></li><li><p>写入效率好 – 目前Pravega 的写入时延在毫秒级，还能无缝的扩展以支持数千个客户端的同时并发读写，是IOT和其他时延敏感型应用的理想选择。</p></li><li><p>无限保存 – 数据永远都在流中采集、处理和保存，对于实时数据和历史数据使用一样的处理范式。</p></li><li><p>高效存储 – Pravega构建了一种数据处理通道，支持将批处理，实时处理以及其他应用比如数据检索，都构建在一个数据处理通道内，无需为每个处理模式都保留一份数据副本。</p></li><li><p>持久化 – Pravega保证无需在高性能，可持久化和一致性之间做权衡，在客户端确认写入操作已完成之前，Pravega将一直存留并保护数据。</p></li><li><p>支持事务 - 开发人员使用Pravega事务来确保一组事件原子性的写入流中。</p></li></ul><a id="more"></a> <h2 id="Pravega的逻辑架构"><a href="#Pravega的逻辑架构" class="headerlink" title="Pravega的逻辑架构"></a>Pravega的逻辑架构</h2><p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/pravega/pravega_overview_1.jpg" alt="Pravega的逻辑架构"></p><p>计算与存储解耦，计算包括 Flink,Spark,一个自我开发的分布式检索系统。<br>存储层实现了一个流抽象层，一级高性能存储采用Bookeeper，二级冷数据存储<br>可以支持开源的HDFS，CEPH，GlusterFS，Swift，云存储等。与Kafka对比，最大区别在于Pravega是专门为流数据而生的原生的流存储。</p>]]></content>
      
      
      <categories>
          
          <category> pravega </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pravega </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/09/13/hello-world/"/>
      <url>/2018/09/13/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><strong>启动新的技术网站:[<a href="http://www.changping.me]">www.changping.me]</a>, [<a href="http://www.yuncunchu.org]以及微信公众号上的文章将迁移到本站。" target="_blank" rel="noopener">www.yuncunchu.org]以及微信公众号上的文章将迁移到本站。</a></strong></p>]]></content>
      
      
      
    </entry>
    
  
  
    
    <entry>
      <title></title>
      <link href="/404.html"/>
      <url>/404.html</url>
      
        <content type="html"><![CDATA[<!DOCTYPE HTML><html><head>  <meta http-equiv="content-type" content="text/html;charset=utf-8;">  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">  <meta name="robots" content="all">  <meta name="robots" content="index,follow">  <link rel="stylesheet" type="text/css" href="https://qzone.qq.com/gy/404/style/404style.css"></head><body>  <script type="text/plain" src="http://www.qq.com/404/search_children.js" charset="utf-8" homepageurl="/" homepagename="�ص��ҵ���ҳ">  </script>  <script src="https://qzone.qq.com/gy/404/data.js" charset="utf-8"></script>  <script src="https://qzone.qq.com/gy/404/page.js" charset="utf-8"></script></body></html>]]></content>
      
    </entry>
    
    <entry>
      <title>archives</title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h2><ul><li>毕业于中国科学技术大学，获得硕士研究生学历学位</li><li>曾就职于Marvell、AMD等，现于EMC负责大数据流处理平台的设计以及产品交付</li></ul><h2 id="联系方式"><a href="#联系方式" class="headerlink" title="联系方式"></a>联系方式</h2><ul><li>Email ： <a href="mailto:wu@changping.me" target="_blank" rel="noopener">wu@changping.me</a></li><li>Github : <a href="https://github.com/wuchangping" target="_blank" rel="noopener">https://github.com/wuchangping</a></li></ul><p>–</p>]]></content>
      
    </entry>
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
  
</search>
