<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#1D2D2D">
    <meta name="msapplication-TileColor" content="#1D2D2D">
    
    
    
    <meta name="keywords" content="flink, pravega, kubernetes, docker, streaming, storage">
    
    
    <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png">
    
    
    <link rel="icon" type="image/png" sizes="192x192" href="/favicons/android-chrome-192x192.png">
    
    
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    
    
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    
    
    <link rel="mask-icon" href="/favicons/safari-pinned-tab.svg" color="#1D2D2D">
    
    
    <link rel="manifest" href="/favicons/site.webmanifest">
    
    
    <meta name="msapplication-config" content="/favicons/browserconfig.xml">
    
    
    
    <link rel="shortcut icon" type="image/x-icon" href="/favicons/favicon.ico">
    
    
    <link rel="stylesheet" type="text/css" href="/css/normalize.css">
    <link rel="stylesheet" type="text/css" href="/css/index.css">
    
    <link rel="stylesheet" type="text/css" href="/css/sidebar.css">
    
    
<link rel="stylesheet" type="text/css" href="/css/page.css">
<link rel="stylesheet" type="text/css" href="/css/post.css">

    <link rel="stylesheet" type="text/css" href="/css/custom.css">
    <link rel="stylesheet" type="text/css" href="/css/atom-one-dark.css">
    <link rel="stylesheet" type="text/css" href="/css/lightgallery.min.css">
    <script type="text/javascript" src="/js/jquery.min.js"></script>
    <script defer type="text/javascript" src="/js/util.js"></script>
    <script defer type="text/javascript" src="/js/scrollspy.js"></script>
    <script defer type="text/javascript" src="/js/fontawesome-all.min.js"></script>
    <script defer type="text/javascript" src="/js/lightgallery.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-fullscreen.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-hash.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-pager.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-thumbnail.min.js"></script>
    <script defer type="text/javascript" src="/js/lg-zoom.min.js"></script>
    
    <script defer src="/js/busuanzi.pure.mini.js"></script>
    
    
    
    <script defer type="text/javascript" src="/js/index.js"></script>
    
    <script defer type="text/javascript" src="/js/custom.js"></script>
    <title>常平的技术博客 - 认识世界是为了改造世界</title>
  </head>
  <body itemscope itemtype="http://schema.org/WebPage" lang="zh_CN"  data-spy="scroll" data-target=".list-group">
    
<header id="header" class="header" style="background: #1D2D2D;">
  <div class="container">
    <div class="header-container">
      <div class="header-title">
        <h1 class="title"><a href="/">常平的技术博客</a></h1>
        <h2 class="subtitle">www.changping.me</h2>
      </div>
      
      <div class="logo">
        <img src="/images/logo.png" alt="logo">
      </div>
      
    </div>
    <nav id="nav" class="nav">
      <a id="nav-toggle" class="nav-toggle" aria-hidden="true"><i class="fas fa-bars" aria-label="切换导航栏"></i></a>
      <ul id="menu" role="menubar" aria-hidden="false">
        
        <li role="menuitem"><a href="/" class="current">首页</a></li>
        
        <li role="menuitem"><a href="/archives">全部</a></li>
        
        <li role="menuitem"><a href="/categories">分类</a></li>
        
        <li role="menuitem"><a href="/tags">标签</a></li>
        
        <li role="menuitem"><a href="/about">关于</a></li>
        
      </ul>
    </nav>
  </div>
</header>


    <main id="main" class="main">
      <div class="container">
        <div class="main-container">
          <div class="content">
            

<div id="index" class="index page">
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2022/04/17/ai-distributed-training-framework-1/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2022/04/17/ai-distributed-training-framework-1/" itemprop="url">分布式训练 - 第5篇 - 分布式训练服务框架基本原理与架构解析</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2022-04-17T17:10:38+08:00">2022-04-17 17:10:38</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <p>​                </p>
<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><font color="#FF8C00">1. 概述</font></h2><p>分布式训练服务框架与集合通信库的组合构成了分布式训练的整体服务软件栈，在第3篇、第4篇文章里已经剖析完集合通信的相关内容，而本文会以Horovod为例介绍数据并行下分布式训练服务框架的基本原理以及进行架构解析。当前，在分布式训练里分布式训练服务框架需要解决以下几个核心问题 ：</p>
<ul>
<li>计算与通信同步耦合问题：如果反向传播一产生一份梯度，就马上对其调用全局AllReduce，计算与通信同步耦合，容易造成死锁同时性能也会很不如意；</li>
<li>计算时间与通信时间串行问题：神经网络是分层的，梯度计算的过程是数据加载，然后前向传播算出损失值，再反向传播算出梯度，而反向计算时梯度是从输出层往输入层方向一层一层产生的，在有些模型里，如果需要等所有的梯度都计算完毕才能触发全局AllReduce，那么对性能的影响也会很大；</li>
<li>梯度生成的落后者问题：集群内每个计算节点的同一份梯度的产生不一定都是同一时刻的，如果梯度没有全部生成就发起对这个梯度的全局规约，否则容易造成训练出来的模型精度不达标或者不收敛的问题；</li>
<li>梯度融合问题：如果每一份梯度都触发一次全局AllReduce，在梯度Tensor较多的神经网络训练里，整体的训练系统性能会变得极低；</li>
<li>易用性问题：从TensorFlow，PyTorch迁移过来需要改的代码需要极少，从单卡训练迁移到多卡训练需要改动的代码也需要极少；</li>
<li>可移植问题：支持多种多样的深度学习训练框架，比如 TensorFlow、PyTorch、MxNet等，也能支持多种多样的通信库，比如openMPI、NCCL、Gloo、CCL、RCCL等；</li>
<li>可靠性问题：在集群训练的过程中网络时不可靠的、计算卡是会出故障的、服务器是会出故障的、系统软件也是会出Bug的，这些因素造成了分布式训练过程中还存在可靠性问题，如何解决这个问题也是一个难题。</li>
</ul>
<p>软件是由人实现的，解析一个软件系统最难的地方在于从庞杂的代码里倒推出背后实现它的人的设计意图，为了更好的理解Horovod，本文会基于以上这几个分布式训练的核心问题，以Horovod为例介绍分布式训练服务框架的基本原理以及进行架构解析。</p>
<h2 id="2-基础知识"><a href="#2-基础知识" class="headerlink" title="2. 基础知识"></a><font color="#FF8C00">2. 基础知识</font></h2><h3 id="2-1-单卡训练"><a href="#2-1-单卡训练" class="headerlink" title="2.1 单卡训练"></a>2.1 单卡训练</h3><p>神经网络的训练，本质上就是Y=F(x)的迭代，通过反复输入X、输出Y，使得神经网络的参数变化与输入输出间的复杂关系拟合。在神经网络训练的过程中，通过输入数据利用梯度下降的方法进行迭代从而优化神经网络参数，并最终输出神经网络模型。而神经网络可以看作一种运算模型，其由大量的神经元（节点）相互联接构成，其由输入层、隐藏层以及输出层组合而成（如下图左侧所示）。神经元(neuron)是神经网络的基本计算单元，也被称作节点(node)，它可以接受来自其他神经元或外部数据的输入，然后计算出一个输出（如下图右上角所示）。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-framework-single-card.png" alt="单卡训练"></p>
<p>如上图右下角所示，在单卡训练迭代中，基于并行梯度下降法，会有以下操作：</p>
<p>第一步，读取部分数据，并且将数据加载进训练卡的存储空间；</p>
<p>第二步，对模型进行前向传播计算，从输入层往输出层一层一层的进行计算，得到损失差LOSS；</p>
<p>第三步，对模型进行反向传播计算，从输出层往输入层一层一层的进行计算，得到梯度值，注意这一步会把每一层都计算出一个梯度张量（Gradient Tensor）出来；</p>
<p>第四步，将新的到的梯度与部分数据 作为新的输入，重新开始以上步骤的迭代。</p>
<p>在这一步里有一个很重要的与性能优化相关的信息是反向传播是每一层输出一个梯度张量，以及反向传播是从输出层往输入层一层一层的进行计算的，这一点信息可以用通信隐藏性能优化与梯度融合优化。</p>
<h3 id="2-2-多卡训练"><a href="#2-2-多卡训练" class="headerlink" title="2.2 多卡训练"></a>2.2 多卡训练</h3><p>以数据并行随机梯度下降法( SGD )为例，多卡神经网络的训练过程如下图，与单卡训练相比，多卡训练多了梯度全局规约的过程：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-framework-multi-card.png" alt="多卡训练"></p>
<p>第一步，通过Broadcast操作将第一个节点参数同步到集群内的所有的训练卡上，保证每个计算节点的初始参数是一致的，同时训练脚本在多个计算节点上运行，每个计算节点包含了整体的模型参数；</p>
<p>第二步，将数据样本切片分发到整个集群内的个计算节点（训练卡）上并且通过数据流水技术将数据样本加载进训练卡的高速内存空间内，作为输入X;</p>
<p>第三步，每个训练卡在其数据样本上运行前向传播，计算出损失差LOSSi；</p>
<p>第四步，对计算出的LOSSi进行反向传播，得到梯度GRADi，这一步也需要注意得是每一层都会计算出一个梯度，同时梯度是以输出的Tensor来表示的；</p>
<p>第五步，所有的训练卡计算出来的部分梯度，在主机内及主机之间通过集合通信进行全局归约(AllReduce)得到全局梯度；</p>
<p>第六步，最后再将这个全局梯度作为参数进行更新，再进行以上2-5步骤的迭代从而获得新的梯度。</p>
<p>以上2-6步骤就是多卡并行梯度下降的基本思想，即多个计算节点通过分片的数据样本进行梯度计算，得到分区梯度后，再通过全局梯度规约以及将这个聚合好的梯度作为新的参数进行更新，从而实现并行梯度下降。</p>
<h2 id="3-几个核心问题"><a href="#3-几个核心问题" class="headerlink" title="3. 几个核心问题"></a><font color="#FF8C00">3. 几个核心问题</font></h2><p>在本章节里会解读本文概述里提到的分布式服务框架需要解决的几个与性能、易用性等相关的几个核心问题，并且以Horovod为例讲述Horovod是如何解决这个几个难题的。</p>
<h3 id="3-1-计算与通信解耦"><a href="#3-1-计算与通信解耦" class="headerlink" title="3.1 计算与通信解耦"></a>3.1 计算与通信解耦</h3><p>在神经网络的训练过程中，每一神经网络层都会计算出一个梯度，同时梯度是以输出Tensor来表示的，如果反向传播一计算出一个梯度就马上调用通信去做梯度规约，将计算与通信同步耦合，那么整体的性能的表现就会很差。比如一个ResNet-50 v3的梯度张量个数是153个，如果一计算出一个梯度就马上进行通信，假设计算梯度花了1ms，通信这个梯度花了 500ms，那么这个过程就是 501ms，总体上就需要501x153 = 76653ms，即近76.6s才能完成一次梯度迭代。而将计算与通信解耦，计算的归计算，通信的归通信，通过性能优化策略减少通信的次数，既能提升整体训练性能也能避免某些死锁问题，比如计算梯度grad i的时候花了很长时间，而通信线程一直在等待这个梯度，表现出来就是死锁现象。</p>
<p>Horovod采用计算与通信分离的设计思想，解耦了计算过程与通信过程，从而提升了整体训练的性能与可靠性。如下图的Horovod逻辑架构图所示，从图中可以看出Horovod解耦了计算与通信，其将框架层计算出来的梯度request信息push 到一个消息队列message_queue里，同时将梯度信息push到一个Tensor_table里，再通过控制层在后台起一个loop线程，周期性的从消息队列里读取梯度消息，在控制层集群的节点之间协商达成一致后，再进行消息分发触发训练行为。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-Horovod-architecture.png" alt="Horovod逻辑架构"></p>
<p>如上图可看出，Horovod从下到上分为7层：物理层、链路层、数据传输层、控制层、消息层、框架层以及用户层。框架层，控制层以及数据传输层体现了Horovod的核心设计理念，即：框架层，用户可以自定义Op，以插件的形式hack进框架；在控制层，worker节点与master节点之间协商达成触发训练行为的约定；在数据传输层，服务器内以及服务器之间采用集合通信库传输数据。</p>
<p>本质上Horovod的整体设计理念之一遵循的是生产者消费者模式，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-horovod-msgqueue.png" alt="生产者-消费者"></p>
<p>在Horovod里每个计算节点都会有有两个核心线程：Execution thread 和 Background thread ：</p>
<ul>
<li>生产者Execution Thread 是用来做梯度计算的，在TensorFlow、PyTorch之类的之类的训练框架计算出梯度Tensor后，将Tensor 信息push进tenor_table队列，同时将Tensor的request信息push进message_queue队列;</li>
<li>消费者Background thread 是做集合通讯以及全局Allreduce的，后台线程会每隔一段时间轮询消息队列，拿到一批Tensor信息之后，会进行相应的操作。</li>
</ul>
<h3 id="3-2-通信隐藏"><a href="#3-2-通信隐藏" class="headerlink" title="3.2 通信隐藏"></a>3.2 通信隐藏</h3><p>神经网络是分层的，在训练的过程中，先是数据加载，然后前向传播算出LOSS，再反向传播算出梯度，而反向计算时梯度是从输出层往输入层方向一层一层产生的，如果需要等所有的梯度都计算完毕才能触发全局AllReduce，对性能不是很友好。如下图所示，计算时间与通信时间是串行的，如果能将全局梯度规约的通信时间与计算时间想办法并行起来，将通信时间隐藏在计算时间之内，那么就能节约梯度的训练时间从而提升分布式训练系统整体的训练性能。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-wfbp-1.png" alt="通信隐藏"></p>
<p>如下图所示，将计算出来的梯度进行分桶触发异步Allreduce，一边反向传播计算梯度，一边做部分梯度的全局规约通信，从而达到将通信时间隐藏在计算时间内的效果。而Horovod为达成这一效果，Background thread 会每隔一段时间轮询梯度消息队列里的梯度信息，获取了可以过全局规约的梯度后，就进行全局规约操作，而这个时间其他的梯度还在计算过程中，通过调整轮询的时间间隔从而达到调整梯度分桶的效果。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-wfbp-2.png" alt="通信隐藏"></p>
<h3 id="3-3-梯度协商"><a href="#3-3-梯度协商" class="headerlink" title="3.3 梯度协商"></a>3.3 梯度协商</h3><p>神经网络的每一层对应一个梯度Tensor，在分布式训练集群里每张训练卡对同一份梯度计算产生的时间是有差异的，当集群内每个计算节点的同一神经网络层的同一梯度都产生时，才能发起对这个梯度的全局AllReduce规约，否则容易造成丢梯度，训练出来模型精度不达标或者模型不收敛。比如在一个128卡的训练集群里，同一份梯度是对应同一个神经网络模型里的同一层神经网络的，只有每张训练卡上都计算出了同一层神经网络的梯度 才能对这一层神经网络的梯度进行全局规约，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-grad-state-1.png" alt="梯度分层"></p>
<p>Horovod设计了一种梯度状态协商机制，它将 计算节点Rank0 作为coordinator（master），其余的rank1-N节点进程为worker，由coordinator来协商确定同一份梯度是否在每个计算节点上都已经计算出来，只有在每个计算节点上都计算出来的同一梯度才可以进行全局规约操作。在Horovod里每个计算节点上都有一个message_queue以及tensor_table，而在coordinator节点上除此之外，还有一个message_table用于保存可以进行全局Allreduce的梯度请求次数信息。Horovod 控制面的ComputeResponseList 函数里实现了这一梯度的协商过程，在从message_queue获取了本节点生成的梯度信息后，coordinator会与其他节点协商这个梯度是否都计算出来，这一过程是阻塞进行的，这个协商过程如下图：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-grad-state-2.png" alt="梯度状态协商"></p>
<p>一个梯度是否能满足全局规约AllReduce的协商过程如下：</p>
<p>首先，集群内的每个计算节点进程都会往coordinator Rank0发送一个 tensor的请求request，表示说本节点这一层神经网络的梯度已经生成，比如tensor1，每个rank都会往rank0 发送一个本梯度tensor1已经计算出来的请求信息；</p>
<p>第二步，coordinator接收到节点的梯度协商请求后（包括本节点），会把收到的tensor请求次数进行累加，并将这个信息记录在message_table里，当这个梯度的请求信息达到集群内节点的个数时，比如在N个节点的集群，一个神经网络层的梯度tensor的通信请求出现了N次，那就表示在本集群里所有的计算节点都已经发出了对该梯度tensor的通信request，这就表明这个梯度tensor是符合全局规约要求的，就能进行集合通信全局规约，不符合要求的梯度tensor将继续留在message_table中，直到条件符合为止；</p>
<p>第三步，再接着coordinator会将满足全局allreduce规约条件的梯度Tensor通过response返回给其他节点，告诉其他节点这个梯度可以启动全局规约AllReduce。</p>
<p>经过这几步的协商达成梯度全局状态一致的目的，从而避免梯度丢失造成的模型精度不达标、不收敛或者进程死锁问题。</p>
<h3 id="3-4-梯度融合"><a href="#3-4-梯度融合" class="headerlink" title="3.4 梯度融合"></a>3.4 梯度融合</h3><p>神经网络的每一层都能对应一个梯度，假设每生成一个梯度就进行一次全局规约时，100个梯度就需要进行100次全局通信100次全局规约，而通信对训练的性能有巨大的影响，这种情况表现出来的效果就是分布式训练集群的整体性能极差。通过梯度融合计算将多个梯度合成一个，从而减少全局规约的次数能大幅提高分布式训练的训练性能，如下图所示，将N个小梯度Tensor合成两个，能将全局通信的次数减少到2次，从而大幅提升训练性能，在Horovod里这个功能对TensorFusion特性。但这个特性也会与<strong>3.2通信隐藏</strong>特性相冲突，需要根据具体情况进行合理的调试优化。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/5/image-tensorfusion.png" alt="tensorfusion"></p>
<h3 id="3-5-易用性"><a href="#3-5-易用性" class="headerlink" title="3.5 易用性"></a>3.5 易用性</h3><p>从TensorFlow，PyTorch等框架迁移到Horovod需要改的的代码极少，horovod接入方式比较简单，与原生训练框架对比，主要的区别在于：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs undefined">1，初始化 Horovod，包括机器资源的分配：<br>horovod.init()<br><br>2，向每个进程分配XPU资源， 典型的设置是 1 个 XPU 一个进程，即设置 local rank：<br><br>config.gpu_options.visible_device_list = str(hvd.local_rank())<br><br>3，对原优化器进行包装，分布式优化器将梯度计算委托给原始优化器，使用allreduce或allgather对梯度求平均，然后应用这些平均梯度：<br><br>opt=hvd.DistributedOptimizer(opt)<br><br>4， 将初始化参数从rank 0广播给其他进程(rank表示进程序号)，实现参数的初始化，确保所有节点的初始化参数保持一致：<br>hvd.BroadcastGlobalVariablesHook(0)：<br></code></pre></td></tr></table></figure>
<h3 id="3-6-可移植"><a href="#3-6-可移植" class="headerlink" title="3.6 可移植"></a>3.6 可移植</h3><p>可移植问题，Horovod通过 OP和OpKernels的插件化机制支持多种多样的深度学习训练框架，比如 TensorFlow、PyTorch、MxNet等。基于的opKernels的可定制化机制，Horovod自定义了Op然后hack了数据链路层的通信协议，从而达到在多个深度学习框架之间可移植。</p>
<h3 id="3-7-可靠性问题"><a href="#3-7-可靠性问题" class="headerlink" title="3.7 可靠性问题"></a>3.7 可靠性问题</h3><p>在集群训练的过程中网络时不可靠的、计算卡是会出故障的、服务器是会出故障的的，这些因素造成了分布式训练过程中需要考虑训练集群的可靠性，Horovod结合集合通信库Gloo对外提供了弹性训练的特性，但可靠性不只是弹性训练就能完全解决的，它还有更多的系统级的问题需要解决，因此可靠性问题留着一个后续研究问题，不在本文阐述。</p>
<h2 id="4-优点缺点、改进点"><a href="#4-优点缺点、改进点" class="headerlink" title="4. 优点缺点、改进点"></a><font color="#FF8C00">4. 优点缺点、改进点</font></h2><p>选择一个框架也是辩证的，在获得它优点的同时也得接受它的缺点，Horovod的优点、缺点以及改进点描述如下：</p>
<h3 id="4-1-Horovod优点"><a href="#4-1-Horovod优点" class="headerlink" title="4.1 Horovod优点"></a>4.1 Horovod优点</h3><ul>
<li>简单易用、可移植，并且支持弹性训练提升了可靠性；</li>
<li>不依赖于某个框架，其通过MPI机制独立建立了一套分布式训练服务系统；</li>
<li>将计算与通信分离，完成了allreduce、allgather等集合通信工作，实现了规模可扩展；</li>
<li>巧妙的通过间隔轮询的机制支持通信时间隐藏，并且完成了梯度协商从而保证训练出来的模型是可收敛、精度达标的；</li>
<li>支持梯度融合，支持将小的tensor合并成一个大的tensor再进行通信传递，从而减小通信操作的额外开销；</li>
<li>自带压缩算法，可以减少集合通信的数据量；</li>
</ul>
<h3 id="4-2-Horovod的缺点"><a href="#4-2-Horovod的缺点" class="headerlink" title="4.2 Horovod的缺点"></a>4.2 Horovod的缺点</h3><ul>
<li>与GPU绑定，对新的训练加速设备的支持不够友好，缺乏设备插件化的机制，要添加一个新的训练加速设备比较困难；</li>
<li>所有的代码都与CUDA绑定，所有的性能优化机制都是针对GPU的，对新的DSA架构的芯片基本忽视；</li>
<li>弹性训练特性比较复杂，很难在生产上使用起来；</li>
<li>的Message_queue，Tensor_table缺乏容错机制，如果丢失数据容易造成丢tensor，从而影响整体模型的收敛与精度；</li>
</ul>
<h3 id="4-3-Horovod的改进点"><a href="#4-3-Horovod的改进点" class="headerlink" title="4.3 Horovod的改进点"></a>4.3 Horovod的改进点</h3><ul>
<li>简单易用的插件化支持新的训练芯片；</li>
<li>即支持SIMT架构芯片的性能优化，也支持DSA架构的芯片性能优化；</li>
<li>支持消息队列、张量表的容错，支持Rank 0 容错机制；</li>
</ul>
<h2 id="5-思考题"><a href="#5-思考题" class="headerlink" title="5. 思考题"></a><font color="#FF8C00">5. 思考题</font></h2><ul>
<li>问题1，将通信时间隐藏在计算时间内能有助于提升训练系统的整体性能，但这一特性是针对SIMT芯片的架构的进行性能优化的，如果DSA芯片不能支持这一特性，那应该如何优化Horovod从而大幅提升整体的训练性能？（可以确定这一定是能做到的）</li>
<li>问题2，梯度协商的过程中，每个梯度都需要协商一次，在梯度较多，网络规模较大的集群里，这一特性也会影响性能，如何进行优化才能有效提升Horovod性能？</li>
<li>问题3，不同的模型对梯度融合有不同的要求，那么梯度融合需要融合到什么程度才能有效提升性能？</li>
</ul>
<p>可以说明的是，这三个问题解决后还能继续提升Horovod在DSA架构芯片上的整体的分布式训练系统级性能。</p>
<h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a><font color="#FF8C00">6. 小结</font></h2><p>本文介绍了分布式训练的基础知识以及剖析了分布式训练服务框架所面临的几个核心问题，以Horovod为例从计算与通信解耦、通信隐藏、梯度协商、梯度融合、易用性以及可移植这几个角度倒推了分布式训练服务框架背后的设计意图，从而帮助大家能更好的理解分布式训练服务框架。</p>
<p>日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="7-作者简介"><a href="#7-作者简介" class="headerlink" title="7. 作者简介"></a><font color="#FF8C00">7. 作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习高级软件主管工程师、架构师，前EMC资深首席工程师，主要工作背景在深度学习、大数据、云计算、分布式中间件以及Linux内核领域。</p>
<h2 id="8-参考资料"><a href="#8-参考资料" class="headerlink" title="8. 参考资料"></a><font color="#FF8C00">8. 参考资料</font></h2><p>[1] <a href="https://www.changping.me">https://www.changping.me</a><br>[2] <a href="https://horovod.ai" target="_blank" rel="noopener">https://horovod.ai</a><br>[3] <a href="https://www.cnblogs.com/rossiXYZ/p/14910959.html" target="_blank" rel="noopener">https://www.cnblogs.com/rossiXYZ/p/14910959.html</a><br>[4] <a href="https://zhuanlan.zhihu.com/p/374575049" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/374575049</a></p>
<h2 id="9-版权申明"><a href="#9-版权申明" class="headerlink" title="9. 版权申明"></a><font color="#FF8C00">9. 版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/AI/" rel="tag"><i class="fas fa-tags"></i>AI</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2022/04/10/ai-distributed-training-coll-topo/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2022/04/10/ai-distributed-training-coll-topo/" itemprop="url">分布式训练 – 第4篇 - 分布式训练常用的网络结构及集合通信拓扑算法</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2022-04-10T15:56:38+08:00">2022-04-10 15:56:38</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><font color="#FF8C00">1. 概述</font></h2><p>在深度学习的分布式训练里，Ring AllReduce拓扑算法奠定了数据并行训练的集合通信基础，但集合通信拓扑不只是仅有Ring Allreduce，经典的集合通信拓扑算法还有2D-Ring/Hierarchical Ring AllReduce，halving and doubling AllReduce，Butterfly AllReduce，2D-Torus AllReduce，2D-Mesh AllReduce，double binary tree等。拓扑算法很多，但也不是所有的拓扑算法都能满足实际的生产需求的，这需要具体问题具体分析、具体场景具体设计。</p>
<p>集合通信的难点在于需要在固定的网络互联结构的约束下进行高效的通信，集合通信拓扑算法与物理网络互联结构强相关，为了发挥网络通信的效率，也不是说就能随意发挥通信拓扑算法，更多的是在效率与成本、带宽与时延、客户要求与质量、创新与产品化等之间进行合理取舍。</p>
<p>充分发挥训练加速卡与网络的效率是通信拓扑算法的初衷，但除了设计高效的集合通信拓扑算法外，分布式训练中需要解决的通信难题还有：网络是异构的，网络带宽是有限的，主机内PCIE SWITCH是有亲和性的，网络是会出故障的，节点是有落后者效应的，设备成本是需要考虑的，数据中心是有部署约束的，用户是有多租户要求的等，这些属于产品化的范畴不在本文阐述。</p>
<h2 id="2-网络互联结构"><a href="#2-网络互联结构" class="headerlink" title="2. 网络互联结构"></a><font color="#FF8C00">2. 网络互联结构</font></h2><p>分布式训练的集合通信拓扑算法与物理的网络互联结构强相关，而网络互联结构又多种多样，因此，本文需要先对网络互联结构进行约束，依据生产中常用的、既定的互联结构设计集合通信算法，网络互联结构描述如下：</p>
<h3 id="2-1-服务内网络互联结构"><a href="#2-1-服务内网络互联结构" class="headerlink" title="2.1 服务内网络互联结构"></a>2.1 服务内网络互联结构</h3><p>以一台集成了8张训练加速卡的服务器为例，如下图:</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-nvlink.png" alt="服务器内互联结构"></p>
<center>图片来源于《volta-architecture-whitepaper》，版权归原作者所有</center>

<p>这台服务器内的网络互联情况如下：</p>
<p>1）在这台服务器内，8张训练加速卡通过私有协议连接组成多个主机内的物理ring环，且可双工；</p>
<p>2）服务期内网络带宽 NVLINK&gt;PCIE switch &gt; QPI；</p>
<p>3）加速卡1、2、3、4之间两两全互联，加速卡5,、6、7、8之间两两全互联，2、5、3、8之间非全互联；</p>
<p>4）加速卡1、4与网卡NIC1 挂在同一个PCIE Switch上，具有亲和性，加速卡2、3与网卡NIC2挂在同一个PCIE Switch上，具有亲和性，而PCIE Switch之间也互联，因此 加速卡 1、2、3、4 与网卡NIC 1、NIC2具备亲和性，它们无需通过CPU的QPI线进行通信；</p>
<p>5）加速卡5、8与网卡NIC3 挂在同一个PCIE Switch上，具有亲和性，加速卡6、7与网卡NIC4挂在同一个PCIE Switch上，具有亲和性，而PCIE Switch之间也互联的，因此 加速卡 5、6、7、8 与网卡NIC 3、NIC4具备亲和性，它们也无需通过CPU的QPI线进行通信；</p>
<p>6）网卡可根据需要 选择 1张、2张、4张或8张，最多可以采用8张RDMA物理网卡；</p>
<h3 id="2-2-服务器间网络互联结构"><a href="#2-2-服务器间网络互联结构" class="headerlink" title="2.2 服务器间网络互联结构"></a>2.2 服务器间网络互联结构</h3><p>以一个训练加速卡集群为例，如下图是一个常用的CLOS互联架构方案：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-net-topo.png" alt="整体网络拓扑结构"></p>
<p>在这个集群内，其网络互联情况如下：</p>
<p>1）集群内每台服务器自带高速RDMA网卡，通过RDMA 交换机在主机间两两全互联；</p>
<p>2）交换机组成CLOS架构，分为Spine与Leaf交换机，当然也可以是更为高端的Spine、Leaf合一的高端交换机；</p>
<p>3）RDMA网卡与Leaf交换机互联，每台服务器的RDMA网卡数量根据成本与性能考虑，可以是1张、2张+每卡虚拟化4卡、4张+每卡虚拟化2卡或8张；</p>
<h3 id="2-3-高速网卡及其虚拟化使用"><a href="#2-3-高速网卡及其虚拟化使用" class="headerlink" title="2.3 高速网卡及其虚拟化使用"></a>2.3 高速网卡及其虚拟化使用</h3><p>RDMA网卡是双工的且可虚拟化，在这里每台服务器可根据成本、性能的考虑选用1张、2张、4张或8张，且在服务器内左右对称，如下图：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-net-nic.png" alt="网卡配置"></p>
<p>从成本与效率的角度考虑，每台服务器内的网卡可以是以下配置：</p>
<ul>
<li>1张物理RDMA网卡，不进行虚拟化，直接用双工通道，适合选用2D/Hierarchical Ring拓扑算法；</li>
<li>2张物理RDMA网卡，可以每张虚拟化出4个虚拟网卡，2X4共8卡，适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>4张物理RDMA网卡，可每张虚拟化出2个虚拟网卡，4X2共8卡，适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>8张物理RDMA网卡，不需要虚拟化，直接采用双工通道，适合选用2D-MESH、2D-Torus拓扑算法；</li>
</ul>
<p>在实际的分布式训练生产集群中，集合通信算法也可以结合RDMA网卡端口（包括虚拟化的）的具体个数进行设计，而拓扑算法的选择也是需要根据成本与效率的进行合理取舍的。</p>
<h3 id="2-4-网络结构抽象"><a href="#2-4-网络结构抽象" class="headerlink" title="2.4 网络结构抽象"></a>2.4 网络结构抽象</h3><p>网络根据连接情况可分为ring结构、mesh结构、 torus 结构以及tree结构，基于以上的服务器内网络互联结构、服务器间网络互联结构以及网卡的具体情况，可以抽象出一个网络结构，即二维环面网络：Torus 网络，而Torus网络横向与纵向都可以看成ring结构，因此相应的拓扑算法基本上就是Ring-Based 集合通信拓扑算法。如下图：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dtorus-logictopo.png" alt="Torus网络结构"></p>
<p>TORUS网络是常见的大规模并行计算机的互连网络，在上图这个Torus网络里：</p>
<p>1）横向：主机内8卡通过私有连接协议，比如CXL/CCIX/NVLINK等组成一个或多个ring，如上图的黄色连接线，横向8卡组成二维Torus的横向维度；</p>
<p>2）纵向：主机间通过RDMA（RoCE/IB）网卡、交换机互联组成1到8个ring，如上图的红色连接线，纵向采用RDMA网卡组成二维Torus的纵向维度；</p>
<p>3）根据物理网卡数量、网卡虚拟化以及PCIe Switch亲和性的实际情况：</p>
<ul>
<li>每台服务器1张网卡可组成主机间一个ring，网卡与XPU0 挂载同一个PCIE switch上，依据最佳实践原则（比如性能、成本、客户要求等），适合选用2D/Hierarchical Ring拓扑算法；</li>
<li>两张网卡可组成主机间两个ring或者经过虚拟化组成8个ring，根据PCIE SWITCH亲和性原则，一张网卡与XPU0挂在同一个pcie switch，另一张网卡与XPU4挂在同一个pcie switch，依据最佳实践原则（比如性能、成本、客户要求等），适合选用2D-MESH、2D-Torus拓扑算法；</li>
<li>4张网卡、8张网卡以此类推，也是根据PCIE SWITCH亲和性原则进行连接，主机间RDMA物理网卡不够就虚拟化网口来凑，并且要服务器内的RDMA出口端口数左右平衡，依据最佳实践原则（比如性能、成本、客户要求等），也是适合选用2D-MESH、2D-Torus拓扑算法，这样才能发挥多张网卡以及XPU的算力优势。</li>
</ul>
<p>4）更复杂的Torus网络组合关系还可以如下图，从横向只有 主机内的8卡纵向只有主机间的RDMA互联，扩展到 横向与纵向 主机内互联与主机间互联混合，但本文仅限于在横向8卡的二维Torus网络下进行拓扑算法选择与设计，因此不展开讲述。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-torus-topo.png" alt="Torus网络结构"></p>
<h2 id="3-常用的通信拓扑算法"><a href="#3-常用的通信拓扑算法" class="headerlink" title="3. 常用的通信拓扑算法"></a><font color="#FF8C00">3. 常用的通信拓扑算法</font></h2><p>Torus 网络结构可以解读本文中的物理网络互联结构的一切，而Torus网络的横向与纵向都可以看成ring结构，因此，相应的集合通信拓扑算法都可以看成是Ring-Based 集合通信拓扑算法。</p>
<h3 id="3-1-Ring-AllReduce"><a href="#3-1-Ring-AllReduce" class="headerlink" title="3.1 Ring AllReduce"></a>3.1 Ring AllReduce</h3><p>在分布式训练中，Ring 是最基础的互联结构，在本文中Ring AllReduce的应用场景是在服务器内将8张加速卡组环通信进行分布式训练。每个XPU都是这个主机内互联环上的一个计算节点，每个节点都有一个前向和一个后向，它只会向它的前向接收数据，并向它的右向发送数据，如下图所示，8张XPU 通过主机内的私有互联网络组成一个环，当然因为这些通信网络是双工的，这8张XPU训练加速卡也可以看成是通过多个逻辑环互联起来的，同时缺点是，如果这个ring太大，Ring Allreduce的效率也会变得很低。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-ring-topo.png" alt="ring拓扑"></p>
<p>Ring Allreduce 有两种组合实现策略：1）先Reduce后broadcast；2）先ScatterReduce后AllGather，这两个策略执行后都会让每个XPU节点得到一样的平均梯度，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-allreduce.png" alt="allreduce"></p>
<h4 id="3-1-1-Reduce-broadcast"><a href="#3-1-1-Reduce-broadcast" class="headerlink" title="3.1.1 Reduce +broadcast"></a>3.1.1 Reduce +broadcast</h4><p>在Reduce + broadcast里，reduce先将8张卡的梯度reduce sum到master节点 XPU0 上，再通过broadcast将这个总的平均梯度复制给其他XPU，如下图：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-ring-reducebroadcast.png" alt="ring reduce broadcast"></p>
<p>Reduce + broadcast这种策略有几个比较大的缺点：1）8张卡的数据都reduce sum到一张卡，假设每张卡的梯度是100MB，8张卡就是800MB，这可能存在XPU 0计算很久，而其他7张卡空闲的情况存在，整体效率不高；2）XPU0 的网络带宽可能会成为瓶颈，8张卡的数据都只能通过XPU0的互联网络进行reduce和broadcast，在数据量比较大的场景 XPU0的带宽成为瓶颈；3）8张XPU不都是两两全互联的，因此，要把8张卡的数据一次Reduce或broadcast，这一点受限于网络互联条件做不到，那么就需要采用 ring或tree的策略进行reduce或broadcast，这样效率也不高。</p>
<h4 id="3-1-2-ScatterReduce-AllGather"><a href="#3-1-2-ScatterReduce-AllGather" class="headerlink" title="3.1.2 ScatterReduce + AllGather"></a>3.1.2 ScatterReduce + AllGather</h4><p>Ring AllReduce 的Ring ScatterReduce + Ring AllGather策略组合里，每个 XPU只会从前向接受数据，并发送数据给后向，其算法主要分为：</p>
<ul>
<li>ScatterReduce：这一步会先scatter拆分数据块再进行reduce，并且在执行完毕后，每张XPU都会包括一个完整的经过融合的同维梯度；</li>
<li>AllGather：这一步会进行全局Gather同步，最后所有 XPU都会得到完整的大的整个梯度；</li>
</ul>
<p>Ring ScatterReduce + Ring AllGather是效率比较高的 Ring AllReduce 组合策略，这个策略考虑到了XPU上的梯度可能很大的情况，比如一个梯度有400MB，在scatterreduce阶段就会先被拆分成 ring上XPU个数份，比如主机内XPU个数等于8，那么 这400MB 就会被 拆分成8份，每份50MB，从而减少了加速卡的计算量以及节约带宽。此外，scatterReduce通过将数据拆分成小块，同时并发进行scatterReduce，从而将通信时间隐藏在计算时间内进而提高Ring AllReduce的效率。</p>
<h5 id="3-1-2-1-ScatterReduce"><a href="#3-1-2-1-ScatterReduce" class="headerlink" title="3.1.2.1 ScatterReduce"></a>3.1.2.1 ScatterReduce</h5><p>首先， ScatterReduce先将梯度拆分为N个更小的块，N等于ring里XPU个数，8张卡就拆分成8份，然后进行N-1次scatterreduce迭代。在第一轮迭代中XPU 0上的A0传递给XPU1上A1并相加，XPU1上的B1传递给XPU2上的B2并相加，XPU 2上的C2传递给XPU3上C3并相加，XPU3上的D3传递给XPU4上的D4并相加，以此类推，过程如下图左侧：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-ring-scatterreduce.png" alt="ring acatterreduce"></p>
<p>接下来，XPU还会进行N-2次 ScatterReduce 迭代，在每次迭代过程中，XPU都会从前向接收一个小梯度块并累加到自己的梯度块中，并且也会向其后向发送一个小梯度块，每个XPU接收和发送的小梯度块在每次迭代中都是不同的，这样经过迭代，到最后，每个XPU将有一个完整的同维梯度，该块梯度中包含所有XPU中该块对应的所有梯度的总和，如上图右侧的累加和部分。</p>
<h5 id="3-1-2-2-Allgather"><a href="#3-1-2-2-Allgather" class="headerlink" title="3.1.2.2 Allgather"></a>3.1.2.2 Allgather</h5><p>在scatterReduce迭代完成之后，每个XPU都会得到一个同维度的完整的梯度累加值，将这些完整的累加值复制到其他的加速卡后，才算完成allReduce。Allgather的迭代次数与scatterReduce是相同的，也都需要进行N-1次（N是ring上的XPU卡数）迭代，但是不同于ScatterReduce的是allGather没有reduce的过程，只有数值的复制。这样迭代到最后，每个XPU都得到大的拆分前的梯度的完整累加值，如下图演示了这一过程，从第一次迭代开始，到最后AllGather拿到整体的结果。这里头的具体过程就不在这里描述了，可以查相关资料。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-ring-allgather.png" alt="image-20220409203458896"></p>
<p>Ring AllReduce 实现简单，在ring较少时，效率也较高，但是在ring比较大时需要的网络节点跳数变得比较大，通信时延增加，因此效率也会降低。比如，一个1000张XPU的 ring，这里头网络的跳数 是N-1= 1000-1 =999， 同时传输的过程中，传输效率还受效率最低、带宽最低的XPU的限制，这时网络上的时延会变得巨高，这个时候ring allreduce拓扑算法就变得不大适用这个场景，同时如果在异构网络里涉及网络的不同连接方式，Ring AllReduce也不大适合使用，因此就需要采用另外的更适合网络结构的更高效的集合通信拓扑算法来进行优化。</p>
<h3 id="3-2-2D-Ring-AllReduce"><a href="#3-2-2D-Ring-AllReduce" class="headerlink" title="3.2 2D-Ring AllReduce"></a>3.2 2D-Ring AllReduce</h3><p>如果一台2.1里的服务器只配置了一张RDMA网卡，每台服务器通过RDMA交换机互联，这个集群的网络是异构的（如下图），那么Ring AllReduce拓扑算法就不适用了，这个时候，对于这个网络拓扑结构比较适合的是2D-Ring AllReduce也叫Hierarchical Ring AllReduce。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dring-topo.png" alt="2D-RING 拓扑"></p>
<p>经过抽象，可以将这个网络结构表达成如下的Torus结构：</p>
<p>横向：每台服务器8个XPU节点，每个XPU节点通过私有协议网络互联；</p>
<p>纵向：每台服务器通过一张RDMA网卡NIC 0 通过交换机互联，这个网卡NIC0 与XPU0 挂在同一个PCIE switch上，满足具备亲和性条件，XPU0上的梯度可以通过NIC 0 与其他服务器上的XPU进行全局规约。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dring-topo-1.png" alt="2D-RING TOPO"></p>
<p>2D-Ring AllReduce的过程如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dring.png" alt="2D-RING allreduce"></p>
<p>第1步，先进行主机内Ring AllReduce，也可以是 Ring Reduce或者根据主机内的互联情况选用的分层reduce方式，将8张卡上的梯度累加到Master节点 XPU0 上；</p>
<p>第2步，进行主机间XPU 0的 Ring AllReduce，将每台服务器的XPU0上的数据进行全局规约；</p>
<p>第3步，进行主机内Broadcast，将XPU0上的梯度复制到服务器内的其他XPU上</p>
<p>2D-Ring AllReduce能充分发挥异构网络的优势，将主机内、主机间的网络带宽充分利用起来。但是XPU的利用率也不是很高，比如在做主机间的Ring AllReduce，每台服务器内的其他7张XPU是处于空闲状态的。</p>
<p>再假设，如果每台服务器配置了 2张/4张/8张RDMA网卡，这个时候 2D-RING AllReduce又难以将网络的优势发挥出来，那么就需要选用 2D-Torus/2D-Mesh AllReduce拓扑算法。</p>
<h3 id="3-3-2D-Torus-AllReduce"><a href="#3-3-2D-Torus-AllReduce" class="headerlink" title="3.3 2D-Torus AllReduce"></a>3.3 2D-Torus AllReduce</h3><p>考虑到服务器内PCIE SWITCH 的亲和性问题，2D-Torus至少需要配备2张 左右对称的RDMA网卡才能发挥这个拓扑算法的优势。在这个集群里主机内每张卡都通过私有的通信协议组成Ring，而主机间，可以通过RDMA网卡（包括虚拟化出来的）与RDMA交换机将XPU两两互联，这个网络也是异构的，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dmesh-topo.png" alt="设备互联拓扑"></p>
<p>经过抽象，可以将这个网络结构表达成如下的Torus结构：</p>
<ul>
<li>横向：每台服务器8个XPU节点，每个XPU节点通过私有协议网络互联；</li>
<li>纵向：每台服务器通过至少2张RDMA网卡NIC 0 /NIC 1通过交换机互联，这个网卡NIC0 与XPU0、1、2、3 挂在同一个PCIE switch上，具备亲和性条件，XPU0、1、2、3上的梯度数据可以通过NIC 0 与其他服务器上的XPU进行交换。网卡NIC1 与XPU4、5、6、7 挂在同一个PCIE switch上，具备亲和性条件，XPU4、5、6、7上的梯度数据可以通过NIC 1 与其他服务器上的XPU进行交换；</li>
<li>当然如果网卡是4个或者8个，也可以根据PCIE SWITCH的亲和性情况合理安排XPU与NIC的对应关系。</li>
</ul>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2d-logictopo.png" alt="2D 拓扑"></p>
<p>2D-Torus AllReduce的过程如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dtorus-allreduce.png" alt="2dtorus allreduce"></p>
<p>第1步，横向，先进行主机内Ring ScatterReduce，将主机内8张卡上的梯度进行拆分与规约，这样经过迭代，到最后每个XPU将有一个完整的同维梯度，该块梯度包含所有XPU中该块所对应的所有梯度的总和（参考3.1.2.1 scatterReduce)</p>
<p>第2步，纵向，进行主机间N个（N等于服务器内XPU个数，这里是8个）纵向的 Ring AllReduce，将每台服务器的XPU0-XPU7上的数据进行集群内纵向全局规约；</p>
<p>第3步，横向，进行主机内AllGather，将XPUi(i=0-7)上的梯度复制到服务器内的其他XPU上；</p>
<p>2D-Torus AllReduce能充分挖掘XPU的效率以及发挥异构网络里多网卡的优势，将XPU以及主机内、主机间的网络带宽优势充分利用起来。此外，除了 2D-Torus AllReduce外，2D-Mesh AllReduce也能发挥类似效率。</p>
<h3 id="3-4-2D-Mesh-AllReduce"><a href="#3-4-2D-Mesh-AllReduce" class="headerlink" title="3.4 2D-Mesh AllReduce"></a>3.4 2D-Mesh AllReduce</h3><p>2D-Mesh AllReduce的主要思想也是分层，与2D-Torus AllReduce类似，都是水平和垂直两个方向，但是有点差异，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dmesh-allreduce.png" alt="2D-MESH allreduce"></p>
<p>不同于2D-Torus AllReduce的拓扑算法，2D-Mesh AllReduce 过程是：</p>
<p>第1步，横向，先进行主机内Ring AllReduce 将主机内的8张XPU的梯度都进行规约；</p>
<p>第2步，纵向，进行主机间N个（N等于主机内XPU个数，这里是8个）纵向的 Ring AllReduce；</p>
<p>经过这两步，完成了整体的梯度累加，2D-Mesh AllReduce 也能充分发挥XPU与多网卡异构网络的优势，将XPU与主机内、主机间的网络带宽优势充分利用起来。这里的2D-Mesh与Google论文上的有点差异，主要是吸取了其分层的思想而不是复制其一样的设计。理论上2D-Mesh AllReduce对比 2D-Torus AllReduce，主机间AllReduce用的是 主机内8卡的全局梯度，数据量会比ScatterReduce部分来的大点，因此效率也会相应降低一点。</p>
<h2 id="4-问题探讨"><a href="#4-问题探讨" class="headerlink" title="4. 问题探讨"></a><font color="#FF8C00">4. 问题探讨</font></h2><p>如下图所示，基于Torus网络的结构，组合Ring AllReduce，2D-Ring AllReduce, 2D-Mesh AllReduce，2D-Torus AllReduce还能构建 3D-Ring/Mesh/Torus AllReduce拓扑算法，但是这些拓扑算法的效率需要进行实践才能证实，也许在规模较大的集群里才能发挥出3D 拓扑算法的优势。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/4/image-2dtorus-logictopo.png" alt="2D-Torus拓扑"></p>
<p>关于 3D-Ring/Mesh/Torus AllReduce的拓扑算法，这里就不在阐述，可作为研究使用。</p>
<h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a><font color="#FF8C00">5. 小结</font></h2><p>本文讲述了分布式训练里最常用的几个网络结构以及通信拓扑算法：</p>
<ul>
<li>Ring AllReduce 的最佳组合是 ScatterReduce + AllGather；</li>
<li>2D-Ring AllReduce = 主机内 ringAllReduce/Ring Reduce +主机间 RingAllReduce + 主机内Broadcast；</li>
<li>2D-Torus AllReduce = 主机内 Ring ReduceScatter + 主机间N个Ring AllReduce + 主机内Ring AllGather；</li>
<li>2D-Mesh AllReduce = 主机内Ring AllReduce + 主机间N个Ring AllReduce;</li>
</ul>
<p>Ring AllReduce适合主机内互联Ring的情况使用，2D-Ring AllReduce适合一台服务器配置了一张网卡的异构网络场景，2D-Torus AllReduce与2D-Mesh AllReduce适合一台服务器配置了2/4/8张网卡的异构网络场景。</p>
<p>集合通信拓扑算法多种多样，但基于成本以及效率的取舍考虑，可生产适用的其实也不多，除了理论上的理解之外更重要的是自己编写代码去实践落地。除此之外，还需要解决网络带宽有限、网络容易出故障、落后者效应、部署约束、多租户等产品化的质量要求。日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="6-作者简介"><a href="#6-作者简介" class="headerlink" title="6. 作者简介"></a><font color="#FF8C00">6. 作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习高级软件主管工程师、架构师，前EMC资深首席工程师，主要工作背景在深度学习、大数据、云计算、分布式中间件以及Linux内核领域。</p>
<h2 id="7-参考资料"><a href="#7-参考资料" class="headerlink" title="7. 参考资料"></a><font color="#FF8C00">7. 参考资料</font></h2><p>[1]  <a href="https://www.changping.me">https://www.changping.me</a></p>
<p>[2] 《volta-architecture-whitepaper》</p>
<p>[3]  2D-HRA: Two-Dimensional Hierarchical Ring-based All-reduce Algorithm in Large-Scale  Distributed Machine Learning</p>
<p>[4]  Massively Distributed SGD:  ImageNet/ResNet-50 Training in a Flash</p>
<p>[5]  <a href="https://zhuanlan.zhihu.com/p/79030485" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79030485</a> , 腾讯机智团队分享–AllReduce算法的前世今生</p>
<p>[6]   <a href="https://zhuanlan.zhihu.com/p/370548366" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/370548366</a>,  ring allreduce和tree allreduce的具体区别是什么？</p>
<p>[7]   <a href="https://zhuanlan.zhihu.com/p/184942777" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/184942777</a> ,  分布式深度学习初探</p>
<p>[8]   <a href="https://arxiv.org/abs/1811.06992" target="_blank" rel="noopener">https://arxiv.org/abs/1811.06992</a> ， Image Classification at Supercomputer Scale</p>
<h2 id="8-版权申明"><a href="#8-版权申明" class="headerlink" title="8. 版权申明"></a><font color="#FF8C00">8. 版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/AI/" rel="tag"><i class="fas fa-tags"></i>AI</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2022/04/04/ai-distributed-training-coll-lang/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2022/04/04/ai-distributed-training-coll-lang/" itemprop="url">分布式训练 – 第3篇 - 集合通信及其通信原语</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2022-04-04T21:23:38+08:00">2022-04-04 21:23:38</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a><font color="#FF8C00">1. 概述</font></h2><p>集合通信（Collective Communications）是一个进程组的所有进程都参与的全局通信操作，其最为基础的操作有 发送send、接收receive、复制copy、组内进程栅障同步Barrier以及节点间进程同步(signal +wait )，这几个最基本的操作经过组合构成了一组通信模板也叫通信原语，比如：1对多的广播broadcast、多对1的收集gather、多对多的收集all-gather、1对多的发散scatter、多对1的规约reduce、多对多的规约all-reduce、组合的规约与发散reduce-scatter、多对多的all-to-all等，集合通信的难点在于通信效率以及网络硬件连接拓扑结构的最佳适用。</p>
<h2 id="2-通信原语"><a href="#2-通信原语" class="headerlink" title="2. 通信原语"></a><font color="#FF8C00">2. 通信原语</font></h2><p>以一台集成了4张训练加速卡的服务器为例，如下图，服务器内四张训练加速卡是全连接的，物理连接方式可以是私有物理互联协议，比如CXL、NVLINK，也可以是PCIe、InfiniBand、Ethernet等，本文将以此物理拓扑结构描述集合通信中常用的几组通信原语。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-connect-topo.png" alt="image-connect-topo"></p>
<h3 id="2-1-Broadcast"><a href="#2-1-Broadcast" class="headerlink" title="2.1 Broadcast"></a>2.1 Broadcast</h3><p>Broadcast属于1对多的通信原语，一个数据发送者，多个数据接收者，可以在集群内把一个节点自身的数据广播到其他节点上。如下图所示，圈圈表示集群中的训练加速卡节点，相同的颜色的小方块则代表相同的数据。当主节点 0 执行Broadcast时，数据即从主节点0被广播至其他节点。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-broadcast.png" alt="image-20220404193446314"></p>
<p>Broadcast是数据的1对多的同步，它将一张XPU卡上的数据同步到其他所有的XPU卡上，其应用场景有：</p>
<p>1）数据并行的参数初始化，确保每张卡上的初始参数是一致的；</p>
<p>2）allReduce里的 broadcast + reduce组合里的broadcast操作；</p>
<p>3）分布式训练parameter server 参数服务器结构里的 master节点 broadcast 数据到worker节点，再从worker节点reduce数据回master节点里的broadcast操作；</p>
<h3 id="2-2-Scatter"><a href="#2-2-Scatter" class="headerlink" title="2.2 Scatter"></a>2.2 Scatter</h3><p>同Broadcast一样，Scatter也是一个1对多的通信原语，也是一个数据发送者，多个数据接收者，可以在集群内把一个节点自身的数据发散到其他节点上。与Broadcast不同的是Broadcast把主节点0的数据发送给所有节点，而Scatter则是将数据的进行切片再分发给集群内所有的节点，如下图所示，不相同的颜色的小方块代表不相同的数据，主节点 0 将数据分为四份分发到了节点0-3。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-scatter.png" alt="image-20220404193446314"></p>
<p>Scatter是数据的1对多的分发，它将一张XPU卡上的数据进行分片再分发到其他所有的XPU卡上，他的反向操作对应Gather，其应用场景有：</p>
<p>1）ReduceScatter组合里的 Scatter操作；</p>
<p>2）模型并行里初始化时将模型scatter到不同的XPU上；</p>
<h3 id="2-3-Gather"><a href="#2-3-Gather" class="headerlink" title="2.3 Gather"></a>2.3 Gather</h3><p>Gather操作属于多对1的通信原语，具有多个数据发送者，一个数据接收者，可以在集群内把多个节点的数据收集到一个节点上，如下图所示，不相同的颜色的小方块代表不相同的数据。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-gather.png" alt="image-20220404193952158"></p>
<p>Gather是数据的多对1的收集，它将多张XPU卡上的数据收集到1张XPU卡上，他的反向操作对应Scatter，其应用场景有：</p>
<p>1）ReduceScatter组合里的 Scatter操作；</p>
<h3 id="2-4-AllGather"><a href="#2-4-AllGather" class="headerlink" title="2.4 AllGather"></a>2.4 AllGather</h3><p>AllGather属于多对多的通信原语，具有多个数据发送者，多个数据接收者，可以在集群内把多个节点的数据收集到一个主节点上（Gather），再把这个收集到的数据分发到其他节点上（broadcast），即收集集群内所有的数据到所有的节点上。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-allgather.png" alt="image-20220404194323067"></p>
<p>AllGather是数据的多对多的同步全收集，它将多张XPU卡上的数据收集到多张XPU卡上，可以看做Gather + Broadcast的操作组合，它的反向操作对应ReduceScatter，其最应用场景有：</p>
<p>1） AllGather可应用于模型并行；</p>
<p>2）模型并行里前向计算里的参数全同步，需要用allgather把模型并行里将切分到不同的XPU上的参数全同步到一张XPU上才能进行前向计算。</p>
<h3 id="2-5-Reduce"><a href="#2-5-Reduce" class="headerlink" title="2.5 Reduce"></a>2.5 Reduce</h3><p>Reduce属于多对1的通信原语，具有多个数据发送者，一个数据接收者，可以在集群内把多个节点的数据<strong>规约运算</strong>到一个主节点上，常用的规约操作符有：求累加和SUM、求累乘积PROD、求最大值MAX、求最小值MIN、逻辑与 LAND、按位与BAND、逻辑或LOR、按位或BOR、逻辑异或LXOR、按位异或BOXR、求最大值和最小大的位置MAXLOC、求最小值和最小值的位置MINLOC等，这些规约运算也需要加速卡支持对应的算子才能生效。</p>
<p>Reuduce操作从集群内每个节点上获取一个输入数据，通过规约运算操作后，得到精简数据，如下图的SUM求累加和：节点0数值 5、节点1数值6、节点2数值7、节点3数值8，经过SUM运算后 累积和为 26，即得到更为精简的数值，在reduce原语里回会去调用 reduce SUM算子来完成这个求和累加。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-reduce.png" alt="image-20220404194633808"></p>
<p>Reduce是数据的多对1的规约运算，它将所有张XPU卡上的数据规约（比如SUM求和）到1张XPU卡上，其应用场景有：</p>
<p>1）AllReduce里的 broadcast + reduce组合里的reduce操作；</p>
<p>2）ReduceScatter组合里的 reduce操作；</p>
<p>3）分布式训练parameter server 参数服务器结构里的 master节点 broadcast 数据到worker节点，再从worker节点reduce数据回master节点里的reduce操作；</p>
<h3 id="2-6-ReduceScatter"><a href="#2-6-ReduceScatter" class="headerlink" title="2.6 ReduceScatter"></a>2.6 ReduceScatter</h3><p>ReduceScatter属于多对多的通信原语，具有多个数据发送者，多个数据接收者，其在集群内的所有节点上都按维度执行相同的Reduce规约运算，再将结果发散到集群内所有的节点上，Reduce-scatter等价于节点个数次的reduce规约运算操作，再后面执行节点个数的scatter次操作，其反向操作是AllGather。</p>
<p>如下图所示，先reduce操作 XPU 0-3的数据reduce为 A(A0+A1+A2+A3) + B(B0 + B1 +B2 + B3) + C(C0 + C1 + C2 + C3) + D(D0 + D1 + D2 + D3 ) 到一张XPU上，再进行分片scatter到集群内所有的XPU卡上。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-reducescatter.png" alt="image-20220404200227476"></p>
<p>ReduceScatter是数据的多对多的reduce + scatter运算，它将所有的XPU卡上的数据先规约（比如SUM求和）到1张XPU卡上，再进行scatter，其应用场景有：</p>
<p>1）ReduceScatter即可应用于数据并行也可应用于模型并行；</p>
<p>2）数据并行allReduce里的 ReduceScatter+ Allgather组合里的ReduceScatter操作；</p>
<p>3）模型并行里在前向allgather后的反向计算里的ReduceScatter；</p>
<h3 id="2-7-AllReduce"><a href="#2-7-AllReduce" class="headerlink" title="2.7 AllReduce"></a>2.7 AllReduce</h3><p>AllReduce属于多对多的通信原语，具有多个数据发送者，多个数据接收者，其在集群内的所有节点上都执行相同的Reduce操作，可以将集群内所有节点的数据<strong>规约运算</strong>得到的结果发送到所有的节点上。AllReduce操作可通过在主节点上执行Reduce + Broadcast或ReduceScatter + AllGather实现，如下图所示：先在主节点上执行reduce得到规约累加和26，再把这个累加和26 broadcast到其他的节点，这样整个集群内，每个节点的数值就都保持一致。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-allreduce.png" alt="image-20220404195550358"></p>
<p>AllReduce是数据的多对多的规约运算，它将所有的XPU卡上的数据规约（比如SUM求和）到集群内每张XPU卡上，其应用场景有：</p>
<p>1） AllReduce应用于数据并行；</p>
<p>2）数据并行各种通信拓扑结构比如Ring allReduce、Tree allReduce里的 allReduce操作；</p>
<h3 id="2-8-All-To-All"><a href="#2-8-All-To-All" class="headerlink" title="2.8 All-To-All"></a>2.8 All-To-All</h3><p>All-To-All操作每一个节点的数据会scatter到集群内所有节点上，同时每一个节点也会Gather集群内所有节点的数据。ALLTOALL是对ALLGATHER的扩展，区别是ALLGATHER 操作中，不同节点向某一节点收集到的数据是相同的，而在ALLTOALL中，不同的节点向某一节点收集到的数据是不同的，如下图所示</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/3/image-allgather-alltoall.png" alt="image-20220404202332268"></p>
<p>AllToAll是数据的多对多的转置，它将所有张XPU卡上的数据转置到所有的XPU卡上，其主要应用场景有：</p>
<p>1） AllToAll应用于模型并行；</p>
<p>2）模型并行里的矩阵转置；</p>
<p>3）数据并行到模型并行的矩阵转置；</p>
<h3 id="2-9-Send-与-Receive"><a href="#2-9-Send-与-Receive" class="headerlink" title="2.9 Send 与 Receive"></a>2.9 Send 与 Receive</h3><p>数据或参数在不同XPU之间的发送与接收。</p>
<h3 id="2-10-Barrier"><a href="#2-10-Barrier" class="headerlink" title="2.10 Barrier"></a>2.10 Barrier</h3><p>BARRIER同步操作会阻塞所有的调用者直到所有的组内成员都调用了它， 用于一个集合通信子中所有进程的同步，调用函数时进程将处于等待状态，直到通信子中所有进程 都调用了该函数后才继续执行。</p>
<h3 id="2-11-Signal与Wait"><a href="#2-11-Signal与Wait" class="headerlink" title="2.11 Signal与Wait"></a>2.11 Signal与Wait</h3><p>Signal与Wait属于记录型信号量机制： wait(s)，signal(s)可用于解决进程间的同步问题，在通信原语里从一个节点发送一个数据到另外一个节点时，会同时signal一个event值到对端，对端的wait操作接收到这个event时会返回一个确认给signal，这样保证在节点的进程间进行数据的同步操作。</p>
<h2 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a><font color="#FF8C00">3. 小结</font></h2><p>在分布式训练过程中，深度学习训练框架不会去直接操作底层的通信网络，而是通过使用网络通信库来完成数据的集合通信，各家AI芯片加速卡厂家都会提供私有的网络通信库比如：xxx-AWARE OpenMPI或xCCL来完成这个底层通信硬件的屏蔽与抽象。在分布式训练集群里网络通信硬件连接样式多种多样，可以是Ethernet、InfiniBand 、RoCE v2/v1 等也可以是CXL、NVLINK等私有协议，这就要求在通信的后端层根据各个厂家的自己的SDK开发库接口，根据实际情况实现 各自的网络通信库，比如cuda-aware MPI、NCCL、NVSHMEM，以及根据实际的网络拓扑组合完成对应的最有效的网络拓扑算法。</p>
<p>本文讲述了分布式训练里的集合通信原语，这些原语是集合通信拓扑算法的基本组成单元，后续的文章里会讲述如何组合这些通信原语以完成合适的通信拓扑算法。日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="4-作者简介"><a href="#4-作者简介" class="headerlink" title="4. 作者简介"></a><font color="#FF8C00">4. 作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习高级软件主管工程师、架构师，前EMC资深首席工程师，主要工作背景在深度学习、大数据、云计算、分布式中间件以及Linux内核领域。</p>
<h2 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a><font color="#FF8C00">5. 参考资料</font></h2><p>[1] <a href="https://www.changping.me">https://www.changping.me</a></p>
<p>[2] <a href="http://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH" target="_blank" rel="noopener">http://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH</a></p>
<p>[3] 《用这拌元宵，一个字：香！| 分布式训练硬核技术——通讯原语》</p>
<p>[4] 《NCCL-Woolley》</p>
<p>[5] 《利用MegEngine分布式通信算子实现复杂的并行训练》</p>
<h2 id="6-版权申明"><a href="#6-版权申明" class="headerlink" title="6. 版权申明"></a><font color="#FF8C00">6. 版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/AI/" rel="tag"><i class="fas fa-tags"></i>AI</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2021/07/25/ai-distributed-training-metrics/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2021/07/25/ai-distributed-training-metrics/" itemprop="url">分布式训练 – 第2篇 - 训练与系统评价指标</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2021-07-25T22:23:38+08:00">2021-07-25 22:23:38</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a><font color="#FF8C00">1. 前言</font></h2><p>不同于教科书里讲的深度学习的评价指标，这里主要讲述生产训练中常用的评价指标。通常在分布式训练中对训练的过程与结果会进行评价，比如选择一个评价指标：准确率，即表明模型求解给定问题的准确度。而本文提到的评价指标主要分为两大类，即训练结果评价与训练系统评价。</p>
<h2 id="2-训练指标"><a href="#2-训练指标" class="headerlink" title="2. 训练指标"></a><font color="#FF8C00">2. 训练指标</font></h2><p>教科书里经常提到的深度学习的评价指标有准确率、精确率、召回率、F1值等，如下：</p>
<ul>
<li><p>准确率（Accuracy），所有的预测正确（正类负类）的占总的比重；</p>
</li>
<li><p>精确率（Precision），查准率，即正确预测为正的占全部预测为正的比例；</p>
</li>
<li><p>召回率（Recall），查全率，即正确预测为正的占全部实际为正的比例；</p>
</li>
<li><p>F1值（H-mean值），F1值为算数平均数除以几何平均数，且越大越好；</p>
</li>
</ul>
<p>实际上这些指标在真正的生产过程中用的不多，在实际的分布式训练过程中，比较关心的训练评价指标有：</p>
<ul>
<li>加速比（speedup），即多卡训练下的单卡吞吐量平均指标除以单卡训练下的吞吐量平均指标，比如，大规模训练下的 ResNet-50 v1.5的单卡FPS指标是600，而单卡训练的FPS指标是800，那么加速比即 600/800 = 0.75，加速比体现的是训练集群的效率与可扩展性，越高的加速比表明训练集群的资源利用率越高，但是越高的加速比要求对训练集群的技术要求也越高。比如 一个 1000张卡的训练集群，要求 加速比 0.9以上，那么对于主机间的网络、主机内的网络、全栈软件、训练卡内部的硬件架构、集合通信拓扑算法、训练算法的优化等的要求都极高，这就涉及到整个分布式训练系统的问题，而不是单个点能彻底解决的；</li>
<li>吞吐量，sequence/sec 或 FPS, 即每秒能处理的图片数或数据量；</li>
<li>收敛时间（Time）与训练次数（epoch），生产过程中对训练所有的时间是有要求的，假设给定一个模型的训练次数(epoch)为100，如果要把这个100次都训练完需要 好几天，甚至好几个星期，那么可以认为生产不适用，基本上可以定义 训练一个模型到收敛需要 24小时以上，都可以看做是生产不适用，需要扩大训练集群的规模，使之训练时间控制在24小时之内；</li>
<li>平均准确率(eval Accuracy)，平均准确率是训练是否收敛的重要评判标准之一，比如定义一个 Resnet50 v1.5 的训练模型的准确率为 76%，如果训练结束的平均准确率能达到这个值就认为训练是收敛的；</li>
<li>可收敛，训练的最终结果可以达到 平均准确率的要求，即认为可收敛，否者即任务训练失败；</li>
<li>学习率(Learning rate)与损失率(Loss)，学习率大模型训练学习速度快，但是易导致损失率爆炸, 学习率小模型训练学习速度慢，而且容易过拟合，收敛速度慢；</li>
<li>曲线拟合(Curve Fitting)，这是一个非常重要的评价手段，在XPU训练的场景下，通常先用一个已有的之前训练好模型为基础或先用GPU训练出一个基础模型，然后把XPU训练的结果指标跟GPU训练模型的指标进行比较，曲线拟合即认为XPU的训练结果达标，这也是调试XPU训练结果的一个重要手段。这里埋一个问题，按照曲线拟合的说法，假设有一个2000张XPU卡的集群，怎样评价这个集群训练的结果是正确的？以GPU训练的结果做比较，那么找一个这么大规模的GPU集群进行训练然后得到想要的模型做基础匹配也是不大现实的，那么需要采用什么技术方案才能解决这个问题？</li>
</ul>
<p>以TensorBoard为例，说明模型的评价指标，在下面的命令行列输入一个baseline:/log_path_2：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># tensorboard --logdir=training_model:/log_path_1, baseline:/log_path_2<br></code></pre></td></tr></table></figure>
<p>这个baseline 的模型已经确定是精度达标，生产可用的。然后 XPU训练的模型的 <code>training_model:/log_path_1</code> 与这个GPU训练处的baseline进行比，在tensorboard里可以表现如下图：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/2/training-results.png" alt="训练结果"></p>
<p>在上图里，新的模型的eval_accuracy值与baseline的值最终是一样的，这说明训练结果是收敛且精度达标，eval_accuracy中间的线有点差异是由于按不同的训练次数进行tensorboard指标保存所造成。新模型的Loss线与Learning_rate 线也与基础线吻合，这说明XPU训练的模型质量可生产适用。eval_accuracy、Loss、Learning_rate是三个最重要的度量指标，只要这样三个指标达标，那么大概率即可判断这个在XPU下新训练的模型具备生产可用能力。</p>
<h2 id="3-系统指标"><a href="#3-系统指标" class="headerlink" title="3. 系统指标"></a><font color="#FF8C00">3. 系统指标</font></h2><p>分布式训练系统其本身也是一个分布式系统，因此除了训练领域相关的度量指标，也有与分布式系统质量有关的一套度量指标，其中比较重要的几项内容如下：</p>
<ul>
<li><p>可用性(Availability)，可用性指的是分布式训练系统长时间可对外提供服务的能力，通常采用小数点后的9的个数作为度量指标，按照这种约定“五个九”等于0.99999（或99.999％）的可用性，默认企业级达标的可用性为6个9。但是当前从时间维度来度量可用性已经没有太大的意义，因为设计得好的系统可以在系统出现故障得情况下也能保证对外提供得服务不中断，因此，当前更合适得可用性度量指标 是请求失败率;</p>
</li>
<li><p>可靠性(Reliability)，可靠性一般指系统在一定时间内、在一定条件下可以无故障地执行指定功能的能力或可能性， 也是采用小数点后的9的个数作为度量指标，通常5个9的可靠性就可以满足企业级达标；</p>
</li>
<li><p>可伸缩性(Scalability)，是指通过向系统添加资源来处理越来越多的工作并且维持高质量服务的能力，其受可用性以及可靠性的制约，集群规模越大出故障的概率越高从而降低可用性、可靠性，为了保证可用性以及可靠性达标，需要适配合理的可伸缩性指标；</p>
</li>
<li><p>韧性(resilience)，通常也叫容错性（fault-tolerant），也就是健壮和强壮的意思，指的是系统的对故障与异常的处理能力，比如在软件故障、硬件故障、认为故障这样的场景下，系统还能保持正常工作的能力，分布式训练系统的容错能力是一个非常重要的指标。</p>
</li>
</ul>
<h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a><font color="#FF8C00">4. 小结</font></h2><p>本文从实践的角度讲述了分布式训练的训练结果评价指标与系统评价指标，这些指标是度量一个分布式训练系统与训练的模型是否生产可用的重要参考。日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="5-作者简介"><a href="#5-作者简介" class="headerlink" title="5. 作者简介"></a><font color="#FF8C00">5. 作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习高级软件主管工程师、架构师，前EMC资深首席工程师，主要工作背景在深度学习、大数据、云计算、分布式中间件以及Linux内核领域。</p>
<h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a><font color="#FF8C00">6. 参考资料</font></h2><p>[1] <a href="https://www.changping.me">https://www.changping.me</a></p>
<h2 id="7-版权申明"><a href="#7-版权申明" class="headerlink" title="7. 版权申明"></a><font color="#FF8C00">7. 版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/AI/" rel="tag"><i class="fas fa-tags"></i>AI</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2021/04/18/ai-distributed-training-whatistraining/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2021/04/18/ai-distributed-training-whatistraining/" itemprop="url">分布式训练 – 第1篇 - 什么是分布式训练</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2021-04-18T22:23:38+08:00">2021-04-18 22:23:38</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a><font color="#FF8C00">1. 前言</font></h2><p>深度学习软件工程具有一体两面性：单卡的功能完备性、质量、用户体验以及多卡大规模。多卡大规模的出现是为了解决这样一个主要矛盾，即：“日益增长的数据、模型训练的需求与当前单卡计算能力无法满足这个需求之间的矛盾”，而分布式训练可以通过扩展卡子的规模解决这个矛盾，因此，这就是分布式训练的价值。</p>
<p>然而，正如懂得很多道理，仍旧过不好这一生一样，懂得很多分布式训练的理论与知识，也不一定就能做好一个分布式训练系统。把这么多机器连接跑起来、跟跑好也是两回事，分布式训练是一门实践的软件工程，只有你PK过设计方案，调试过一个个Bug，手把手的敲过一行行的代码，为了性能指标能达标无所不用其极的去验证各种性能优化方案，才能知道细节在哪里，难点在哪里，痛点、挑战点在哪里。因此，宏观处着眼，微观处着手，才能完全理解分布式训练的道理。</p>
<p>一个知识领域里的 “道 法 术 器” 这四个境界需要从 微观、中观以及宏观 三个角度来把握，微观是实践，中观讲方法论，宏观靠领悟。本系列文章我把它命名为《分布式训练与推理实战》，从工程实战的角度拆解分布式训练里最重要的十八个套路，也是从“微观实践、中观方法论、宏观领悟”这三个维度系统性的讲述分布式训练技术，本文讲述第1式，也是最难讲清楚的一式（也后续再迭代更新），即本质的一问：<strong>“什么是分布式训练“</strong>。</p>
<h2 id="2-什么是分布式训练"><a href="#2-什么是分布式训练" class="headerlink" title="2. 什么是分布式训练"></a><font color="#FF8C00">2. 什么是分布式训练</font></h2><p>简单来说，<strong>分布式训练 = 分布式训练系统 = 分布式系统 + 训练系统</strong>，因此，要解答什么是分布式训练就需要解答什么是分布式系统以及什么是训练系统，而<strong>“系统 = 要素x连接 + 目的 + 边界”</strong>，因此进一步的就是需要分析分布式系统的要素、连接、目的与边界以及训练系统的要素、连接、目的与边界。</p>
<h3 id="2-1-分布式系统"><a href="#2-1-分布式系统" class="headerlink" title="2.1 分布式系统"></a>2.1 分布式系统</h3><p>在AI训练过程中采用单卡总会遇到一些问题，比如原始的数据样本太大无法加载进训练卡，或者模型太大无法训练，那么这就需要用到分布式技术把大量的数据分割成小块由多个训练卡分别进行计算，在更新运算结果后，再将结果统一合并得出最终的可用模型。百科上对分布式系统的定义有：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. The components interact with one another in order to achieve a common goal.<br></code></pre></td></tr></table></figure>
<p>即：</p>
<blockquote>
<p>分布式系统是指其组件位于不同的网络计算机上的系统，这些组件通过相互传递消息来进行通信和协调其动作，且彼此相互交互以完成一个共同的任务目标。</p>
</blockquote>
<p>从这句话可以得出三个结论：</p>
<ul>
<li>分布式系统的组件是位于不同的网络计算机上的；</li>
<li>分布式系统的组件通过传递消息进行通信与协调的；</li>
<li>分布式系统的组件是通过相互交互以完成一个共同的任务目标，同时是有边界的；</li>
</ul>
<p>因此基于此定义，拆解分布式系统的概念，从中可以看到分布式系统里的要素即为组件，连接即网络，目的是共同的任务目标。其中的位于不同的网络计算机上的“组件”是分布式系统的要素，即各种计算单元，比如Ai训练卡，“网络”是分布式系统的连接，即神经网与数据网，“共同的任务目标”是分布式系统的目的，即训练，至此，再进一步抽象，可以推导出分布式系统的公理化定义，也是分布式系统的本质理论定义：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">分布式系统 = 计算 x 网络 + 目的 + 边界<br></code></pre></td></tr></table></figure>
<p>在这个公式里，计算即计算单元，是各种AI训练卡，比如GPU, TPU, DPU, DTU。网络即网络连接单元，在单个训练卡内为计算用的神经网，主机内的多个卡子之间是PCIE 以及PCIE Switch，以及各种高带宽通信网，比如GenZ,CXL,NVLINK,OpenCAPI,CCIX等，在主机之间是各种通信网络，比如RDMA网络、InfiniBand网络、普通的TCP网络以及对应的各种交换机，另外从磁盘 + 主机内存 + 训练卡的HBM这个IO路径我们认为属于IO网络，而这里的目的 即<strong>训练</strong>，同时这个系统是有边界的，其专注于解决Ai训练过程中的难题，不是什么功能都能往里塞都能解决。</p>
<h3 id="2-2-训练系统"><a href="#2-2-训练系统" class="headerlink" title="2.2 训练系统"></a>2.2 训练系统</h3><p>以数据并行随机梯度下降( SGD )技术为例，神经网络训练的过程如下:</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/aitraining/1/ai-training.PNG" alt="AI训练"></p>
<p>1，首先需要通过在第一个step进行Broadcast操作将参数同步到集群内的所有的训练卡上;</p>
<p>2，将数据样本切片分发到整个集群的每张训练卡上并且通过data pipeline技术将数据样本加载进训练卡的高速内存空间内，作为输入X;</p>
<p>3，每个训练卡在其数据样本上运行前向传播，计算出误差LOSSi；</p>
<p>4，对计算出的LOSSi进行反向传播，得到梯度GRADi；</p>
<p>5，所有的训练卡在主机内及主机之间进行集合通信并进行梯度归约(AllReduce)；</p>
<p>6，最后再进行参数更新以获得新的梯度参数。</p>
<p>本质上分布式训练是<strong><em>数据加载、前向传播、反向传播、集合通信以及参数更新\</em></strong>这5个步骤的逻辑组合，因此，基于以上步骤，这里可以推导出<strong>训练系统</strong>的公式定义如下：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">训练系统 = 数据加载 + （前向传播 + 反向传播） + 集合通信 + 参数更新<br></code></pre></td></tr></table></figure>
<p>从上面的步骤可知分布式训练是在固定的步骤迭代中进行的，并且需要系统内的所有的训练卡都完成它们的迭代步骤，才能进行最后的参数更新，这相当于在单个训练卡上执行梯度下降技术，但是通过在系统内所有的训练卡之间分发数据样本并同时执行计算来获得训练的加速。</p>
<h3 id="2-3-举例说明"><a href="#2-3-举例说明" class="headerlink" title="2.3 举例说明"></a>2.3 举例说明</h3><p>以TensorFlow为例说明模型的训练过程，TensorFlow 是用数据流图做计算的，如下图所示:</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/distributed/tensorflow/tf_arch_dataflow.gif" alt="计算流图示例" style="zoom:150%;"></p>
<center>图片来源于网络版权归原作者所有</center>

<p>图中显示了 TensorFlow 的训练过程，其包含输入（input）、塑形（reshape）、Relu 层（Relu layer）、Logit 层（Logit layer）、Softmax、交叉熵（cross entropy）、梯度（gradient）、SGD 训练（SGD Trainer）等部分。</p>
<p>它的训练过程是，首先从数据分片输入开始，经过Reshape数据清洗后，进行前向传播运算，通过Relu 层后得到LOSS值，然后进入 Logit  层，再进行反向传播并且用 Cross Entropy、softmax等  来计算梯度，接着进行梯度归约(Allreduce)，这一步在分布式场景就涉及集合通信的过程，最后进行参数更新SGD  Trainer，如此迭代循环直到获得收敛指标达标的结果为止。</p>
<h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a><font color="#FF8C00">4. 小结</font></h2><p>采用分布式训练的目的往往也是因为数据量或模型太大，一个训练卡放不下，因此对数据或者模型进行切分，分发到多卡上进行计算与归约。本文很概况性的讲述了什么是分布式训练，简单来说分布式训练就是分布式计算的一种，通过对数据样本的计算，得出最后可用的模型再用于数据推理。本系列文章的后续内将展开讲述分布式训练系统的基础理论、训练过程、质量保证、集合通信、系统工程、产品化等，同样分布式训练系统除了解决训练所带来的各种故障也还需要解决分布式所带来的各种故障。</p>
<p>日拱一卒，功不唐捐，分享是最好的学习，与其跟随不如创新，希望这个知识点对大家有用。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="5-作者简介"><a href="#5-作者简介" class="headerlink" title="5. 作者简介"></a><font color="#FF8C00">5. 作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习高级软件主管工程师、架构师，前EMC资深首席工程师，主要工作背景在深度学习、大数据、云计算、分布式中间件以及Linux内核领域。</p>
<h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a><font color="#FF8C00">6. 参考资料</font></h2><p>[1] <a href="https://blog.tensorflow.org/2019/09/tensorflow-20-is-now-available.html" target="_blank" rel="noopener">https://blog.tensorflow.org/2019/09/tensorflow-20-is-now-available.html</a></p>
<h2 id="7-版权申明"><a href="#7-版权申明" class="headerlink" title="7. 版权申明"></a><font color="#FF8C00">7. 版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/AI/" rel="tag"><i class="fas fa-tags"></i>AI</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2021/02/17/distributed-product_Lean_development/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2021/02/17/distributed-product_Lean_development/" itemprop="url">分布式系统架构设计 – 第28式 - 五大原则，三大基石 - 10人以内小团队极简精益产品开发法</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2021-02-17T21:30:00+08:00">2021-02-17 21:30:00</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/product/" itemprop="url" rel="index"><span itemprop="name">product</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="产品开发的本质"><a href="#产品开发的本质" class="headerlink" title="产品开发的本质"></a><font color="#FF8C00">产品开发的本质</font></h2><p>根据<strong>第一性原理</strong>思维，可以得出产品开发的本质即：<strong>“高效、高质量地交付有用的价值”</strong>[1]。那么从这句话可以推导出四个需要解决的命题，即：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">1，需要交付的价值是什么？<br>2，如何判断什么是有用的价值？<br>3，如何高效地交付？<br>4，如何高质量地交付？<br></code></pre></td></tr></table></figure>
<p>此外，在软件产品开发过程中除了面临确定性复杂度之外还经常面临不确定性复杂度，不确定性的复杂度比确定性的复杂度更难解决，更容易引起焦虑，更容易带来团队<strong>熵增</strong>，也更容易造成软件开发交付的失败。因此，为了<strong>简单有效</strong>地解决这四个命题以及其所自带的复杂度问题，结合精益产品理论，本文提出一种从实战中总结出来的<strong>“极简精益产品开发法”</strong>。需要额外说明的是，本方法论还有两个约束条件：</p>
<ul>
<li><p>本方法论仅适用于以搞生产力为主而非以搞生产关系为主的企业、部门、团队或个人；</p>
</li>
<li><p>本方法论仅适用于<strong>“The two pizza team”</strong>，即小于10人的小团队或单兵作战能力很强的个体，比如10倍工程师；</p>
</li>
</ul>
<h2 id="极简精益产品开发法"><a href="#极简精益产品开发法" class="headerlink" title="极简精益产品开发法"></a><font color="#FF8C00">极简精益产品开发法</font></h2><p>为了简单有效地解决<strong>“需要交付的价值是什么？如何判断什么是有用的价值？如何高效地交付？如何高质量地交付？”</strong> 这四大命题以及软件开发所面临的确定与不确定复杂度，这里采用结构化的方法提出的极简精益产品开发思维模型，其涵盖五大原则与三大基石，即：</p>
<ul>
<li>五大原则：以终为始，架构先行，有拆有合，迭代更新，相关满意</li>
<li>三大基石：领域能力，企业文化，组织到位</li>
</ul>
<p>本极简精益产品开发法思维模型导图如下：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/lean/lean-1.png" alt=""></p>
<p>在极简精益产品开发法思维模型里，“领域能力、企业文化、组织到位”属于基石的范畴，三大基石不到位，则产品开发与交付的原则与行动无效。在五大原则里，“以终为始”是为了解决“判断需要交付的价值是什么？如何判断什么是有用的价值？”这两个问题，“架构先行，有拆有合，迭代更新，相关满意”是为了解决如何高效地交付，如何高质量地交付以及如何解决软件开发的确定性与不确定性复杂度。</p>
<h2 id="五大原则"><a href="#五大原则" class="headerlink" title="五大原则"></a><font color="#FF8C00">五大原则</font></h2><h3 id="以终为始"><a href="#以终为始" class="headerlink" title="以终为始"></a><font color="#008C">以终为始</font></h3><p>“以终为始”是一种逆向思维，在软件开发里的应用指的是从最终的交付价值出发，反向推理交付过程，寻找软件开发的关键要素，获取反馈采取正确的策略，从而达成有用的价值交付。如下图所示，从未来的终局看现在，使用获取的未来信息强化现在的行为，赋予现在的行为以塑造未来的力量。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/lean/lean-4.png" alt=""></p>
<p>“以终为始”是确保正确地做事以及做的是正确的事的一种思维方式，在进行软件开发之前就进行了从交付结果开始的倒推分析，从交付结果得出开发的方向与方案。这里以“逆向工作法”以及“客户导向法” 为例说明软件开发过程中 “以终为始”思维的使用。</p>
<h4 id="逆向工作法"><a href="#逆向工作法" class="headerlink" title="逆向工作法"></a>逆向工作法</h4><p>逆向工作法是亚马逊所推崇的商业新哲学，因为产品最终的交付要“以客户为中心”，那就先从客户侧开始倒推开发，逆向工作法有助于一开始就把重心放在客户所真正关心的问题上，而不是一堆程序员坐在办公室内拍脑袋替客户做假想，进而避免一些无效的开发决策，使得确保推出的是有用、有价值、用户体验佳的产品。</p>
<p>拆解这个概念为具体的行动，亚马逊讲述了这个思维方式的应用步骤[4]：</p>
<p>1，先写一篇内部用的产品新闻稿，简单描述一下这个产品的特点与好处是什么，是为了解决什么痛点而产生，然后描述具体的问题，拿出一个新的解决方案，虚构一个用户的心声，设身处地为用户着想，然后发出来给内部同事，经过审核的也可以外发给公众；</p>
<p>2，写一个常见的FAQ问题文档，定义产品的用途以及考虑用户使用时会遇到的问题以及回答如何解决，FAQ需要包含外部客户会问的问题以及内部用户会问的问题；</p>
<p>3，定义客户体验，详细描述客户使用产品时会遇到的使用场景。比如用户界面是怎么样的，软件部署是怎么部署的，软件的技术架构图是怎么样的，先给客户一个能够呈现端到端体验的假象视图；</p>
<p>4，编写用户手册，用户手册是客户用来真正了解产品是什么以及如何使用它的，用户手册通常有三个部分：概念、操作方法和参考，它们之间告诉用户使用产品所需的所有知识。,</p>
<p>5，获取反馈，以上内容完成后，获取用户/客户反馈，然后在产品的生命周期中，不断迭代而演化产品的开发交付文档。</p>
<p>逆向工作法的目的是为了明白真正的用户/客户是谁，用户/客户的真实需求是什么，用户/客户的需要最先解决什么痛点等，而且以上步骤也只是一种应用方案，目的一致则无需拘泥于形式，只要思路和效果可以达到有效理解客户的目的即可。</p>
<h4 id="客户导向法"><a href="#客户导向法" class="headerlink" title="客户导向法"></a>客户导向法</h4><p>客户导向法讲的是“先有客户再有产品与技术”， 详细可以参考《第25式- 让技术回归常识,先有客户再有技术》，其理念上与逆向工作法类似，不同之处在于 逆向工作法是具体的行动，而客户导向法是认知上的提升。在这里，需要先定义什么是客户以及什么是客户价值：</p>
<ul>
<li>什么是客户？狭义的客户 = 买单的，广义上的客户 = 客户的客户 + 客户 + 客户的用户 + 所有相关方；</li>
<li>什么是客户价值？客户价值就是对客户有用的东西，价值来源于价值的交换。技术的目的就是做对客户有用的东西，并且技术的进化方向是由市场所决定的。以客户为中心，就是给客户创造价值，替解决用户难点、痛点、挑战点、为客户提供高质量低成本的产品，同时响应要及时；</li>
</ul>
<p>在产品开发上，软件开发人员常见的认知错误有：</p>
<ul>
<li>“我为用户想”，这是研发人员最容易犯的错，其已经有用户意识，但是却没有进一步与用户沟通，直接替用户做决定，也不清楚用户的使用场景，因此容易造成”所想“实际上并不是用户真正所想；</li>
<li>追求有挑战的技术而非技术的实用价值，也非从合适的解决用户问题的角度出发，将技术上的自嗨当成客户需求，比如用户需要从A地到B地，简单一点给用户一辆自行车就可以解决的问题，而技术自嗨就容易非要先自行造个飞机，然后拼命的给用户推销这个好这个快，但是用户却不买单；</li>
<li>闭门造车，不实事求是，不与客户做探讨，不做调查就把想象的或还处于概念上的东西当成客户需求，带来的是低效、低质量的产品开发过程；</li>
</ul>
<p>因此，开发产品需要先关注客户价值，在实现一个产品之前先确定这个是对客户有价值的，与客户/用户多沟通、一起共创，在“客户要的与我能提供的”二者之间保持理解一致，避免无效开发与交付。</p>
<h3 id="架构先行"><a href="#架构先行" class="headerlink" title="架构先行"></a><font color="#008C">架构先行</font></h3><h4 id="架构投影"><a href="#架构投影" class="headerlink" title="架构投影"></a>架构投影</h4><p>柏拉图在《理想国》中构建了他的哲学王国—理念世界，其把世界分成两个：“现象世界与理念世界，柏拉图认为这两个世界的关系是原本和慕本的关系，理念世界是原本、模型，现象世界是理念世界的影子或慕本”。基于此设想，产品架构可以是产品在现象世界的原本，通过产品架构可以看到产品的最终形态。在开发（编码）产品之前可以先定义软件产品的架构，给出产品的画像，提供产品的总体概要架构设计文档（注意这里概要设计文档即可，无需详细设计文档），设计文档内定义产品的设计哲学、设计原则、技术架构图、设计提案、所需要实现的功能特性、交付目标以及风险管理、用户操作等，然后发给团队一起评审，利用团队的力量避免交付的技术路线风险，同时为下一阶段的工作分解做准备。</p>
<h4 id="架构思维"><a href="#架构思维" class="headerlink" title="架构思维"></a>架构思维</h4><p>狭义上的架构通常指的是架构的技能，其属于“术”的范畴，而广义的架构则是客户需求、市场趋势、架构理念、架构方法论、架构技能、架构用的工具以及架构的边界这几个方面的组合体，应用抽象思维，即<strong>“势、道、法、术、器、界”</strong>这六个字 ，简称架构思维六元组，具体可以参考《第15式 - 架构思维》。</p>
<p><strong>势：时势</strong></p>
<p>“势”是架构的方向。从宏观处着眼，“势”是产品架构设计的市场趋势、是客户需求趋势也是技术的应用趋势；从微观处着手，“势”是功能设计的价值与目的。架构设计需要从宏观处着眼微观处着手，看清客户的需求趋势、市场趋势以及技术趋势，功能设计需要分析清楚当前功能的价值与目的。</p>
<p><strong>道：本质</strong></p>
<p>“道”是架构的认知，是架构师的设计理念、设计意图，是产品架构的灵魂，这里我把它定义为产品架构的设计哲学。</p>
<p><strong>法：方法论</strong></p>
<p>”法“是方法论，是架构设计的方法论，是架构设计的套路，这里我把它定义为产品架构的设计原则</p>
<p><strong>术：技能</strong></p>
<p>术，技能，是架构技能，这里定义为设计提案，以及各种功能与特性实现的思路</p>
<p><strong>器：工具</strong></p>
<p>”器“是工具，是架构设计用的工具，”工欲善其事必先利其器“，</p>
<p><strong>界：边界</strong></p>
<p>”界“是边界，是架构的约束限制，是技术边界、也是技术约束与技术限制，也是架构的取舍因素之一，是架构能做什麽不能做什麽的解读，对市场来说它是技术壁垒，对产品来说它是法律法规、是功能约束，对团队来说它是资源约束、是自我能力约束。</p>
<h3 id="有拆有合"><a href="#有拆有合" class="headerlink" title="有拆有合"></a><font color="#008C">有拆有合</font></h3><p>有拆有合指的是工作分解结构法，详细参考《第27式- 系统思考，分而治之 - 工作分解结构法》。基于“架构先行”这一步输出的概要设计进行工作分解，工作分解的结果可以直接影响了产品开发的效率与质量。</p>
<p>工作分解是个有拆有合的过程，一个大的工程任务往往是由很多的小个的任务组成的，如何将一个大任务拆解成合适的小任务，能将大任务拆解成什么样的小任务，能拆解到什么粒度，拆解是否准确，这是”拆“的过程。怎样对任务进行量化与质化，怎么将任务落地到具体的执行人员上，怎么进集成怎么验收，这是“合”。以下图的工作包分解表为例：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-7.PNG" alt="项目"></p>
<p>1，首先前三行定义了项目的目标、原则以及投入，说明了项目的交付目标、交付进度、交付的原则以及约束、还有确定了人力资源的投入；</p>
<p>2，任务分解成了一级任务、二级任务以及工作包、工作包名称，关键特性可以是具体需要实现的功能，也包括项目文档、测试部署、相关采购等；</p>
<p>3，除这些在任务树上也可以体现的内容外，还增加了 工作进展、状态、评论、风险、阶段目标、执行的人员、复杂度以及依赖条件；</p>
<p>4，输出最终交付成果。</p>
<p>工作包分解表可以使得团队协作更加顺畅，将不确定的大任务包分解成一个个确定的最小可执行工作包，是的开发工作从不确定到确定，有利于保证软件产品开发的效率与质量。</p>
<h3 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a><font color="#008C">迭代更新</font></h3><p>产品根据环境改变而迭代，根据反馈结果而更新。迭代更新其目的为了逐步逼近所需的最终目标，而每一次迭代更新得到的结果都会作为下一次迭代更新的初始输入。在不确定性较大的软件产品开发过程中，将产品开发分为迭代0、 迭代1、迭代2…..最终迭代目标这几个阶段，每个阶段的输出都是一个最小可行产品，即 MVP（Minimum Viable Product）。</p>
<p>如下图“10人以内小团队你极简精益产品开发法”所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/lean/lean-2.png" alt=""></p>
<p>1，迭代0，先完成信息输入，依据“以终为始、架构先行、有拆有合”的步骤完成客户目标确认、价值确定、需求分析、初版概要设计、初版工作拆解；</p>
<p>2，迭代1，进行软件开发，输出MVP1，依据“以终为始、架构先行、有拆有合”的步骤刷新客户目标、交付价值、客户需求、概要设计、进行二次工作拆解；</p>
<p>3，迭代2，进行软件开发，输出MVP2，依据“以终为始、架构先行、有拆有合”的步骤刷新客户目标、交付价值、客户需求、概要设计、进行三次工作拆解；</p>
<p>以此类推，逐步迭代更新，过程根据变化微调控保证方向正确，直至抵达最终交付目标。</p>
<h3 id="相关满意"><a href="#相关满意" class="headerlink" title="相关满意"></a><font color="#008C">相关满意</font></h3><p>相关满意指的是项目相关方满意，项目相关方是会影响项目或受项目所影响的组织、团队或人员。项目相关方的参与是项目成功的前提与保证，没有项目相关方，也就没有项目，忽略任何项目相关方都可能导致项目的失败。项目的开启、过程以及结果都需要能令项目相关方满意。</p>
<p>达成项目相关方满意需要从以下4个方面进行迭代推进以支持项目团队的工作：</p>
<p>1，识别相关方，相关方一般包括需要知晓情况的用户、客户、供应商、合作第三方，需要通知到的项目批准人、项目负责人，负责具体执行的项目开发团队：项目经理、产品经理、架构师、领域专家、各级工程师、测试、采购，以及其他的组织内外的项目支持职能部门；</p>
<p>2，管理预期，准确识别相关方的需求和期望；</p>
<p>3，提出方案，依据”以终为始、架构先行、有拆有合、迭代更新“法输出满足相关方需求和期望的提案；</p>
<p>4，迭代更新确保事态向最终目标的方向逐步推进；</p>
<p>项目相关方满意的核心就是在所有的相关方的预期中取得一个彼此都接受的解。</p>
<h2 id="三大基石"><a href="#三大基石" class="headerlink" title="三大基石"></a><font color="#FF8C00">三大基石</font></h2><h3 id="领域能力"><a href="#领域能力" class="headerlink" title="领域能力"></a><font color="#008C">领域能力</font></h3><p>领域能力狭义上指的是个人或团队所具备的相应专业能力，广义上指完成整个软件开发交付所需要的技能，包括产品思维能力、项目管理能力、架构设计能力、编码能力、测试能力、维护能力以及质量保证能力。领域能力是软件开发原则与行动的最重要的基石之一。没有对应的交付能力就不要谈什么产品开发原则与交付，比如一个拧螺丝的团队你非要他们在限定的时间内打造一根火箭，必然无法达成。</p>
<h3 id="企业文化"><a href="#企业文化" class="headerlink" title="企业文化"></a><font color="#008C">企业文化</font></h3><p>企业文化是极简精益产品开发法的基石之一，其涵盖了使命，目标，制度，边界，奖罚等，属于软件开发团队运作、开发原则与产品交付的最底层的基础设施。企业的使命讲的是企业与世界的关系，这跟企业员工离的较远，姑且不谈。而目标讲的是员工与企业的关系，每年制定的企业发展目标属于企业的战略范畴，体现了企业的战略方向以及资源投入的方向，团队目标与企业目标保持一致，个人目标与团队目标保持一致，这是最基本的团队运作准则，如果个人或团队年度目标不与企业目标对齐，那么个人或团队也拿不到资源得不到发展。而企业制度、边界与奖罚是团队运作的保障，奖什么罚什么更是最明显的企业文化导向，这里就不作举例说明了。</p>
<h3 id="组织到位"><a href="#组织到位" class="headerlink" title="组织到位"></a><font color="#008C">组织到位</font></h3><p>组织到位的第一个意思是：”组织结构影响产品结构“，组织基因即产品基因，依据康威第一定律：</p>
<figure class="hljs highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. - Melvin Conway(1967)<br></code></pre></td></tr></table></figure>
<p>翻译成中文即：”组织设计的产品/设计等价于这个组织的沟通结构“，通俗的来讲：产品结构必然是其组织内成员沟通结构的缩影，比如微服务。</p>
<p>组织到位的第二个意思是：人员到位，人事匹配。进行软件产品开发一方面需要配备相应的能力需求的人员，另一方面也需要讲究合适的人放在合适的岗位上，合适的岗位需要合适的人选，人选对了，事就成一半。</p>
<h2 id="自组织、自平衡与强管控"><a href="#自组织、自平衡与强管控" class="headerlink" title="自组织、自平衡与强管控"></a><font color="#FF8C00">自组织、自平衡与强管控</font></h2><p>软件产品的开发过程除了是一个业务管理过程也是一个团队/个人管理过程，“极简精益产品开发法”更多讲的是业务管理的过程，然而团队/个人管理的成功与否也可以决定产品开发的成败。软件开发过程是一个复杂的系统过程，而业务与团队也是一个复杂系统，需要以系统化的思维而非线性思维来看待。团队能否有效的自我组织、自我驱动直接影响软件开发的效率与质量。</p>
<p>基于此，这里提出团队/个人的三个组织形态：<strong>自组织、自平衡与强管控</strong>，如下面的组织形态变化图所示，组织的形态变化有自组织、自平衡、强管控这三个形态，自组织态会过度到自平衡态，如果没有外力的干预作用，自组织态与自平衡态都容易滚落到强管控态，强管控态势能最低也是最稳定的最终形态。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/lean/lean-0.png" alt=""></p>
<p>在一定的企业文化以及组织架构下，每一个管理决策和管理措施的输出背后，都有一种人性假设，道斯·麦格里格（Douglas McGregor）的X-Y理论（Theory X-Theory Y）概括了对人性的根本性理解：X理论-人性本恶，Y理论-人性本善。中国古代也有儒家人性论之争，最具有代表的就是孟子的“人性本善论”和荀子的“人性本恶论”。</p>
<p>人性本恶论认为员工需要被强力控制与安排，员工都讨厌工作、工作的驱动力只是为了保住饭碗、对于工作能躲就躲，因此有了强KPI管理法，271、361、末位淘汰、设定严格的规则制度等，如上图所示，其“具有低势能稳定性的形态，但不会带来创新和竞争力”[1]；</p>
<p>人性本善论认为员工是能自我驱动的，愿意自我承担责任、积极向上、会自我以目标为驱动努力工作，因此有了OKR管理法、工作-生活平衡、对员工授权、人性激发、信任管理法等，如上图所示，其具有“高势能非稳定的状态，但确是激发创造力、提升效能的利器”[1]；</p>
<p>软件开发是一个创造性的脑力劳动，其并不适合以人性本恶论为出发点强管控管理法，但是高效率、高创造性的自组织形态却具有不稳定性的特征，因此需要微管控使之一直处于自平衡态，既保持自组织的高效能、高创造力的特性，又规避了强管控管理法带来的无创新、无竞争力、低效率的弊端。以上三种形态在一个大的组织结构内可能同时存在。</p>
<p>本文将方法论局限在“10人以内小团队” 也是为了更容易达到团队的自组织或自平衡态，使之保持团队的勇于担当、自管理、高效率、高质量输出、高创造力形态。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font color="#FF8C00">小结</font></h2><p>本文提出了一种 “10人以内小团队极简精益产品开发法”，用以解决软件开发的复杂度问题以及需要解决的四个命题，即：需要交付的价值是什么？如何判断什么是有用的价值？如何高效地交付？如何高质量地交付？详解了“以终为始，架构先行，有拆有合，迭代更新，相关满意”这五大原则以及“领域能力，企业文化，组织到位”这三大基石，此外还论述了“自组织、自平衡、强管控”这三种团队组织形态。其目的都是为了使得团队/个人目标与组织目标保持一致并且“高效、高质量地交付有用的价值”。此外，作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a><font color="#FF8C00">作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习首席软件工程师，前EMC 大数据资深首席工程师，主要工作背景在深度学习、流式大数据、云计算、分布式中间件以及Linux内核。</p>
<h2 id="版权申明"><a href="#版权申明" class="headerlink" title="版权申明"></a><font color="#FF8C00">版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color="#FF8C00">参考资料</font></h2><p>[1] 《精益产品开发：原则、方法与实施》 何勉著</p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/56556328" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56556328</a></p>
<p>[3] <a href="https://www.cnblogs.com/yanglang/p/10270592.html" target="_blank" rel="noopener">https://www.cnblogs.com/yanglang/p/10270592.html</a></p>
<p>[4] <a href="https://www.allthingsdistributed.com/2006/11/working_backwards.html" target="_blank" rel="noopener">https://www.allthingsdistributed.com/2006/11/working_backwards.html</a></p>
<p>[5] <a href="https://www.sohu.com/a/299920333_263553" target="_blank" rel="noopener">https://www.sohu.com/a/299920333_263553</a></p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/product/" rel="tag"><i class="fas fa-tags"></i>product</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2021/02/13/distributed-product_project_wbs/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2021/02/13/distributed-product_project_wbs/" itemprop="url">分布式系统架构设计 – 第27式- 系统思考，分而治之 - 工作分解结构法</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2021-02-13T21:30:00+08:00">2021-02-13 21:30:00</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/product/" itemprop="url" rel="index"><span itemprop="name">product</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><font color="#FF8C00">前言</font></h2><p>在一个软件工程项目组里有四种角色特别重要，即：架构师、领域专家、产品经理以及项目经理，在一个比较大的项目里或者大公司大部门里，这四类角色一般分别对应四个人，然而在中小型项目里，特别是创业型公司或大公司里的小部门，这四类角色可能是四者合一的，即从团队当中选定一位有能力担当这四个角色的成员。如果这个成员原来就是架构师，那么对这位架构师的要求就是除了专业技能之外还应该具有项目管理能力。</p>
<p>根据定义项目是“为提供某项独特产品或服务所做的临时性努力”，其本身的特点是<strong>“变化”</strong>，因此，项目管理对架构师本身来说也是具有很大的挑战性的一项管理工作。项目管理具有入门容易、精通难的特性，大多数项目管理给工程师的感觉就是定计划、看进度、催活、做汇报，实际上这是没有领悟项目管理的精髓。</p>
<p>PMBOK定义项目管理为：“项目的管理者，在有限的资源约束下，运用系统的观点、方法和理论，对项目涉及的全部工作进行有效地管理。即从项目开始到项目结束的全过程进行启动、计划、执行、监控和验收，以实现项目的目标。”，即项目管理有其自身的<strong>“观点、方法与理论”</strong>，而在项目的落地过程中，所有节点的计划、执行、监控、风险管理以及验收等都依赖于工作分解结构（WBS: Work Breakdown Structure ），不同的行业对项目管理有不同的需求，进而有不同的工作分解结构方法，本文讲述的是与软件工程紧密相关的工作分解结构法。</p>
<h2 id="项目管理方法论"><a href="#项目管理方法论" class="headerlink" title="项目管理方法论"></a><font color="#FF8C00">项目管理方法论</font></h2><p>依据PMBOK理论定义以及软件工程实践，软件工程项目管理可以分为两大部分：一是5大流程，二是12大知识领域 ，如下：</p>
<h3 id="项目管理五过程"><a href="#项目管理五过程" class="headerlink" title="项目管理五过程"></a>项目管理五过程</h3><p>项目管理五大过程包括项目的启动、计划、执行、监视和验收，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-2.PNG" alt="项目5过程"></p>
<ul>
<li><p>启动阶段，首先需要确定项目的目标和项目的关键负责人、进行业务分析、需求分析以及组建团队，并且宣布项目正式立项；</p>
</li>
<li><p>计划阶段，编写项目计划，把项目目标量化与质化，制定达到目标的里程碑，在这个阶段，还需要完成的是概要设计、目标分解、任务分派、风险评估等工作内容；</p>
</li>
<li>执行阶段，按计划开展项目，进行详细设计、编写代码、调试自测，阶段性的实现目标，这一步是软件工程最核心的一个步骤；</li>
<li>监控阶段，监控阶段与执行阶段是循环的关系，这一阶段需要准确识别偏差，判断影响项目进度因素，同时进行计划刷新与风险刷新以及任务刷新；</li>
<li>验收阶段，按照项目要求，进行项目验收，包括版本发布、文档归档以及项目复盘。</li>
</ul>
<p>不同于理论，在实践中，这5大过程是个计划、执行、检查、更新的循环过程，而不是一个线性过程。</p>
<h3 id="项目管理12项"><a href="#项目管理12项" class="headerlink" title="项目管理12项"></a>项目管理12项</h3><p>项目管理1.0版里涉及 范围、时间、成本以及质量，这四者是个平衡的过程。项目管理2.0里，又增加了 业务与组织，项目管理是为业务服务的，同时给组织保证结果。而在实践过程中容易发现软件工程项目管理其实涉及12大领域，即：业务、范围、进度、成本、质量、资源、风险、采购、相关方、沟通、测试以及综合管理，具体如下：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-0.PNG" alt="项目12项"></p>
<ul>
<li>业务管理，其涉及业务分析、客户管理；</li>
<li>范围管理，包括规划范围、需求分析、定义范围、工作分解、确认范围以及控制范围；</li>
<li>进度管理，其包括：规划进度里程碑，定义任务，排列任务优先级，估算人力资源，估算所需时间，制定进度计划表以及控制进度；</li>
<li>成本管理，包括规划成本管理，估算成本，制定预算以及控制成本；</li>
<li>质量管理，包括规划质量指标，QA测试，质量保证以及控制质量；</li>
<li>资源管理，这一部分与组织结构相关，是决定项目成败的关键要素，合适的项目需要合适的人选，其包括4个子过程：人力资源管理，组建团队，建设团队以及管理团队；</li>
<li>沟通管理，包括3个子过程：规划沟通，管理沟通，控制沟通；</li>
<li>风险管理，包括识别风险，风险分析，定量风险，提出应对以及控制风险；</li>
<li>采购管理，包括4个子过程：规划采购，实施采购，控制采购，结束采购；</li>
<li>相关方管理，相关方管理也是项目成败与否的一个非常关键的要素，包括4个过程：识别相关方，相关方参与，相关方满意度等；</li>
<li>测试管理，项目管理往往容易忽略掉QA测试的作用，在代码编写好后需要提前安排QA介入，记住一点没有经过QA严格质量测试的软件包是不能输出给客户的；</li>
<li>综合管理，综合管理的关键是综合平衡最优，平衡以上11项使之达到最优，包括制订章程，制定计划，指导与管理执行，监控项目，变更控制，结束项目。</li>
</ul>
<p>不管是项目管理5大流程还是项目管理12项，其中有一个非常重要的工作即创建<strong>“工作分解结构”</strong>，工作分解结构是开展一切项目管理的依据与基础，可以说没有“工作分解结构”就没有项目管理。</p>
<h3 id="工作分解结构"><a href="#工作分解结构" class="headerlink" title="工作分解结构"></a>工作分解结构</h3><p>在工作过程中，人的认知是有差异的，比较资深的人员描述一个任务或问题时会比较的抽象，而初级工程师比较能理解的是具体的任务描述，为了团队具有较好的执行力，就需要把抽象的概念或描述分解成具体的可执行的行为或任务，这就需要一种合适的工具或方法来分解概念与工作。</p>
<p>工作分解结构法（WBS: Work Breakdown Structure，是一个“描述思路的规划和设计的工具”，是以项目结果为导向的工作过程的结构分解方法论，是将抽象的概念或任务分解成具体的行为的一种工具。将工作进行合理的分解是项目负责人的重要能力之一，缺乏项目分解能力的项目负责人容易造成项目失败，“工作分解”的好坏与否 也可以决定项目的成败与否。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-1.PNG" alt="项目"></p>
<p>如上图所示，项目管理有两大基石：<strong>工作分解结构法与 企业文化+组织结构</strong>， WBS在PMBOK中给出的定义是：“WBS是针对可交付成果对项目要素进行的分组，它归纳和定义了项目的整个工作范围，每下降一层就代表对项目工作更详细地定义。”。这一定义说明WBS所分解的目标是工作包(Work Package)，以可交付成果为导向的。企业文化与组织结构也是项目成功与否的基石，企业文化作保证，组织调整到位可以给项目提供坚强的基础资源与能力。</p>
<p>“工作分解结构法”的任务拆解是个有拆有合的过程。一个大的工程任务往往是由很多的小个的任务组成的，如何将一个大任务拆解成合适的小任务，能将大任务拆解成什么样的小任务，能拆解到什么粒度，拆解是否准确，这是”拆“的过程。怎样对任务进行量化与质化，怎么将任务落地到具体的执行人员上，怎么进集成怎么验收，这是“合”。任务拆解比较的考验工程人员的工程能力，任务拆解是否成功是工程进度与工程交付能否成功的关键要素。</p>
<h2 id="如何进行工作结构分解"><a href="#如何进行工作结构分解" class="headerlink" title="如何进行工作结构分解"></a><font color="#FF8C00">如何进行工作结构分解</font></h2><h3 id="任务树分解法"><a href="#任务树分解法" class="headerlink" title="任务树分解法"></a>任务树分解法</h3><p>如下图所示，工作分解结构就是把项目可交付成果，比如总目标一级级的分解成较小的、更易于执行的组成部分的过程，建立工作分解结构的过程就是将项目进行显性化、结构化，将复杂的总目标分解成一级任务、二级任务，最后分解成最小可执行工作包的过程。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-4.PNG" alt="项目"></p>
<p>工作分解结构可以按功能、组成、生命周期以及组织的形式进行分析，在软件工程项目中，通常以项目生命周期 + 功能以任务树的方式分解项目。如下图所示，</p>
<p><strong>最终交付成果</strong>按生命周期分解成了 项目管理、业务、需求、设计与编码、集成以及交付这6项。然后设计与编码又按功能的形式分为一级任务特性：关键特性1、关键特性2、关键特性N，以此类推，直至分解成最小可执行工作包。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-3.PNG" alt="项目"></p>
<p>工作分解结构法是面向结果为导向的分解，分解过程中也需要适可而止，比如初级工程师需要给他分解到很具体的最小可执行工作包这种层次，而比较资深的工程师可以给他分解到关键特性这一层次即可，当然前提是这个资深工程师有能力承担这一特性的分解与执行。</p>
<p>此外，任务树适合项目汇报使用，而工作包表格适合团队内部协作使用，其能提供更详细的细节，因此在任务树拆解后，下一步是进行工作表格分解。</p>
<h3 id="工作表格分解法"><a href="#工作表格分解法" class="headerlink" title="工作表格分解法"></a>工作表格分解法</h3><p>对比于任务树，工作包表格可以在右侧增加更多的细节，这有利于团队协作，以下图的软件项目工作包表格为例：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-7.PNG" alt="项目"></p>
<p>1，首先前三行定义了项目的目标、原则以及投入，说明了项目的交付目标、交付的原则以及约束、还有确定了人力资源的投入,；</p>
<p>2，任务分解成了一级任务、二级任务以及工作包、工作包名称；</p>
<p>3，除这些在任务树上也可以体现的内容外，还增加了 工作进展、状态、评论、风险、阶段目标、执行的人员、复杂度以及依赖条件；</p>
<p>4，输出最终交付成果。</p>
<p>工作包表格的这些细节可以使得团队协作更加顺畅，所以一般开发过程中选用工作包分解表作为项目跟进的任务进度表。</p>
<h3 id="关键路径网络图分解法"><a href="#关键路径网络图分解法" class="headerlink" title="关键路径网络图分解法"></a>关键路径网络图分解法</h3><p>除了任务树与工作包表格之外还有关键路径网络图分解法，在项目当中有些任务比较重要而复杂，有些任务相互独立、有些任务又有依赖关系，</p>
<p>如下图所示，S 表示Start，F表示Finish，SS 表示同时启动，FS表示先完成再启动。F11任务需要在F1任务启动后10天再启动，F23与F31任务可以同时并发进行，</p>
<p>而最终交付的目标需要 F2211 与 F3111 同时完成。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-5.PNG" alt="项目"></p>
<p>关键路径网络图分解法明确了任务之间逻辑关系、先后关系、时间关系，有利于项目计划进度时间的评估与规划。</p>
<h2 id="工作分解结构法的理念与原则"><a href="#工作分解结构法的理念与原则" class="headerlink" title="工作分解结构法的理念与原则"></a><font color="#FF8C00">工作分解结构法的理念与原则</font></h2><p>以上内容讲的都是 工作分解结构法的 “术”的层次，其本身还有背后的理念 ，即“道”。工作分解结构法是<strong>“系统思维与分治思维”</strong>在软件工程项目中的应用。工作分解除了非常重要的专业技能，还需要遵循以下几个理念与原则：</p>
<p>1，滚动原则</p>
<p>如下图所示，项目的推进是一个 “分解、执行、检查、更新”滚动推进的过程，依据导弹思维，先保证大方向正确，开工，然后再在过程中调整小方向，直至最终精确的命中目标。项目管理也是一个逐步实现目标的过程，时间上近的获取的信息多、那么分解的层次与类目自然也就更细、更多、更准确，而时间上远的，因为信息不够完善，分解的层次与类目自然更少少，也没那么精确，在目标逐步达成后，可以进行刷新重新进行分解，完善工作包，直至最终达成目标。</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/wbs/wbs-6.PNG" alt="项目"></p>
<p>2，MECE原则</p>
<p>MECE即“相互独立，完全穷尽”，是应用系统思维从全局的角度分解任务，既不重复也不遗漏，纵向递进，横向遍历。</p>
<p>3，量化质化原则</p>
<p>分解出来的工作包需要能量化，需要可量化的完成时间、可量化的验收标准，不能量化的任务需要质化，即以满意度的方式验收。</p>
<p>4，以上统下原则</p>
<p>工作包分解的目标是可交付成果，每一个下级成果有且只有一个上级成果，每一个上级成果是其下级成果之和，不同的交付成果可以分解到不同的层级；</p>
<p>5，适可而止原则</p>
<p>工作包并不是分解的越细越好，而应当根据项目特性的难易程度进行分解，难的复杂度高的分解细点、适当增加分解的层级，而容易的复杂度低的可以减少分层。</p>
<p>6，人事匹配原则</p>
<p>需要考虑每一级任务到底需要什么样的领域专家或者工程人员，任务要能匹配工程人员的能力，工程人员能力要能匹配任务，考虑到执行力，还需要将任务分解到人人有事做，事事能完成的层次。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font color="#FF8C00">小结</font></h2><p>本文从宏观角度讲述了工作分解结构法的“术”与“道”，在宏观上理解了工作分解结构法，还需要微观上进行实践，才能做到“学以致用”。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a><font color="#FF8C00">作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习首席软件工程师，前EMC 大数据资深首席工程师，主要工作背景在深度学习、流式大数据、云计算、分布式中间件以及Linux内核。</p>
<h2 id="版权申明"><a href="#版权申明" class="headerlink" title="版权申明"></a><font color="#FF8C00">版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color="#FF8C00">参考资料</font></h2><p>[1] 《极简项目管理》 郭致星著</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/product/" rel="tag"><i class="fas fa-tags"></i>product</a>
        
      </div>
      
    </footer>
  </article>
  
  <article class="article post card" itemscope itemtype="http://schema.org/Article">
    <div class="post-block">
      <link itemprop="mainEntityOfPage" href="www.changping.me/2020/12/27/distributed-product_the_engineering_and_project_management/">
      <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
        <meta itemprop="name" content="常平">
        <meta itemprop="description" content="“认识实践，实践认识”">
        <meta itemprop="image" content="/images/avatar.jpg">
      </span>
      <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
        <meta itemprop="name" content="常平的技术博客">
      </span>
    </div>
    <header class="post-header">
      <h1 class="post-title" itemprop="name headline">
        <a class="post-title-link post-title-link-external" href="/2020/12/27/distributed-product_the_engineering_and_project_management/" itemprop="url">分布式系统架构设计 – 第26式- 什么是工程化与如何做项目管理</a>
      </h1>
      <div class="post-meta">
        
        <span class="post-date">
          <i class="far fa-calendar-plus"></i><span><time title="post-date" itemprop="dateCreated datePublished" datetime="2020-12-27T20:55:00+08:00">2020-12-27 20:55:00</time></span>
        </span>
        
        
        
        <span class="post-meta-divider divider">|</span>
        
        <span class="post-categories">
          
          <i class="far fa-folder-open"></i><span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/product/" itemprop="url" rel="index"><span itemprop="name">product</span></a></span>
        </span>
        
        
      </div>
    </header>
    <main class="post-main" itemprop="articleBody">
      
      <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><font color="#FF8C00">前言</font></h2><p>不同于科研可以专研的很深而无需转化成生产力，工程化是一套将项目转化成产品从而达成生产力目标的科学方法论。架构师是业务与技术、产品之前的桥梁，除了熟练掌握软件开发的本质、方法论、技能、工具还应该具备工程化能力与项目管理能力，即任务的分析，拆解，计划，执行，领导，管控与团队的组织与管理。</p>
<h2 id="工程化"><a href="#工程化" class="headerlink" title="工程化"></a><font color="#FF8C00">工程化</font></h2><h3 id="工程化就是任务拆解"><a href="#工程化就是任务拆解" class="headerlink" title="工程化就是任务拆解"></a>工程化就是任务拆解</h3><p>任务拆解是个有拆有合的过程。一个大的工程任务往往是由很多的小个的任务组成的，如何将一个大任务拆解成合适的小任务，能将大任务拆解成什么样的小任务，能拆解到什么粒度，拆解是否准确，这是”拆“。怎样对任务进行量化与质化，怎么将任务落地到具体的执行人员上，这是“合”。任务拆解比较的考验工程人员的工程能力，任务拆解是否成功是工程进度与工程交付能否成功的关键要素之一。</p>
<h3 id="工程化就是取舍"><a href="#工程化就是取舍" class="headerlink" title="工程化就是取舍"></a>工程化就是取舍</h3><p>工程化是取舍的艺术，工程受限于已有的条件，需要做出适当的取舍，才能解决实际问题。资源都是有限的，项目开工之前先搞清资源与人员能力，然后制定人员调配策略与合理的进度安排，需要计划让哪些人员去处理哪些问题，也需要制定合理的资源调度策略，从而才能稳操胜券。</p>
<p>比如“田忌赛马”就是一个经典的人力与资源的调配故事，精明的项目管理人员在项目开工之前就能彻底的预判结果，告诉资源掌控者我缺啥需要啥，满足这些条件才有取胜可能，然后再采用合理的策略从而抵达目标获取胜利。不同的人员会制定不同的策略，这也是工程人员的能力差异所在。</p>
<h3 id="工程化就是组织与沟通"><a href="#工程化就是组织与沟通" class="headerlink" title="工程化就是组织与沟通"></a>工程化就是组织与沟通</h3><p>一个工程往往需要几十、几百甚至几千、几万人协同工作为达成一个共同的目标而奋斗，因此，在工程中人员的组织、沟通协调也非常重要，合理的人放到合适的位置，才能发挥其能力从而高效的解决实际问题，同时工程也讲究文档化、标准化、流程化与规范化。</p>
<h3 id="工程化就是进度安排"><a href="#工程化就是进度安排" class="headerlink" title="工程化就是进度安排"></a>工程化就是进度安排</h3><p>将任务进行拆解仅是工程化解决问题的一个步骤，任务拆解之后还要区别主要矛盾与次要矛盾，进行优先级排序与里程碑确定，需要从时间上进行合理的安排与调度，最终让每一个子任务之间做到有效的协同与交付。</p>
<h3 id="项目管理是工程化的子集"><a href="#项目管理是工程化的子集" class="headerlink" title="项目管理是工程化的子集"></a>项目管理是工程化的子集</h3><p>工程化是以现有资源与技术为基础，通过加人员、技能、知识组合起来，短时间内快速解决实际的复杂问题的一种方法。软件工程是指将系统化、规范、可度量的方法应用于软件开发的过程以及软件的运行和维护，其包括两方面内容：<strong>软件开发和软件项目管理</strong>。因此，项目管理是工程化的子集。</p>
<h2 id="项目管理"><a href="#项目管理" class="headerlink" title="项目管理"></a><font color="#FF8C00">项目管理</font></h2><h4 id="项目铁三角"><a href="#项目铁三角" class="headerlink" title="项目铁三角"></a>项目铁三角</h4><p>项目铁三角指的是项目管理的四个重要方面，即：</p>
<ul>
<li><p>范围：需要做什么；</p>
</li>
<li><p>时间：什么时间做完；</p>
</li>
<li><p>成本：投入多少资源；</p>
</li>
<li><p>质量：做到什么程度才算达标。</p>
</li>
</ul>
<p>范围、成本、时间三者之间任何一个变动均会对其他两项产生影响，如下图所示：</p>
<p><img src="https://wuchangping.oss-cn-hangzhou.aliyuncs.com/product/distributed-product-project_management.png" alt="image-20201227171818955"></p>
<p>范围扩大（需求增加），做的事情多了，时间进度就需要延长，并且成本也会增加。如果做的事情多了而时间与成本投入又不变那么必然影响到质量。如果不打算减少所做的事情，就必然需要多投入成本（比如时间，人力），否者质量与范围又无法保证。范围、时间、成本之间的制约关系是必然存在的，但是也要依据实际情况而采取合适的方法，有的项目时间节点是固定的、有的质量要求严格、有的有固定的成本约束等，因此，需要根据情况进行合理的调整。</p>
<h4 id="项目三重境"><a href="#项目三重境" class="headerlink" title="项目三重境"></a>项目三重境</h4><p>第一重，满足客户质量要求，这是项目管理的最基本的需求，在范围、时间、成本之间获取平衡是为了达到客户的质量要求；</p>
<p>第二重，满足业务的需求，项目管理人员需要懂业务，需要知道为什么需要做，什么是客户需要的，避免项目大方向的错误；</p>
<p>第三重，满足组织成员的需求，项目是由人实施的，其注入了人的精神与意志，因此也需要了解组织成员的需求，准确并且满足成员的需求，才能更好的推进项目。</p>
<h3 id="项目拆解"><a href="#项目拆解" class="headerlink" title="项目拆解"></a>项目拆解</h3><h4 id="道与法"><a href="#道与法" class="headerlink" title="道与法"></a>道与法</h4><p>架构师完成概要设计后就需要进行项目拆解，拆解的成功与否是项目能否按期交付的关键，在进行项目拆解的时候需要遵循以下的<strong>“一人二法三要素四角色”</strong>原则，</p>
<ul>
<li><p>一人： 项目是由人执行的，人是项目里最关键的要素，合适的任务要拆解到具体的合适的人员上；</p>
</li>
<li><p>二法：量化与质化，任务拆解需要能量化（类似KPI，比如先赚它一个小目标：一个亿）有具体的时间、具体的数值，不能量化的需要质化（如同OKR，比如客户对这个质量感到满意，这个缺陷的修复方案QA已验证接受等，这个策略已经被客户所接受）；</p>
</li>
<li><p>三要素：即范围、时间、成本，在有限的成本、范围、时间约束下达成质量目标；</p>
</li>
<li><p>四角色：即RACI角色: 谁负责（R = Responsible）: 谁来干这活，谁批准（A = Accountable）：谁说了算，咨询谁（C = Consulted）：专家团，顾问是谁， 通知谁 (I =Informed)：谁需要被通知到，谁需要知道这个进度与风险。</p>
</li>
</ul>
<h4 id="术与器"><a href="#术与器" class="headerlink" title="术与器"></a>术与器</h4><p>如何进行拆解，能拆解成什么样的任务，这也比较考验项目管理人员的专业技能，这属于”术” 的范畴，同时也可以借助合适的工具（器）编排拆解的任务与进度。</p>
<h3 id="项目计划"><a href="#项目计划" class="headerlink" title="项目计划"></a>项目计划</h3><p>1，项目计划的本质是项目执行人员的<strong>承诺</strong>，因此，不同于产品与项目制定人员比较看重的是项目的价值，执行人员通常会比较关注项目的资源投入、技术难度、技术实现、里程碑、奖罚等，因此对于计划都会比较的谨慎；</p>
<p>2，计划本身没有太大作用的，但是没有计划却万万不行，如同“天气预报”，没几次预报会准确，但是好处是有了计划就有了可预测性；</p>
<p>3，大的项目计划需要拆分成合理的里程碑，每个里程碑都能对应到一个最小可交付版本（MVP）用于进行市场验证（PMF）；</p>
<p>4， 大方向与里程碑先保证正确，过程中进行小调整。</p>
<p>有时候计划的交付时间点是固定的，其由商业交付时间点反推，那么就需要在 “范围，成本与质量承诺”这三点上进行合理的取舍。</p>
<h3 id="人员与组织"><a href="#人员与组织" class="headerlink" title="人员与组织"></a>人员与组织</h3><p>项目是由人完成的，了解团队成员的需求，满足TA所要的，这样具有保持团队稳定以及项目价值输出的可行性。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><font color="#FF8C00">小结</font></h2><p>本文从宏观角度概述了工程化与项目管理的差异，只会项目计划那是最初级的项目管理，更高级的项目管理是懂方法论，懂任务拆解，懂项目计划，懂业务、也懂人与组织等。在宏观上了解了工程化与项目管理，还需要微观上进行实践，手把手的操作过、练过、蹲过坑、吃过苦头才能叫做“实践出真知”。另作者能力与认知都有限，”我讲的，可能都是错的“，欢迎大家拍砖留念。</p>
<h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a><font color="#FF8C00">作者简介</font></h2><p>常平，中科大硕，某AI独角兽深度学习首席软件工程师，前EMC 大数据资深首席工程师，主要工作背景在深度学习、流式大数据、云计算、分布式中间件以及Linux内核。</p>
<h2 id="版权申明"><a href="#版权申明" class="headerlink" title="版权申明"></a><font color="#FF8C00">版权申明</font></h2><p>本文的版权协议为 CC-BY-NC-ND license：<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" target="_blank" rel="noopener">https://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh</a></p>
<p>在遵循署名、非商业使用（以获利为准）以及禁止演绎的前提下可以自由阅读、分享、转发、复制、分发等。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color="#FF8C00">参考资料</font></h2><p>[1] <a href="https://www.zhihu.com/question/26699139" target="_blank" rel="noopener">https://www.zhihu.com/question/26699139</a></p>
<p>[2] 《网易一千零一夜 - 互联网产品项目管理实战》 网易杭研项目管理部著</p>

      
    </main>
    <footer class="post-footer">
      
      <div class="post-tags">
        
        <a class="post-tag button" href="/tags/product/" rel="tag"><i class="fas fa-tags"></i>product</a>
        
      </div>
      
    </footer>
  </article>
  
  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right" aria-label="下一页"></i></a>
  </nav>
  
  
</div>

          </div>
          
          
          
<aside class="sidebar" id="sidebar" >
  
  
<div class="info sidebar-item" id="info">
  
  <img class="author-avatar" src="/images/avatar.jpg" alt="常平">
  
  <h1 class="author-name">常平</h1>
  <h2 class="author-description">“认识实践，实践认识”</h2>
  <div class="site-count">
    
    <div class="archives-count">
      <div class="site-count-title">全部</div>
      <div><a href="/archives">57</a></div>
    </div>
    
    
    
    <span class="site-count-divider divider">|</span>
    
    <div class="categories-count">
      <div class="site-count-title">分类</div>
      <div><a href="/categories">6</a></div>
    </div>
    
    
    
    <span class="site-count-divider divider">|</span>
    
    <div class="tags-count">
      <div class="site-count-title">标签</div>
      <div><a href="/tags">6</a></div>
    </div>
    
  </div>
  
</div>


  <div class="sidebar-sticky">
    
    
    <hr>
    <div class="social-link sidebar-item">
      <div><i class="far fa-address-card"></i>链接</p></div>
      <ul>
        
        <li><i class="fab fa-github"></i><a href="https://github.com/wuchangping" target="_blank">GitHub</a></li>
        
      </ul>
    </div>
    
    
  </div>
</aside>


          
        </div>
      </div>
    </main>
    
<footer id="footer" class="footer" style="background: #1D2D2D;">
  <div class="container">
    <div class="back-to-top">
      <button id="back-to-top"><i class="fas fa-angle-double-up" aria-label="回到顶部"></i></button>
    </div>
    <div class="footer-container">
      <div class="footer-left">
        <div class="copyright">
          <span class="author">常平</span><span class="year"><i class="far fa-copyright"></i>2017 - 2022</span>
        </div>
        
        <div class="busuanzi">
          <span id="busuanzi_container_site_pv"><i class="fas fa-eye" aria-label="站点点击量" aria-hidden="false"></i><span id="busuanzi_value_site_pv"></span></span><span id="busuanzi_container_site_uv"><i class="fas fa-user" aria-label="站点用户数" aria-hidden="false"></i><span id="busuanzi_value_site_uv"></span></span><span id="busuanzi_container_page_pv"><i class="far fa-file-alt"></i><span id="busuanzi_value_page_pv" aria-label="页面点击量" aria-hidden="false"></span></span>
        </div>
        
      </div>
      <div class="footer-right">
        <div class="custom-info">
          
          PoweredBy<i class="fab fa-github-alt"></i><a href="https://github.com/wuchangping" target="_blank">GitHub</a>
          
        </div>
        <div class="powered-by">
          由 <a href="https://hexo.io/" target="_blank">Hexo</a> 强力驱动 | 主题 <a href="https://github.com/AlynxZhou/hexo-theme-aria/" target="_blank">ARIA</a>
        </div>
      </div>
    </div>
  </div>
</footer>


  </body>
</html>
